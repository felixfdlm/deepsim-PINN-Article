{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9bcfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "import numpy as np\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bf5058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- SCIANN 0.6.4.5 ---------------------- \n",
      "For details, check out our review paper and the documentation at: \n",
      " +  \"https://www.sciencedirect.com/science/article/pii/S0045782520307374\", \n",
      " +  \"https://arxiv.org/abs/2005.08803\", \n",
      " +  \"https://www.sciann.com\". \n",
      "\n",
      " Need support or would like to contribute, please join sciann`s slack group: \n",
      " +  \"https://join.slack.com/t/sciann/shared_invite/zt-ne1f5jlx-k_dY8RGo3ZreDXwz0f~CeA\" \n",
      " \n",
      "TensorFlow Version: 2.3.0 \n",
      "Python Version: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "%run plate_EQS.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ae1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pinn_Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661ae1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valData = [['u.txt'],'ofen']\n",
    "test_gridObj = SEQ.Grid(3,{'x':0,'y':1,'t':2},[[[-2,2],[-2,2],[0,10]]],10)\n",
    "configspecs = {\n",
    "    'denspt':[3,8],\n",
    "    'numNeurons':[10,100],\n",
    "    'numLayers':[2,50],\n",
    "    'activator': ['elu','relu','selu','swish',\n",
    "                 'tanh','Addons>gelu','Addons>mish',\n",
    "                          'Addons>softshrink'], \n",
    "    'loss':['mae','mse'],\n",
    "    'optimizer':['Adam','RMSprop','SGD','Nadam','Ftrl'],\n",
    "    'batch_size':[5000,10000,15000,25000],\n",
    "    'initial_lr':[1e-4,1e-2]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16be55da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating worker\n",
      "Creating client\n",
      "New experiment. Creating instance.\n",
      "Parsing validation data\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Worker created, starting it\n",
      "Worker running, adding it to the list\n",
      "Worker added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:06:11 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x1dd3c560148; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "17:06:11 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "17:06:11 WORKER: start listening for jobs\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Start a nameserver\n",
    "# Every run needs a nameserver. It could be a 'static' server with a\n",
    "# permanent address, but here it will be started for the local machine with the default port.\n",
    "# The nameserver manages the concurrent running workers across all possible threads or clusternodes.\n",
    "# Note the run_id argument. This uniquely identifies a run of any HpBandSter optimizer.\n",
    "NS = hpns.NameServer(run_id='example1', host='127.0.0.1', port=None)\n",
    "NS.start()\n",
    "\n",
    "# Step 2: Start a worker\n",
    "# Now we can instantiate a worker, providing the mandatory information\n",
    "# Besides the sleep_interval, we need to define the nameserver information and\n",
    "# the same run_id as above. After that, we can start the worker in the background,\n",
    "# where it will wait for incoming configurations to evaluate.\n",
    "numWorkers = 1\n",
    "experiment_name = 'plate_exp'\n",
    "run_id='plate_Gridsearch_1'\n",
    "\n",
    "\n",
    "workers=[]\n",
    "for i in range(1,numWorkers+1):\n",
    "    print('Creating worker')\n",
    "    w = Pinn_Worker.PINN_Worker(\n",
    "    valData = valData,\n",
    "    test_gridObj=test_gridObj,\n",
    "    PDESystem=mySys,\n",
    "    configspecs = configspecs,\n",
    "    valFromFEM=True,\n",
    "    experiment_name = experiment_name,\n",
    "    nameserver='127.0.0.1',\n",
    "    run_id=run_id,\n",
    "    id=i)\n",
    "    \n",
    "    print('Worker created, starting it')\n",
    "    w.run(background=True)\n",
    "    print('Worker running, adding it to the list')\n",
    "    workers.append(w)\n",
    "    print('Worker added')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d9ff26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:06:11 wait_for_workers trying to get the condition\n",
      "17:06:11 DISPATCHER: started the 'discover_worker' thread\n",
      "17:06:11 DISPATCHER: started the 'job_runner' thread\n",
      "17:06:11 DISPATCHER: Pyro daemon running on localhost:49906\n",
      "17:06:11 DISPATCHER: Starting worker discovery\n",
      "17:06:11 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
      "17:06:11 DISPATCHER: discovered new worker, hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:06:11 HBMASTER: number of workers changed to 1\n",
      "17:06:11 Enough workers to start this run!\n",
      "17:06:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:06:11 adjust_queue_size: lock accquired\n",
      "17:06:11 HBMASTER: starting run at 1648310771.895985\n",
      "17:06:11 HBMASTER: adjusted queue size to (0, 1)\n",
      "17:06:11 DISPATCHER: Finished worker discovery\n",
      "17:06:11 start sampling a new configuration.\n",
      "17:06:11 DISPATCHER: Trying to submit another job.\n",
      "17:06:11 done sampling a new configuration.\n",
      "17:06:11 HBMASTER: schedule new run for iteration 0\n",
      "17:06:11 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "17:06:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:06:11 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "17:06:11 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "17:06:11 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:06:11 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "17:06:11 DISPATCHER: Trying to submit another job.\n",
      "17:06:11 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:06:11 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:06:11 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:06:11 WORKER: start processing job (0, 0, 0)\n",
      "17:06:11 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:06:11 WORKER: args: ()\n",
      "17:06:11 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0011225914254538842, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 4320 \n",
      "Batch size: 4320 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:07:05 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "17:07:05 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "17:07:05 DISPATCHER: job (0, 0, 0) finished\n",
      "17:07:05 DISPATCHER: register_result: lock acquired\n",
      "17:07:05 DISPATCHER: job (0, 0, 0) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:07:05 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0011225914254538842, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 171.14094707676958, 'info': {'L1': 171.14094707676958, 'L2': 269.3640637903978, 'MAX': 3.1972122192382812, 'TrainTime': 40.3125}}\n",
      "exception: None\n",
      "\n",
      "17:07:05 job_callback for (0, 0, 0) started\n",
      "17:07:05 DISPATCHER: Trying to submit another job.\n",
      "17:07:05 job_callback for (0, 0, 0) got condition\n",
      "17:07:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:07:05 Only 1 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:07:05 HBMASTER: Trying to run another job!\n",
      "17:07:05 job_callback for (0, 0, 0) finished\n",
      "17:07:05 start sampling a new configuration.\n",
      "17:07:05 done sampling a new configuration.\n",
      "17:07:05 HBMASTER: schedule new run for iteration 0\n",
      "17:07:05 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "17:07:05 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "17:07:05 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "17:07:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:07:05 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "17:07:05 DISPATCHER: Trying to submit another job.\n",
      "17:07:05 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:07:05 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:07:05 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:07:05 WORKER: start processing job (0, 0, 1)\n",
      "17:07:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:07:05 WORKER: args: ()\n",
      "17:07:05 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.0054477338287029844, 'loss': 'mae', 'numLayers': 12, 'numNeurons': 87, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 20000 \n",
      "Batch size: 20000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:07:11 DISPATCHER: Starting worker discovery\n",
      "17:07:11 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:07:11 DISPATCHER: Finished worker discovery\n",
      "17:08:11 DISPATCHER: Starting worker discovery\n",
      "17:08:11 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:08:12 DISPATCHER: Finished worker discovery\n",
      "17:09:12 DISPATCHER: Starting worker discovery\n",
      "17:09:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:09:12 DISPATCHER: Finished worker discovery\n",
      "17:10:12 DISPATCHER: Starting worker discovery\n",
      "17:10:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:10:12 DISPATCHER: Finished worker discovery\n",
      "17:11:12 DISPATCHER: Starting worker discovery\n",
      "17:11:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:11:12 DISPATCHER: Finished worker discovery\n",
      "17:12:12 DISPATCHER: Starting worker discovery\n",
      "17:12:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:12:12 DISPATCHER: Finished worker discovery\n",
      "17:12:22 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "17:12:22 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "17:12:22 DISPATCHER: job (0, 0, 1) finished\n",
      "17:12:22 DISPATCHER: register_result: lock acquired\n",
      "17:12:22 DISPATCHER: job (0, 0, 1) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:12:22 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.0054477338287029844, 'loss': 'mae', 'numLayers': 12, 'numNeurons': 87, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: None\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20000,87] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Ftrl/gradients/gradients/Grad2_u_x/_2/gradients_1/Grad2_u_x/_2/gradients/sci_activation_3/IdentityN_grad/Sigmoid_grad/SigmoidGrad_grad/SigmoidGrad}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "\n",
      "17:12:22 job_callback for (0, 0, 1) started\n",
      "17:12:22 DISPATCHER: Trying to submit another job.\n",
      "17:12:22 job_callback for (0, 0, 1) got condition\n",
      "17:12:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:12:22 job (0, 0, 1) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20000,87] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Ftrl/gradients/gradients/Grad2_u_x/_2/gradients_1/Grad2_u_x/_2/gradients/sci_activation_3/IdentityN_grad/Sigmoid_grad/SigmoidGrad_grad/SigmoidGrad}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "17:12:22 Only 2 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:12:22 HBMASTER: Trying to run another job!\n",
      "17:12:22 job_callback for (0, 0, 1) finished\n",
      "17:12:22 start sampling a new configuration.\n",
      "17:12:22 done sampling a new configuration.\n",
      "17:12:22 HBMASTER: schedule new run for iteration 0\n",
      "17:12:22 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "17:12:22 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "17:12:22 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "17:12:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:12:22 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "17:12:22 DISPATCHER: Trying to submit another job.\n",
      "17:12:22 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:12:22 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:12:22 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:12:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:12:22 WORKER: start processing job (0, 0, 2)\n",
      "17:12:22 WORKER: args: ()\n",
      "17:12:22 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 10240 \n",
      "Batch size: 5000 \n",
      "Total batches: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:13:12 DISPATCHER: Starting worker discovery\n",
      "17:13:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:13:12 DISPATCHER: Finished worker discovery\n",
      "17:14:07 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "17:14:07 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "17:14:07 DISPATCHER: job (0, 0, 2) finished\n",
      "17:14:07 DISPATCHER: register_result: lock acquired\n",
      "17:14:07 DISPATCHER: job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:14:07 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 132.26600117883856, 'info': {'L1': 132.26600117883856, 'L2': 187.40280507653355, 'MAX': 4.238853292786475, 'TrainTime': 145.421875}}\n",
      "exception: None\n",
      "\n",
      "17:14:07 job_callback for (0, 0, 2) started\n",
      "17:14:07 DISPATCHER: Trying to submit another job.\n",
      "17:14:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:14:07 job_callback for (0, 0, 2) got condition\n",
      "17:14:07 Only 3 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:14:07 HBMASTER: Trying to run another job!\n",
      "17:14:07 job_callback for (0, 0, 2) finished\n",
      "17:14:07 start sampling a new configuration.\n",
      "17:14:07 done sampling a new configuration.\n",
      "17:14:07 HBMASTER: schedule new run for iteration 0\n",
      "17:14:07 HBMASTER: trying submitting job (0, 0, 3) to dispatcher\n",
      "17:14:07 HBMASTER: submitting job (0, 0, 3) to dispatcher\n",
      "17:14:07 DISPATCHER: trying to submit job (0, 0, 3)\n",
      "17:14:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:14:07 HBMASTER: job (0, 0, 3) submitted to dispatcher\n",
      "17:14:07 DISPATCHER: Trying to submit another job.\n",
      "17:14:07 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:14:07 DISPATCHER: starting job (0, 0, 3) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:14:07 DISPATCHER: job (0, 0, 3) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:14:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:14:07 WORKER: start processing job (0, 0, 3)\n",
      "17:14:07 WORKER: args: ()\n",
      "17:14:07 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.005356364754283974, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 60, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "17:14:12 DISPATCHER: Starting worker discovery\n",
      "17:14:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:14:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 20000 \n",
      "Batch size: 20000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:15:12 DISPATCHER: Starting worker discovery\n",
      "17:15:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:15:12 DISPATCHER: Finished worker discovery\n",
      "17:15:17 WORKER: done with job (0, 0, 3), trying to register it.\n",
      "17:15:17 WORKER: registered result for job (0, 0, 3) with dispatcher\n",
      "17:15:17 DISPATCHER: job (0, 0, 3) finished\n",
      "17:15:17 DISPATCHER: register_result: lock acquired\n",
      "17:15:17 DISPATCHER: job (0, 0, 3) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:15:17 job_id: (0, 0, 3)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.005356364754283974, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 60, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 378.17298435613543, 'info': {'L1': 378.17298435613543, 'L2': 1022.7500895338197, 'MAX': 5.393468774532959, 'TrainTime': 82.71875}}\n",
      "exception: None\n",
      "\n",
      "17:15:17 job_callback for (0, 0, 3) started\n",
      "17:15:17 DISPATCHER: Trying to submit another job.\n",
      "17:15:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:15:17 job_callback for (0, 0, 3) got condition\n",
      "17:15:17 Only 4 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:15:17 HBMASTER: Trying to run another job!\n",
      "17:15:17 job_callback for (0, 0, 3) finished\n",
      "17:15:17 start sampling a new configuration.\n",
      "17:15:17 done sampling a new configuration.\n",
      "17:15:17 HBMASTER: schedule new run for iteration 0\n",
      "17:15:17 HBMASTER: trying submitting job (0, 0, 4) to dispatcher\n",
      "17:15:17 HBMASTER: submitting job (0, 0, 4) to dispatcher\n",
      "17:15:17 DISPATCHER: trying to submit job (0, 0, 4)\n",
      "17:15:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:15:17 HBMASTER: job (0, 0, 4) submitted to dispatcher\n",
      "17:15:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:15:17 DISPATCHER: Trying to submit another job.\n",
      "17:15:17 DISPATCHER: starting job (0, 0, 4) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:15:17 DISPATCHER: job (0, 0, 4) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:15:17 WORKER: start processing job (0, 0, 4)\n",
      "17:15:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:15:17 WORKER: args: ()\n",
      "17:15:17 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.007946507024941664, 'loss': 'mse', 'numLayers': 47, 'numNeurons': 90, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 54880 \n",
      "Batch size: 5000 \n",
      "Total batches: 11 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:16:12 DISPATCHER: Starting worker discovery\n",
      "17:16:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:16:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2347: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1664: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "17:16:21 WORKER: done with job (0, 0, 4), trying to register it.\n",
      "17:16:21 WORKER: registered result for job (0, 0, 4) with dispatcher\n",
      "17:16:21 DISPATCHER: job (0, 0, 4) finished\n",
      "17:16:21 DISPATCHER: register_result: lock acquired\n",
      "17:16:21 DISPATCHER: job (0, 0, 4) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:16:21 job_id: (0, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.007946507024941664, 'loss': 'mse', 'numLayers': 47, 'numNeurons': 90, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 1.7976931348623157e+308, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 57.640625}}\n",
      "exception: None\n",
      "\n",
      "17:16:21 job_callback for (0, 0, 4) started\n",
      "17:16:21 DISPATCHER: Trying to submit another job.\n",
      "17:16:21 job_callback for (0, 0, 4) got condition\n",
      "17:16:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:16:21 Only 5 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:16:21 HBMASTER: Trying to run another job!\n",
      "17:16:21 job_callback for (0, 0, 4) finished\n",
      "17:16:21 start sampling a new configuration.\n",
      "17:16:21 done sampling a new configuration.\n",
      "17:16:21 HBMASTER: schedule new run for iteration 0\n",
      "17:16:21 HBMASTER: trying submitting job (0, 0, 5) to dispatcher\n",
      "17:16:21 HBMASTER: submitting job (0, 0, 5) to dispatcher\n",
      "17:16:21 DISPATCHER: trying to submit job (0, 0, 5)\n",
      "17:16:21 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:16:21 HBMASTER: job (0, 0, 5) submitted to dispatcher\n",
      "17:16:21 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:16:21 DISPATCHER: Trying to submit another job.\n",
      "17:16:21 DISPATCHER: starting job (0, 0, 5) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:16:21 DISPATCHER: job (0, 0, 5) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:16:21 WORKER: start processing job (0, 0, 5)\n",
      "17:16:21 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:16:21 WORKER: args: ()\n",
      "17:16:21 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0015065168018113158, 'loss': 'mse', 'numLayers': 6, 'numNeurons': 49, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 20000 \n",
      "Batch size: 10000 \n",
      "Total batches: 2 \n",
      "\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.000753258413169533.\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.0003766292065847665.\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.00018831460329238325.\n",
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 9.415730164619163e-05.\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 4.7078650823095813e-05.\n",
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 2.3539325411547907e-05.\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 1.1769662705773953e-05.\n",
      "\n",
      "Epoch 00330: ReduceLROnPlateau reducing learning rate to 5.884831352886977e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:17:03 WORKER: done with job (0, 0, 5), trying to register it.\n",
      "17:17:03 WORKER: registered result for job (0, 0, 5) with dispatcher\n",
      "17:17:03 DISPATCHER: job (0, 0, 5) finished\n",
      "17:17:03 DISPATCHER: register_result: lock acquired\n",
      "17:17:03 DISPATCHER: job (0, 0, 5) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:17:03 job_id: (0, 0, 5)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0015065168018113158, 'loss': 'mse', 'numLayers': 6, 'numNeurons': 49, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 980.9530422840699, 'info': {'L1': 980.9530422840699, 'L2': 18296.628738262843, 'MAX': 41.02994554639453, 'TrainTime': 39.953125}}\n",
      "exception: None\n",
      "\n",
      "17:17:03 job_callback for (0, 0, 5) started\n",
      "17:17:03 DISPATCHER: Trying to submit another job.\n",
      "17:17:03 job_callback for (0, 0, 5) got condition\n",
      "17:17:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:17:03 Only 6 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:17:03 HBMASTER: Trying to run another job!\n",
      "17:17:03 job_callback for (0, 0, 5) finished\n",
      "17:17:03 start sampling a new configuration.\n",
      "17:17:03 done sampling a new configuration.\n",
      "17:17:03 HBMASTER: schedule new run for iteration 0\n",
      "17:17:03 HBMASTER: trying submitting job (0, 0, 6) to dispatcher\n",
      "17:17:03 HBMASTER: submitting job (0, 0, 6) to dispatcher\n",
      "17:17:03 DISPATCHER: trying to submit job (0, 0, 6)\n",
      "17:17:03 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:17:03 HBMASTER: job (0, 0, 6) submitted to dispatcher\n",
      "17:17:03 DISPATCHER: Trying to submit another job.\n",
      "17:17:03 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:17:03 DISPATCHER: starting job (0, 0, 6) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:17:03 DISPATCHER: job (0, 0, 6) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:17:03 WORKER: start processing job (0, 0, 6)\n",
      "17:17:03 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:17:03 WORKER: args: ()\n",
      "17:17:03 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 4, 'initial_lr': 0.009367510522111737, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 34, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 10240 \n",
      "Batch size: 10240 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:17:12 DISPATCHER: Starting worker discovery\n",
      "17:17:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:17:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00198: ReduceLROnPlateau reducing learning rate to 0.004683755338191986.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:17:45 WORKER: done with job (0, 0, 6), trying to register it.\n",
      "17:17:45 WORKER: registered result for job (0, 0, 6) with dispatcher\n",
      "17:17:45 DISPATCHER: job (0, 0, 6) finished\n",
      "17:17:45 DISPATCHER: register_result: lock acquired\n",
      "17:17:45 DISPATCHER: job (0, 0, 6) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:17:45 job_id: (0, 0, 6)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 4, 'initial_lr': 0.009367510522111737, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 34, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 460.8195537677876, 'info': {'L1': 460.8195537677876, 'L2': 4447.805457398232, 'MAX': 30.92001690486914, 'TrainTime': 33.75}}\n",
      "exception: None\n",
      "\n",
      "17:17:45 job_callback for (0, 0, 6) started\n",
      "17:17:45 DISPATCHER: Trying to submit another job.\n",
      "17:17:45 job_callback for (0, 0, 6) got condition\n",
      "17:17:45 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:17:45 Only 7 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:17:45 HBMASTER: Trying to run another job!\n",
      "17:17:45 job_callback for (0, 0, 6) finished\n",
      "17:17:45 start sampling a new configuration.\n",
      "17:17:45 done sampling a new configuration.\n",
      "17:17:45 HBMASTER: schedule new run for iteration 0\n",
      "17:17:45 HBMASTER: trying submitting job (0, 0, 7) to dispatcher\n",
      "17:17:45 HBMASTER: submitting job (0, 0, 7) to dispatcher\n",
      "17:17:45 DISPATCHER: trying to submit job (0, 0, 7)\n",
      "17:17:45 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:17:45 HBMASTER: job (0, 0, 7) submitted to dispatcher\n",
      "17:17:45 DISPATCHER: Trying to submit another job.\n",
      "17:17:45 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:17:45 DISPATCHER: starting job (0, 0, 7) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:17:45 DISPATCHER: job (0, 0, 7) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:17:45 WORKER: start processing job (0, 0, 7)\n",
      "17:17:45 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:17:45 WORKER: args: ()\n",
      "17:17:45 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 4, 'initial_lr': 0.008893109911191136, 'loss': 'mse', 'numLayers': 37, 'numNeurons': 17, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 10240 \n",
      "Batch size: 10000 \n",
      "Total batches: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:18:12 DISPATCHER: Starting worker discovery\n",
      "17:18:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:18:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 0.004446554929018021.\n",
      "\n",
      "Epoch 00281: ReduceLROnPlateau reducing learning rate to 0.0022232774645090103.\n",
      "\n",
      "Epoch 00315: ReduceLROnPlateau reducing learning rate to 0.0011116387322545052.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:18:54 WORKER: done with job (0, 0, 7), trying to register it.\n",
      "17:18:54 WORKER: registered result for job (0, 0, 7) with dispatcher\n",
      "17:18:54 DISPATCHER: job (0, 0, 7) finished\n",
      "17:18:54 DISPATCHER: register_result: lock acquired\n",
      "17:18:54 DISPATCHER: job (0, 0, 7) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:18:54 job_id: (0, 0, 7)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 4, 'initial_lr': 0.008893109911191136, 'loss': 'mse', 'numLayers': 37, 'numNeurons': 17, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 772.1366245728486, 'info': {'L1': 772.1366245728486, 'L2': 3888.8711286633306, 'MAX': 7.900786630487129, 'TrainTime': 85.71875}}\n",
      "exception: None\n",
      "\n",
      "17:18:54 job_callback for (0, 0, 7) started\n",
      "17:18:54 DISPATCHER: Trying to submit another job.\n",
      "17:18:54 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:18:54 job_callback for (0, 0, 7) got condition\n",
      "17:18:54 Only 8 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "17:18:54 HBMASTER: Trying to run another job!\n",
      "17:18:54 job_callback for (0, 0, 7) finished\n",
      "17:18:54 start sampling a new configuration.\n",
      "17:18:54 done sampling a new configuration.\n",
      "17:18:54 HBMASTER: schedule new run for iteration 0\n",
      "17:18:54 HBMASTER: trying submitting job (0, 0, 8) to dispatcher\n",
      "17:18:54 HBMASTER: submitting job (0, 0, 8) to dispatcher\n",
      "17:18:54 DISPATCHER: trying to submit job (0, 0, 8)\n",
      "17:18:54 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:18:54 HBMASTER: job (0, 0, 8) submitted to dispatcher\n",
      "17:18:54 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:18:54 DISPATCHER: Trying to submit another job.\n",
      "17:18:54 DISPATCHER: starting job (0, 0, 8) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:18:54 DISPATCHER: job (0, 0, 8) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:18:54 WORKER: start processing job (0, 0, 8)\n",
      "17:18:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:18:54 WORKER: args: ()\n",
      "17:18:54 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.0030593934225881625, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 85, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 5000 \n",
      "Total batches: 17 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:19:12 DISPATCHER: Starting worker discovery\n",
      "17:19:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:19:12 DISPATCHER: Finished worker discovery\n",
      "17:20:12 DISPATCHER: Starting worker discovery\n",
      "17:20:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:20:12 DISPATCHER: Finished worker discovery\n",
      "17:21:12 DISPATCHER: Starting worker discovery\n",
      "17:21:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:21:12 DISPATCHER: Finished worker discovery\n",
      "17:22:12 DISPATCHER: Starting worker discovery\n",
      "17:22:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:22:12 DISPATCHER: Finished worker discovery\n",
      "17:23:12 DISPATCHER: Starting worker discovery\n",
      "17:23:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:23:12 DISPATCHER: Finished worker discovery\n",
      "17:24:12 DISPATCHER: Starting worker discovery\n",
      "17:24:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:24:12 DISPATCHER: Finished worker discovery\n",
      "17:25:12 DISPATCHER: Starting worker discovery\n",
      "17:25:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:25:12 DISPATCHER: Finished worker discovery\n",
      "17:26:12 DISPATCHER: Starting worker discovery\n",
      "17:26:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:26:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00263: ReduceLROnPlateau reducing learning rate to 0.0015296967467293143.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:27:12 DISPATCHER: Starting worker discovery\n",
      "17:27:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:27:12 DISPATCHER: Finished worker discovery\n",
      "17:28:12 DISPATCHER: Starting worker discovery\n",
      "17:28:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:28:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00306: ReduceLROnPlateau reducing learning rate to 0.0007648483733646572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:29:12 DISPATCHER: Starting worker discovery\n",
      "17:29:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:29:12 DISPATCHER: Finished worker discovery\n",
      "17:29:17 WORKER: done with job (0, 0, 8), trying to register it.\n",
      "17:29:17 WORKER: registered result for job (0, 0, 8) with dispatcher\n",
      "17:29:17 DISPATCHER: job (0, 0, 8) finished\n",
      "17:29:17 DISPATCHER: register_result: lock acquired\n",
      "17:29:17 DISPATCHER: job (0, 0, 8) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:29:17 job_id: (0, 0, 8)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.0030593934225881625, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 85, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 207.42174434552877, 'info': {'L1': 207.42174434552877, 'L2': 352.27832613461993, 'MAX': 4.1635836732146, 'TrainTime': 941.96875}}\n",
      "exception: None\n",
      "\n",
      "17:29:17 job_callback for (0, 0, 8) started\n",
      "17:29:17 DISPATCHER: Trying to submit another job.\n",
      "17:29:17 job_callback for (0, 0, 8) got condition\n",
      "17:29:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:29:17 HBMASTER: Trying to run another job!\n",
      "17:29:17 job_callback for (0, 0, 8) finished\n",
      "17:29:17 ITERATION: Advancing config (0, 0, 0) to next budget 1000.000000\n",
      "17:29:17 ITERATION: Advancing config (0, 0, 2) to next budget 1000.000000\n",
      "17:29:17 ITERATION: Advancing config (0, 0, 8) to next budget 1000.000000\n",
      "17:29:17 HBMASTER: schedule new run for iteration 0\n",
      "17:29:17 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "17:29:17 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "17:29:17 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "17:29:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:29:17 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "17:29:17 DISPATCHER: Trying to submit another job.\n",
      "17:29:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:29:17 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:29:17 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:29:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:29:17 WORKER: start processing job (0, 0, 0)\n",
      "17:29:17 WORKER: args: ()\n",
      "17:29:17 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0011225914254538842, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 4320 \n",
      "Batch size: 4320 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:30:12 DISPATCHER: Starting worker discovery\n",
      "17:30:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:30:12 DISPATCHER: Finished worker discovery\n",
      "17:31:12 DISPATCHER: Starting worker discovery\n",
      "17:31:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:31:12 DISPATCHER: Finished worker discovery\n",
      "17:31:23 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "17:31:23 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "17:31:23 DISPATCHER: job (0, 0, 0) finished\n",
      "17:31:23 DISPATCHER: register_result: lock acquired\n",
      "17:31:23 DISPATCHER: job (0, 0, 0) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:31:23 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0011225914254538842, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 174.1731902036674, 'info': {'L1': 174.1731902036674, 'L2': 253.14989159659103, 'MAX': 2.4782243417375485, 'TrainTime': 137.375}}\n",
      "exception: None\n",
      "\n",
      "17:31:23 job_callback for (0, 0, 0) started\n",
      "17:31:23 job_callback for (0, 0, 0) got condition\n",
      "17:31:23 DISPATCHER: Trying to submit another job.\n",
      "17:31:23 Only 1 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "17:31:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:31:23 HBMASTER: Trying to run another job!\n",
      "17:31:23 job_callback for (0, 0, 0) finished\n",
      "17:31:23 HBMASTER: schedule new run for iteration 0\n",
      "17:31:23 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "17:31:23 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "17:31:23 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "17:31:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:31:23 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "17:31:23 DISPATCHER: Trying to submit another job.\n",
      "17:31:23 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:31:23 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:31:23 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:31:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:31:23 WORKER: start processing job (0, 0, 2)\n",
      "17:31:23 WORKER: args: ()\n",
      "17:31:23 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 10240 \n",
      "Batch size: 5000 \n",
      "Total batches: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:32:12 DISPATCHER: Starting worker discovery\n",
      "17:32:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:32:12 DISPATCHER: Finished worker discovery\n",
      "17:33:12 DISPATCHER: Starting worker discovery\n",
      "17:33:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:33:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00536: ReduceLROnPlateau reducing learning rate to 0.0036754983011633158.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:34:12 DISPATCHER: Starting worker discovery\n",
      "17:34:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:34:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00748: ReduceLROnPlateau reducing learning rate to 0.0018377491505816579.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:35:12 DISPATCHER: Starting worker discovery\n",
      "17:35:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:35:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00960: ReduceLROnPlateau reducing learning rate to 0.0009188745752908289.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:35:54 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "17:35:54 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "17:35:54 DISPATCHER: job (0, 0, 2) finished\n",
      "17:35:54 DISPATCHER: register_result: lock acquired\n",
      "17:35:54 DISPATCHER: job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "17:35:54 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 109.05497269166305, 'info': {'L1': 109.05497269166305, 'L2': 164.89220790414382, 'MAX': 4.750469999634619, 'TrainTime': 404.921875}}\n",
      "exception: None\n",
      "\n",
      "17:35:54 job_callback for (0, 0, 2) started\n",
      "17:35:54 DISPATCHER: Trying to submit another job.\n",
      "17:35:54 job_callback for (0, 0, 2) got condition\n",
      "17:35:54 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "17:35:54 Only 2 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "17:35:54 HBMASTER: Trying to run another job!\n",
      "17:35:54 job_callback for (0, 0, 2) finished\n",
      "17:35:54 HBMASTER: schedule new run for iteration 0\n",
      "17:35:54 HBMASTER: trying submitting job (0, 0, 8) to dispatcher\n",
      "17:35:54 HBMASTER: submitting job (0, 0, 8) to dispatcher\n",
      "17:35:54 DISPATCHER: trying to submit job (0, 0, 8)\n",
      "17:35:54 DISPATCHER: trying to notify the job_runner thread.\n",
      "17:35:54 HBMASTER: job (0, 0, 8) submitted to dispatcher\n",
      "17:35:54 DISPATCHER: Trying to submit another job.\n",
      "17:35:54 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "17:35:54 DISPATCHER: starting job (0, 0, 8) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:35:54 DISPATCHER: job (0, 0, 8) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "17:35:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "17:35:54 WORKER: start processing job (0, 0, 8)\n",
      "17:35:54 WORKER: args: ()\n",
      "17:35:54 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.0030593934225881625, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 85, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 5000 \n",
      "Total batches: 17 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:36:12 DISPATCHER: Starting worker discovery\n",
      "17:36:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:36:12 DISPATCHER: Finished worker discovery\n",
      "17:37:12 DISPATCHER: Starting worker discovery\n",
      "17:37:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:37:12 DISPATCHER: Finished worker discovery\n",
      "17:38:12 DISPATCHER: Starting worker discovery\n",
      "17:38:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:38:12 DISPATCHER: Finished worker discovery\n",
      "17:39:12 DISPATCHER: Starting worker discovery\n",
      "17:39:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:39:12 DISPATCHER: Finished worker discovery\n",
      "17:40:12 DISPATCHER: Starting worker discovery\n",
      "17:40:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:40:12 DISPATCHER: Finished worker discovery\n",
      "17:41:12 DISPATCHER: Starting worker discovery\n",
      "17:41:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:41:12 DISPATCHER: Finished worker discovery\n",
      "17:42:12 DISPATCHER: Starting worker discovery\n",
      "17:42:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:42:12 DISPATCHER: Finished worker discovery\n",
      "17:43:12 DISPATCHER: Starting worker discovery\n",
      "17:43:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:43:12 DISPATCHER: Finished worker discovery\n",
      "17:44:12 DISPATCHER: Starting worker discovery\n",
      "17:44:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:44:12 DISPATCHER: Finished worker discovery\n",
      "17:45:12 DISPATCHER: Starting worker discovery\n",
      "17:45:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:45:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00317: ReduceLROnPlateau reducing learning rate to 0.0015296967467293143.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:46:12 DISPATCHER: Starting worker discovery\n",
      "17:46:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:46:12 DISPATCHER: Finished worker discovery\n",
      "17:47:12 DISPATCHER: Starting worker discovery\n",
      "17:47:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:47:12 DISPATCHER: Finished worker discovery\n",
      "17:48:12 DISPATCHER: Starting worker discovery\n",
      "17:48:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:48:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00417: ReduceLROnPlateau reducing learning rate to 0.0007648483733646572.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:49:12 DISPATCHER: Starting worker discovery\n",
      "17:49:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:49:12 DISPATCHER: Finished worker discovery\n",
      "17:50:12 DISPATCHER: Starting worker discovery\n",
      "17:50:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:50:12 DISPATCHER: Finished worker discovery\n",
      "17:51:12 DISPATCHER: Starting worker discovery\n",
      "17:51:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:51:12 DISPATCHER: Finished worker discovery\n",
      "17:52:12 DISPATCHER: Starting worker discovery\n",
      "17:52:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:52:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00554: ReduceLROnPlateau reducing learning rate to 0.0003824241866823286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:53:12 DISPATCHER: Starting worker discovery\n",
      "17:53:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:53:12 DISPATCHER: Finished worker discovery\n",
      "17:54:12 DISPATCHER: Starting worker discovery\n",
      "17:54:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:54:12 DISPATCHER: Finished worker discovery\n",
      "17:55:12 DISPATCHER: Starting worker discovery\n",
      "17:55:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:55:12 DISPATCHER: Finished worker discovery\n",
      "17:56:12 DISPATCHER: Starting worker discovery\n",
      "17:56:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:56:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00670: ReduceLROnPlateau reducing learning rate to 0.0001912120933411643.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:57:12 DISPATCHER: Starting worker discovery\n",
      "17:57:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:57:12 DISPATCHER: Finished worker discovery\n",
      "17:58:12 DISPATCHER: Starting worker discovery\n",
      "17:58:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:58:12 DISPATCHER: Finished worker discovery\n",
      "17:59:12 DISPATCHER: Starting worker discovery\n",
      "17:59:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "17:59:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00770: ReduceLROnPlateau reducing learning rate to 9.560604667058215e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:00:12 DISPATCHER: Starting worker discovery\n",
      "18:00:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:00:12 DISPATCHER: Finished worker discovery\n",
      "18:01:12 DISPATCHER: Starting worker discovery\n",
      "18:01:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:01:12 DISPATCHER: Finished worker discovery\n",
      "18:02:12 DISPATCHER: Starting worker discovery\n",
      "18:02:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:02:12 DISPATCHER: Finished worker discovery\n",
      "18:03:12 DISPATCHER: Starting worker discovery\n",
      "18:03:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:03:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00921: ReduceLROnPlateau reducing learning rate to 4.780302333529107e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:04:12 DISPATCHER: Starting worker discovery\n",
      "18:04:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:04:12 DISPATCHER: Finished worker discovery\n",
      "18:05:12 DISPATCHER: Starting worker discovery\n",
      "18:05:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:05:12 DISPATCHER: Finished worker discovery\n",
      "18:06:10 WORKER: done with job (0, 0, 8), trying to register it.\n",
      "18:06:10 WORKER: registered result for job (0, 0, 8) with dispatcher\n",
      "18:06:10 DISPATCHER: job (0, 0, 8) finished\n",
      "18:06:10 DISPATCHER: register_result: lock acquired\n",
      "18:06:10 DISPATCHER: job (0, 0, 8) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "18:06:10 job_id: (0, 0, 8)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.0030593934225881625, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 85, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 207.70178915967617, 'info': {'L1': 207.70178915967617, 'L2': 353.06652837137017, 'MAX': 4.1658438813444825, 'TrainTime': 2799.296875}}\n",
      "exception: None\n",
      "\n",
      "18:06:10 job_callback for (0, 0, 8) started\n",
      "18:06:10 DISPATCHER: Trying to submit another job.\n",
      "18:06:10 job_callback for (0, 0, 8) got condition\n",
      "18:06:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "18:06:10 Only 3 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "18:06:10 HBMASTER: Trying to run another job!\n",
      "18:06:10 job_callback for (0, 0, 8) finished\n",
      "18:06:10 ITERATION: Advancing config (0, 0, 2) to next budget 3000.000000\n",
      "18:06:10 HBMASTER: schedule new run for iteration 0\n",
      "18:06:10 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "18:06:10 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "18:06:10 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "18:06:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "18:06:10 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "18:06:10 DISPATCHER: Trying to submit another job.\n",
      "18:06:10 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "18:06:10 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "18:06:10 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764\n",
      "18:06:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "18:06:10 WORKER: start processing job (0, 0, 2)\n",
      "18:06:10 WORKER: args: ()\n",
      "18:06:10 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "18:06:12 DISPATCHER: Starting worker discovery\n",
      "18:06:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:06:12 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 10240 \n",
      "Batch size: 5000 \n",
      "Total batches: 3 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:07:12 DISPATCHER: Starting worker discovery\n",
      "18:07:12 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:07:13 DISPATCHER: Finished worker discovery\n",
      "18:08:13 DISPATCHER: Starting worker discovery\n",
      "18:08:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:08:13 DISPATCHER: Finished worker discovery\n",
      "18:09:13 DISPATCHER: Starting worker discovery\n",
      "18:09:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:09:13 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00779: ReduceLROnPlateau reducing learning rate to 0.0036754983011633158.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:10:13 DISPATCHER: Starting worker discovery\n",
      "18:10:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:10:13 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01120: ReduceLROnPlateau reducing learning rate to 0.0018377491505816579.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:11:13 DISPATCHER: Starting worker discovery\n",
      "18:11:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:11:13 DISPATCHER: Finished worker discovery\n",
      "18:12:13 DISPATCHER: Starting worker discovery\n",
      "18:12:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:12:13 DISPATCHER: Finished worker discovery\n",
      "18:13:13 DISPATCHER: Starting worker discovery\n",
      "18:13:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:13:13 DISPATCHER: Finished worker discovery\n",
      "18:14:13 DISPATCHER: Starting worker discovery\n",
      "18:14:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:14:13 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01880: ReduceLROnPlateau reducing learning rate to 0.0009188745752908289.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:15:13 DISPATCHER: Starting worker discovery\n",
      "18:15:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:15:13 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02180: ReduceLROnPlateau reducing learning rate to 0.00045943728764541447.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:16:13 DISPATCHER: Starting worker discovery\n",
      "18:16:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:16:13 DISPATCHER: Finished worker discovery\n",
      "18:17:13 DISPATCHER: Starting worker discovery\n",
      "18:17:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:17:13 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02670: ReduceLROnPlateau reducing learning rate to 0.00022971864382270724.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:18:13 DISPATCHER: Starting worker discovery\n",
      "18:18:13 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "18:18:13 DISPATCHER: Finished worker discovery\n",
      "18:19:02 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "18:19:02 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "18:19:02 DISPATCHER: job (0, 0, 2) finished\n",
      "18:19:02 DISPATCHER: register_result: lock acquired\n",
      "18:19:02 DISPATCHER: job (0, 0, 2) on hpbandster.run_plate_Gridsearch_1.worker.DESKTOP-NKTB8UL.2608.19764 finished\n",
      "18:19:02 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.007350996419417513, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 91, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 109.08237774613525, 'info': {'L1': 109.08237774613525, 'L2': 164.9391542913586, 'MAX': 4.751703577362891, 'TrainTime': 1190.703125}}\n",
      "exception: None\n",
      "\n",
      "18:19:02 job_callback for (0, 0, 2) started\n",
      "18:19:02 DISPATCHER: Trying to submit another job.\n",
      "18:19:02 job_callback for (0, 0, 2) got condition\n",
      "18:19:02 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "18:19:02 Only 1 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "18:19:02 HBMASTER: Trying to run another job!\n",
      "18:19:02 job_callback for (0, 0, 2) finished\n"
     ]
    }
   ],
   "source": [
    "bohb = BOHB(  configspace = w.get_configspace(),\n",
    "              run_id = run_id, nameserver='127.0.0.1',\n",
    "              min_budget=250, max_budget=3000\n",
    "           )\n",
    "res = bohb.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e29a746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:19:02 HBMASTER: shutdown initiated, shutdown_workers = True\n",
      "18:19:02 WORKER: shutting down now!\n",
      "18:19:02 DISPATCHER: Dispatcher shutting down\n",
      "18:19:02 DISPATCHER: Trying to submit another job.\n",
      "18:19:02 DISPATCHER: job_runner shutting down\n",
      "18:19:02 DISPATCHER: discover_workers shutting down\n",
      "18:19:02 DISPATCHER: 'discover_worker' thread exited\n",
      "18:19:02 DISPATCHER: 'job_runner' thread exited\n",
      "18:19:02 DISPATCHER: shut down complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# np.save('HpBandster_Results',res,allow_pickle=True)\n",
    "\n",
    "bohb.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc1d99-bcfb-42b2-86c1-30dcd13bdde2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
