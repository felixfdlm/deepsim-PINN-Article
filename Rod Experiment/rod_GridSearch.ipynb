{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979070ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "import hpbandster.visualization as hpvis\n",
    "\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "import pickle\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2a99fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- SCIANN 0.6.4.5 ---------------------- \n",
      "For details, check out our review paper and the documentation at: \n",
      " +  \"https://www.sciencedirect.com/science/article/pii/S0045782520307374\", \n",
      " +  \"https://arxiv.org/abs/2005.08803\", \n",
      " +  \"https://www.sciann.com\". \n",
      "\n",
      " Need support or would like to contribute, please join sciann`s slack group: \n",
      " +  \"https://join.slack.com/t/sciann/shared_invite/zt-ne1f5jlx-k_dY8RGo3ZreDXwz0f~CeA\" \n",
      " \n",
      "TensorFlow Version: 2.3.0 \n",
      "Python Version: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "import System as SEQ\n",
    "%run rod_EQS.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e17c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pinn_Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4973be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gridObj = SEQ.Grid(2,{'x':0,'t':1},[[[0,2],[0,10]]],10)\n",
    "configspecs = {\n",
    "    'denspt':[5,12],\n",
    "    'numNeurons':[10,100],\n",
    "    'numLayers':[2,50],\n",
    "    'activator': ['elu','relu','selu','swish',\n",
    "                 'tanh','Addons>gelu','Addons>mish',\n",
    "                          'Addons>softshrink'],\n",
    "    'loss':['mae','mse'],\n",
    "    'optimizer':['Adam','RMSprop','SGD','Nadam','Ftrl'],\n",
    "    'batch_size':[5000,10000,15000,25000],\n",
    "    'initial_lr':[1e-4,1e-2]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "958646b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_n(n,x,t):\n",
    "  mu_n = (2*n+1)*np.pi/4\n",
    "  T_n =  np.exp(-t*(mu_n**2))\n",
    "  X_n = -np.cos(mu_n*x)*2.5/(mu_n**2)\n",
    "  return X_n*T_n\n",
    "\n",
    "depth = 100\n",
    "\n",
    "u_pred_Simb = (np.sum((np.array([u_n(n,test_gridObj.grid[:,0],test_gridObj.grid[:,1]) for n in range(depth)])),axis=0)+5)[:,None]\n",
    "valData = np.concatenate((test_gridObj.grid,u_pred_Simb),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89a954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x17951b10408; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "13:56:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "13:56:29 WORKER: start listening for jobs\n",
      "13:56:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x17952f70448; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "13:56:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "13:56:29 WORKER: start listening for jobs\n",
      "13:56:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x17952f81b48; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "13:56:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "13:56:29 WORKER: start listening for jobs\n",
      "13:56:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x17952f81688; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "13:56:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "13:56:29 WORKER: start listening for jobs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating client\n",
      "New experiment. Creating instance.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Creating client\n",
      "Existing experiment. Recovering id.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Creating client\n",
      "Existing experiment. Recovering id.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Creating client\n",
      "Existing experiment. Recovering id.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Creating client\n",
      "Existing experiment. Recovering id.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:29 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x17953126ac8; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "13:56:29 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "13:56:29 WORKER: start listening for jobs\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Start a nameserver\n",
    "# Every run needs a nameserver. It could be a 'static' server with a\n",
    "# permanent address, but here it will be started for the local machine with the default port.\n",
    "# The nameserver manages the concurrent running workers across all possible threads or clusternodes.\n",
    "# Note the run_id argument. This uniquely identifies a run of any HpBandSter optimizer.\n",
    "NS = hpns.NameServer(run_id='rod_GridSearch_1', host='127.0.0.1', port=None)\n",
    "NS.start()\n",
    "\n",
    "# Step 2: Start a worker\n",
    "# Now we can instantiate a worker, providing the mandatory information\n",
    "# Besides the sleep_interval, we need to define the nameserver information and\n",
    "# the same run_id as above. After that, we can start the worker in the background,\n",
    "# where it will wait for incoming configurations to evaluate.\n",
    "\n",
    "numWorkers = 5\n",
    "experiment_name = 'rod_experiment'\n",
    "\n",
    "workers=[]\n",
    "for i in range(1,numWorkers+1):\n",
    "    \n",
    "    w = Pinn_Worker.PINN_Worker(\n",
    "    valData = valData,\n",
    "    test_gridObj=test_gridObj,\n",
    "    PDESystem=mySys,\n",
    "    configspecs = configspecs,\n",
    "    valFromFEM=False,\n",
    "    experiment_name = experiment_name,\n",
    "    nameserver='127.0.0.1',\n",
    "    run_id='rod_GridSearch_1',id=i)\n",
    "    \n",
    "    w.run(background=True)\n",
    "    \n",
    "    workers.append(w)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691636e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:29 DISPATCHER: started the 'discover_worker' thread\n",
      "13:56:29 DISPATCHER: started the 'job_runner' thread\n"
     ]
    }
   ],
   "source": [
    "# bohb = BOHB(  configspace = w.get_configspace(),\n",
    "#               run_id = 'example1', nameserver='127.0.0.1',\n",
    "#               min_budget=10, max_budget=1000\n",
    "#            )\n",
    "# res = bohb.run(n_iterations=25\n",
    "#               )\n",
    "\n",
    "bohb = BOHB(  configspace = w.get_configspace(),\n",
    "              run_id = 'rod_GridSearch_1',\n",
    "              min_budget=100, max_budget=3000, num_samples = 10\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae4327e-4b79-4a54-919e-5f5a751ffd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'working_directory': '.',\n",
       " 'logger': <Logger hpbandster (DEBUG)>,\n",
       " 'result_logger': None,\n",
       " 'config_generator': <hpbandster.optimizers.config_generators.bohb.BOHB at 0x1795313f488>,\n",
       " 'time_ref': None,\n",
       " 'iterations': [],\n",
       " 'jobs': [],\n",
       " 'num_running_jobs': 0,\n",
       " 'job_queue_sizes': (-1, 0),\n",
       " 'user_job_queue_sizes': (-1, 0),\n",
       " 'dynamic_queue_size': True,\n",
       " 'warmstart_iteration': [],\n",
       " 'thread_cond': <Condition(<unlocked _thread.RLock object owner=0 count=0 at 0x0000017951AF3630>, 0)>,\n",
       " 'config': {'time_ref': None,\n",
       "  'eta': 3,\n",
       "  'min_budget': 100,\n",
       "  'max_budget': 3000,\n",
       "  'budgets': array([ 111.11111111,  333.33333333, 1000.        , 3000.        ]),\n",
       "  'max_SH_iter': 4,\n",
       "  'min_points_in_model': None,\n",
       "  'top_n_percent': 15,\n",
       "  'num_samples': 10,\n",
       "  'random_fraction': 0.3333333333333333,\n",
       "  'bandwidth_factor': 3,\n",
       "  'min_bandwidth': 0.001},\n",
       " 'dispatcher': <hpbandster.core.dispatcher.Dispatcher at 0x1795313f2c8>,\n",
       " 'dispatcher_thread': <Thread(Thread-30, started 9644)>,\n",
       " 'eta': 3,\n",
       " 'min_budget': 100,\n",
       " 'max_budget': 3000,\n",
       " 'max_SH_iter': 4,\n",
       " 'budgets': array([ 111.11111111,  333.33333333, 1000.        , 3000.        ])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:29 DISPATCHER: Pyro daemon running on localhost:53133\n",
      "13:56:29 DISPATCHER: Starting worker discovery\n",
      "13:56:29 DISPATCHER: Found 5 potential workers, 0 currently in the pool.\n",
      "13:56:29 DISPATCHER: discovered new worker, hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "13:56:29 DISPATCHER: discovered new worker, hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n"
     ]
    }
   ],
   "source": [
    "bohb.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc0ba48f-584b-4b34-b909-362c6223cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:56:29 wait_for_workers trying to get the condition\n",
      "13:56:29 DISPATCHER: discovered new worker, hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "13:56:29 DISPATCHER: discovered new worker, hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:56:29 DISPATCHER: discovered new worker, hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:56:29 HBMASTER: number of workers changed to 5\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 5 -> waiting!\n",
      "13:56:29 Enough workers to start this run!\n",
      "13:56:29 adjust_queue_size: lock accquired\n",
      "13:56:29 HBMASTER: adjusted queue size to (4, 5)\n",
      "13:56:29 DISPATCHER: Finished worker discovery\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 5 -> waiting!\n",
      "13:56:29 HBMASTER: starting run at 1648299389.7113676\n",
      "13:56:29 start sampling a new configuration.\n",
      "13:56:29 done sampling a new configuration.\n",
      "13:56:29 HBMASTER: schedule new run for iteration 0\n",
      "13:56:29 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "13:56:29 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "13:56:29 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "13:56:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:56:29 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 start sampling a new configuration.\n",
      "13:56:29 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "13:56:29 done sampling a new configuration.\n",
      "13:56:29 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 4 -> waiting!\n",
      "13:56:29 WORKER: start processing job (0, 0, 0)\n",
      "13:56:29 HBMASTER: schedule new run for iteration 0\n",
      "13:56:29 WORKER: args: ()\n",
      "13:56:29 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "13:56:29 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.006379384704252637, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 18, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:56:29 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "13:56:29 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "13:56:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:56:29 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 start sampling a new configuration.\n",
      "13:56:29 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:56:29 done sampling a new configuration.\n",
      "13:56:29 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:56:29 HBMASTER: schedule new run for iteration 0\n",
      "13:56:29 WORKER: start processing job (0, 0, 1)\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 3 -> waiting!\n",
      "13:56:29 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "13:56:29 WORKER: args: ()\n",
      "13:56:29 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0006913809895965321, 'loss': 'mae', 'numLayers': 39, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:56:29 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "13:56:29 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "13:56:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:56:29 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 start sampling a new configuration.\n",
      "13:56:29 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:56:29 done sampling a new configuration.\n",
      "13:56:29 HBMASTER: schedule new run for iteration 0\n",
      "13:56:29 HBMASTER: trying submitting job (0, 0, 3) to dispatcher\n",
      "13:56:29 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:56:29 HBMASTER: submitting job (0, 0, 3) to dispatcher\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 2 -> waiting!\n",
      "13:56:29 WORKER: start processing job (0, 0, 2)\n",
      "13:56:29 DISPATCHER: trying to submit job (0, 0, 3)\n",
      "13:56:29 WORKER: args: ()\n",
      "13:56:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:56:29 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.003987253991785093, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 56, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:56:29 HBMASTER: job (0, 0, 3) submitted to dispatcher\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 start sampling a new configuration.\n",
      "13:56:29 DISPATCHER: starting job (0, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "13:56:29 done sampling a new configuration.\n",
      "13:56:29 DISPATCHER: job (0, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "13:56:29 WORKER: start processing job (0, 0, 3)\n",
      "13:56:29 HBMASTER: schedule new run for iteration 0\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:56:29 WORKER: args: ()\n",
      "13:56:29 HBMASTER: trying submitting job (0, 0, 4) to dispatcher\n",
      "13:56:29 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.0022788780286384027, 'loss': 'mse', 'numLayers': 24, 'numNeurons': 17, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:56:29 HBMASTER: submitting job (0, 0, 4) to dispatcher\n",
      "13:56:29 DISPATCHER: trying to submit job (0, 0, 4)\n",
      "13:56:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:56:29 HBMASTER: job (0, 0, 4) submitted to dispatcher\n",
      "13:56:29 DISPATCHER: Trying to submit another job.\n",
      "13:56:29 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:56:29 DISPATCHER: starting job (0, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:56:29 DISPATCHER: job (0, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:56:29 WORKER: start processing job (0, 0, 4)\n",
      "13:56:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:56:29 WORKER: args: ()\n",
      "13:56:29 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008237092039893007, 'loss': 'mse', 'numLayers': 20, 'numNeurons': 52, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:57:29 DISPATCHER: Starting worker discovery\n",
      "13:57:29 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "13:57:29 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.004118545912206173.\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0020592729561030865.\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0010296364780515432.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:57:49 WORKER: done with job (0, 0, 4), trying to register it.\n",
      "13:57:49 DISPATCHER: job (0, 0, 4) finished\n",
      "13:57:49 WORKER: registered result for job (0, 0, 4) with dispatcher\n",
      "13:57:49 DISPATCHER: register_result: lock acquired\n",
      "13:57:49 DISPATCHER: job (0, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "13:57:49 job_id: (0, 0, 4)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008237092039893007, 'loss': 'mse', 'numLayers': 20, 'numNeurons': 52, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 5.912182831454428, 'info': {'L1': 5.912182831454428, 'L2': 2.62150661222862, 'MAX': 0.8229426113219738, 'TrainTime': 129.609375}}\n",
      "exception: None\n",
      "\n",
      "13:57:49 job_callback for (0, 0, 4) started\n",
      "13:57:49 DISPATCHER: Trying to submit another job.\n",
      "13:57:49 job_callback for (0, 0, 4) got condition\n",
      "13:57:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:57:49 Only 1 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:57:49 HBMASTER: Trying to run another job!\n",
      "13:57:49 job_callback for (0, 0, 4) finished\n",
      "13:57:49 start sampling a new configuration.\n",
      "13:57:49 done sampling a new configuration.\n",
      "13:57:49 HBMASTER: schedule new run for iteration 0\n",
      "13:57:49 HBMASTER: trying submitting job (0, 0, 5) to dispatcher\n",
      "13:57:49 HBMASTER: submitting job (0, 0, 5) to dispatcher\n",
      "13:57:49 DISPATCHER: trying to submit job (0, 0, 5)\n",
      "13:57:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:57:49 HBMASTER: job (0, 0, 5) submitted to dispatcher\n",
      "13:57:49 DISPATCHER: Trying to submit another job.\n",
      "13:57:49 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:57:49 DISPATCHER: starting job (0, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:57:49 DISPATCHER: job (0, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:57:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:57:49 WORKER: start processing job (0, 0, 5)\n",
      "13:57:49 WORKER: args: ()\n",
      "13:57:49 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.009806883053223364, 'loss': 'mae', 'numLayers': 10, 'numNeurons': 79, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:57:54 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "13:57:54 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "13:57:54 DISPATCHER: job (0, 0, 2) finished\n",
      "13:57:54 DISPATCHER: register_result: lock acquired\n",
      "13:57:54 DISPATCHER: job (0, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "13:57:54 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.003987253991785093, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 56, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 20.782268514681146, 'info': {'L1': 20.782268514681146, 'L2': 24.404664515400917, 'MAX': 3.738836090456781, 'TrainTime': 144.671875}}\n",
      "exception: None\n",
      "\n",
      "13:57:54 job_callback for (0, 0, 2) started\n",
      "13:57:54 DISPATCHER: Trying to submit another job.\n",
      "13:57:54 job_callback for (0, 0, 2) got condition\n",
      "13:57:54 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:57:54 Only 2 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:57:54 HBMASTER: Trying to run another job!\n",
      "13:57:54 job_callback for (0, 0, 2) finished\n",
      "13:57:54 start sampling a new configuration.\n",
      "13:57:54 done sampling a new configuration.\n",
      "13:57:54 HBMASTER: schedule new run for iteration 0\n",
      "13:57:54 HBMASTER: trying submitting job (0, 0, 6) to dispatcher\n",
      "13:57:54 HBMASTER: submitting job (0, 0, 6) to dispatcher\n",
      "13:57:54 DISPATCHER: trying to submit job (0, 0, 6)\n",
      "13:57:54 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:57:54 HBMASTER: job (0, 0, 6) submitted to dispatcher\n",
      "13:57:54 DISPATCHER: Trying to submit another job.\n",
      "13:57:54 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:57:54 DISPATCHER: starting job (0, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:57:54 DISPATCHER: job (0, 0, 6) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:57:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:57:54 WORKER: start processing job (0, 0, 6)\n",
      "13:57:54 WORKER: args: ()\n",
      "13:57:54 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.002070408644608349, 'loss': 'mae', 'numLayers': 26, 'numNeurons': 38, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00034569049603305757.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00017284524801652879.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.642262400826439e-05.\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 4.3211312004132196e-05.\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 2.1605656002066098e-05.\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 1.0802828001033049e-05.\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 5.4014140005165245e-06.\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 2.7007070002582623e-06.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 1.3503535001291311e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:58:23 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "13:58:23 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "13:58:23 DISPATCHER: job (0, 0, 1) finished\n",
      "13:58:23 DISPATCHER: register_result: lock acquired\n",
      "13:58:23 DISPATCHER: job (0, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "13:58:23 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0006913809895965321, 'loss': 'mae', 'numLayers': 39, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 91.59942231024793, 'info': {'L1': 91.59942231024793, 'L2': 430.1142222592387, 'MAX': 4.999944818038784, 'TrainTime': 207.34375}}\n",
      "exception: None\n",
      "\n",
      "13:58:23 job_callback for (0, 0, 1) started\n",
      "13:58:23 DISPATCHER: Trying to submit another job.\n",
      "13:58:23 job_callback for (0, 0, 1) got condition\n",
      "13:58:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:58:23 Only 3 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:58:23 HBMASTER: Trying to run another job!\n",
      "13:58:23 job_callback for (0, 0, 1) finished\n",
      "13:58:23 start sampling a new configuration.\n",
      "13:58:23 done sampling a new configuration.\n",
      "13:58:23 HBMASTER: schedule new run for iteration 0\n",
      "13:58:23 HBMASTER: trying submitting job (0, 0, 7) to dispatcher\n",
      "13:58:23 HBMASTER: submitting job (0, 0, 7) to dispatcher\n",
      "13:58:23 DISPATCHER: trying to submit job (0, 0, 7)\n",
      "13:58:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:58:23 HBMASTER: job (0, 0, 7) submitted to dispatcher\n",
      "13:58:23 DISPATCHER: Trying to submit another job.\n",
      "13:58:23 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:58:23 DISPATCHER: starting job (0, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:58:23 DISPATCHER: job (0, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:58:23 WORKER: start processing job (0, 0, 7)\n",
      "13:58:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:58:23 WORKER: args: ()\n",
      "13:58:23 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.009975228544768665, 'loss': 'mae', 'numLayers': 46, 'numNeurons': 100, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.004903441295027733.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:58:29 DISPATCHER: Starting worker discovery\n",
      "13:58:29 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "13:58:29 DISPATCHER: Finished worker discovery\n",
      "13:58:33 WORKER: done with job (0, 0, 5), trying to register it.\n",
      "13:58:33 WORKER: registered result for job (0, 0, 5) with dispatcher\n",
      "13:58:33 DISPATCHER: job (0, 0, 5) finished\n",
      "13:58:33 DISPATCHER: register_result: lock acquired\n",
      "13:58:33 DISPATCHER: job (0, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "13:58:33 job_id: (0, 0, 5)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.009806883053223364, 'loss': 'mae', 'numLayers': 10, 'numNeurons': 79, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.494726421481198, 'info': {'L1': 8.494726421481198, 'L2': 3.759975436021218, 'MAX': 1.4509718822353452, 'TrainTime': 97.8125}}\n",
      "exception: None\n",
      "\n",
      "13:58:33 job_callback for (0, 0, 5) started\n",
      "13:58:33 job_callback for (0, 0, 5) got condition\n",
      "13:58:33 DISPATCHER: Trying to submit another job.\n",
      "13:58:33 Only 4 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:58:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:58:33 HBMASTER: Trying to run another job!\n",
      "13:58:33 job_callback for (0, 0, 5) finished\n",
      "13:58:33 start sampling a new configuration.\n",
      "13:58:33 done sampling a new configuration.\n",
      "13:58:33 HBMASTER: schedule new run for iteration 0\n",
      "13:58:33 HBMASTER: trying submitting job (0, 0, 8) to dispatcher\n",
      "13:58:33 HBMASTER: submitting job (0, 0, 8) to dispatcher\n",
      "13:58:33 DISPATCHER: trying to submit job (0, 0, 8)\n",
      "13:58:33 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:58:33 HBMASTER: job (0, 0, 8) submitted to dispatcher\n",
      "13:58:33 DISPATCHER: Trying to submit another job.\n",
      "13:58:33 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:58:33 DISPATCHER: starting job (0, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:58:33 DISPATCHER: job (0, 0, 8) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:58:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:58:33 WORKER: start processing job (0, 0, 8)\n",
      "13:58:33 WORKER: args: ()\n",
      "13:58:33 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.009728304207848627, 'loss': 'mse', 'numLayers': 41, 'numNeurons': 54, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.004864152055233717.\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024320760276168585.\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0012160380138084292.\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0006080190069042146.\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0003040095034521073.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:12 WORKER: done with job (0, 0, 6), trying to register it.\n",
      "13:59:12 WORKER: registered result for job (0, 0, 6) with dispatcher\n",
      "13:59:12 DISPATCHER: job (0, 0, 6) finished\n",
      "13:59:12 DISPATCHER: register_result: lock acquired\n",
      "13:59:12 DISPATCHER: job (0, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "13:59:12 job_id: (0, 0, 6)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.002070408644608349, 'loss': 'mae', 'numLayers': 26, 'numNeurons': 38, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 76.36012613905267, 'info': {'L1': 76.36012613905267, 'L2': 301.90731994000214, 'MAX': 4.236489176750183, 'TrainTime': 198.0}}\n",
      "exception: None\n",
      "\n",
      "13:59:12 job_callback for (0, 0, 6) started\n",
      "13:59:12 DISPATCHER: Trying to submit another job.\n",
      "13:59:12 job_callback for (0, 0, 6) got condition\n",
      "13:59:12 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:12 Only 5 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:59:12 HBMASTER: Trying to run another job!\n",
      "13:59:12 job_callback for (0, 0, 6) finished\n",
      "13:59:12 start sampling a new configuration.\n",
      "13:59:12 done sampling a new configuration.\n",
      "13:59:12 HBMASTER: schedule new run for iteration 0\n",
      "13:59:12 HBMASTER: trying submitting job (0, 0, 9) to dispatcher\n",
      "13:59:12 HBMASTER: submitting job (0, 0, 9) to dispatcher\n",
      "13:59:12 DISPATCHER: trying to submit job (0, 0, 9)\n",
      "13:59:12 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:12 HBMASTER: job (0, 0, 9) submitted to dispatcher\n",
      "13:59:12 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:12 DISPATCHER: Trying to submit another job.\n",
      "13:59:12 DISPATCHER: starting job (0, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:59:12 DISPATCHER: job (0, 0, 9) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:59:12 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:12 WORKER: start processing job (0, 0, 9)\n",
      "13:59:12 WORKER: args: ()\n",
      "13:59:12 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0028890742802121614, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 97, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.00015200475172605366.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 7.600237586302683e-05.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 3.8001187931513414e-05.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.003189692273736.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:21 WORKER: done with job (0, 0, 8), trying to register it.\n",
      "13:59:21 WORKER: registered result for job (0, 0, 8) with dispatcher\n",
      "13:59:21 DISPATCHER: job (0, 0, 8) finished\n",
      "13:59:21 DISPATCHER: register_result: lock acquired\n",
      "13:59:21 DISPATCHER: job (0, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "13:59:21 job_id: (0, 0, 8)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.009728304207848627, 'loss': 'mse', 'numLayers': 41, 'numNeurons': 54, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 20.595806089577344, 'info': {'L1': 20.595806089577344, 'L2': 23.836933156199954, 'MAX': 3.665932218920526, 'TrainTime': 131.296875}}\n",
      "exception: None\n",
      "\n",
      "13:59:21 job_callback for (0, 0, 8) started\n",
      "13:59:21 job_callback for (0, 0, 8) got condition\n",
      "13:59:21 DISPATCHER: Trying to submit another job.\n",
      "13:59:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:21 Only 6 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:59:21 HBMASTER: Trying to run another job!\n",
      "13:59:21 job_callback for (0, 0, 8) finished\n",
      "13:59:21 start sampling a new configuration.\n",
      "13:59:21 done sampling a new configuration.\n",
      "13:59:21 HBMASTER: schedule new run for iteration 0\n",
      "13:59:21 HBMASTER: trying submitting job (0, 0, 10) to dispatcher\n",
      "13:59:21 HBMASTER: submitting job (0, 0, 10) to dispatcher\n",
      "13:59:21 DISPATCHER: trying to submit job (0, 0, 10)\n",
      "13:59:21 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:21 HBMASTER: job (0, 0, 10) submitted to dispatcher\n",
      "13:59:21 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:21 DISPATCHER: Trying to submit another job.\n",
      "13:59:21 DISPATCHER: starting job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:59:21 DISPATCHER: job (0, 0, 10) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "13:59:21 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:21 WORKER: start processing job (0, 0, 10)\n",
      "13:59:21 WORKER: args: ()\n",
      "13:59:21 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:29 DISPATCHER: Starting worker discovery\n",
      "13:59:29 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "13:59:29 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.001444537192583084.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:44 WORKER: done with job (0, 0, 3), trying to register it.\n",
      "13:59:44 WORKER: registered result for job (0, 0, 3) with dispatcher\n",
      "13:59:44 DISPATCHER: job (0, 0, 3) finished\n",
      "13:59:44 DISPATCHER: register_result: lock acquired\n",
      "13:59:44 DISPATCHER: job (0, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "13:59:44 job_id: (0, 0, 3)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.0022788780286384027, 'loss': 'mse', 'numLayers': 24, 'numNeurons': 17, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 90.99659443498938, 'info': {'L1': 90.99659443498938, 'L2': 424.6068685618372, 'MAX': 4.969783339649439, 'TrainTime': 429.78125}}\n",
      "exception: None\n",
      "\n",
      "13:59:44 job_callback for (0, 0, 3) started\n",
      "13:59:44 DISPATCHER: Trying to submit another job.\n",
      "13:59:44 job_callback for (0, 0, 3) got condition\n",
      "13:59:44 Only 7 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:59:44 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:44 HBMASTER: Trying to run another job!\n",
      "13:59:44 job_callback for (0, 0, 3) finished\n",
      "13:59:44 start sampling a new configuration.\n",
      "13:59:44 done sampling a new configuration.\n",
      "13:59:44 HBMASTER: schedule new run for iteration 0\n",
      "13:59:44 HBMASTER: trying submitting job (0, 0, 11) to dispatcher\n",
      "13:59:44 HBMASTER: submitting job (0, 0, 11) to dispatcher\n",
      "13:59:44 DISPATCHER: trying to submit job (0, 0, 11)\n",
      "13:59:44 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:44 HBMASTER: job (0, 0, 11) submitted to dispatcher\n",
      "13:59:44 DISPATCHER: Trying to submit another job.\n",
      "13:59:44 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:44 DISPATCHER: starting job (0, 0, 11) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "13:59:44 DISPATCHER: job (0, 0, 11) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "13:59:44 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:44 WORKER: start processing job (0, 0, 11)\n",
      "13:59:44 WORKER: args: ()\n",
      "13:59:44 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.005740801613874142, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 52, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.000722268596291542.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:47 WORKER: done with job (0, 0, 7), trying to register it.\n",
      "13:59:47 WORKER: registered result for job (0, 0, 7) with dispatcher\n",
      "13:59:47 DISPATCHER: job (0, 0, 7) finished\n",
      "13:59:47 DISPATCHER: register_result: lock acquired\n",
      "13:59:47 DISPATCHER: job (0, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "13:59:47 job_id: (0, 0, 7)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.009975228544768665, 'loss': 'mae', 'numLayers': 46, 'numNeurons': 100, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 87.97240925112158, 'info': {'L1': 87.97240925112158, 'L2': 397.51858903816066, 'MAX': 4.818422719836235, 'TrainTime': 202.890625}}\n",
      "exception: None\n",
      "\n",
      "13:59:47 job_callback for (0, 0, 7) started\n",
      "13:59:47 DISPATCHER: Trying to submit another job.\n",
      "13:59:47 job_callback for (0, 0, 7) got condition\n",
      "13:59:47 Only 8 run(s) for budget 111.111111 available, need more than 10 -> can't build model!\n",
      "13:59:47 HBMASTER: Trying to run another job!\n",
      "13:59:47 job_callback for (0, 0, 7) finished\n",
      "13:59:47 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:47 start sampling a new configuration.\n",
      "13:59:47 done sampling a new configuration.\n",
      "13:59:47 HBMASTER: schedule new run for iteration 0\n",
      "13:59:47 HBMASTER: trying submitting job (0, 0, 12) to dispatcher\n",
      "13:59:47 HBMASTER: submitting job (0, 0, 12) to dispatcher\n",
      "13:59:47 DISPATCHER: trying to submit job (0, 0, 12)\n",
      "13:59:47 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:47 HBMASTER: job (0, 0, 12) submitted to dispatcher\n",
      "13:59:47 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:47 DISPATCHER: Trying to submit another job.\n",
      "13:59:47 DISPATCHER: starting job (0, 0, 12) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:59:47 DISPATCHER: job (0, 0, 12) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "13:59:47 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:47 WORKER: start processing job (0, 0, 12)\n",
      "13:59:47 WORKER: args: ()\n",
      "13:59:47 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0055546581014190724, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 93, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.001594846136868.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.000361134298145771.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0001805671490728855.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:59:56 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "13:59:56 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "13:59:56 DISPATCHER: job (0, 0, 0) finished\n",
      "13:59:56 DISPATCHER: register_result: lock acquired\n",
      "13:59:56 DISPATCHER: job (0, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "13:59:56 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.006379384704252637, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 18, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 7.5860812892975105, 'info': {'L1': 7.5860812892975105, 'L2': 4.035492592512771, 'MAX': 1.219363191761195, 'TrainTime': 478.3125}}\n",
      "exception: None\n",
      "\n",
      "13:59:56 job_callback for (0, 0, 0) started\n",
      "13:59:56 job_callback for (0, 0, 0) got condition\n",
      "13:59:56 DISPATCHER: Trying to submit another job.\n",
      "13:59:56 HBMASTER: Trying to run another job!\n",
      "13:59:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:56 job_callback for (0, 0, 0) finished\n",
      "13:59:56 start sampling a new configuration.\n",
      "13:59:56 done sampling a new configuration.\n",
      "13:59:56 HBMASTER: schedule new run for iteration 0\n",
      "13:59:56 HBMASTER: trying submitting job (0, 0, 13) to dispatcher\n",
      "13:59:56 HBMASTER: submitting job (0, 0, 13) to dispatcher\n",
      "13:59:56 DISPATCHER: trying to submit job (0, 0, 13)\n",
      "13:59:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:56 HBMASTER: job (0, 0, 13) submitted to dispatcher\n",
      "13:59:56 DISPATCHER: Trying to submit another job.\n",
      "13:59:56 DISPATCHER: starting job (0, 0, 13) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "13:59:56 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:56 DISPATCHER: job (0, 0, 13) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "13:59:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:56 WORKER: start processing job (0, 0, 13)\n",
      "13:59:56 WORKER: args: ()\n",
      "13:59:56 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 10, 'initial_lr': 0.009770508096866717, 'loss': 'mse', 'numLayers': 16, 'numNeurons': 96, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "13:59:59 WORKER: done with job (0, 0, 9), trying to register it.\n",
      "13:59:59 WORKER: registered result for job (0, 0, 9) with dispatcher\n",
      "13:59:59 DISPATCHER: job (0, 0, 9) finished\n",
      "13:59:59 DISPATCHER: register_result: lock acquired\n",
      "13:59:59 DISPATCHER: job (0, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "13:59:59 job_id: (0, 0, 9)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0028890742802121614, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 97, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 7.671647364746313, 'info': {'L1': 7.671647364746313, 'L2': 8.849910609202773, 'MAX': 2.7101916669042105, 'TrainTime': 142.671875}}\n",
      "exception: None\n",
      "\n",
      "13:59:59 job_callback for (0, 0, 9) started\n",
      "13:59:59 DISPATCHER: Trying to submit another job.\n",
      "13:59:59 job_callback for (0, 0, 9) got condition\n",
      "13:59:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "13:59:59 HBMASTER: Trying to run another job!\n",
      "13:59:59 job_callback for (0, 0, 9) finished\n",
      "13:59:59 start sampling a new configuration.\n",
      "13:59:59 done sampling a new configuration.\n",
      "13:59:59 HBMASTER: schedule new run for iteration 0\n",
      "13:59:59 HBMASTER: trying submitting job (0, 0, 14) to dispatcher\n",
      "13:59:59 HBMASTER: submitting job (0, 0, 14) to dispatcher\n",
      "13:59:59 DISPATCHER: trying to submit job (0, 0, 14)\n",
      "13:59:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "13:59:59 HBMASTER: job (0, 0, 14) submitted to dispatcher\n",
      "13:59:59 DISPATCHER: Trying to submit another job.\n",
      "13:59:59 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "13:59:59 DISPATCHER: starting job (0, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:59:59 DISPATCHER: job (0, 0, 14) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "13:59:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "13:59:59 WORKER: start processing job (0, 0, 14)\n",
      "13:59:59 WORKER: args: ()\n",
      "13:59:59 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.00634104168576129, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 0.0028704009018838406.\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:00:03 WORKER: done with job (0, 0, 11), trying to register it.\n",
      "14:00:03 WORKER: registered result for job (0, 0, 11) with dispatcher\n",
      "14:00:03 DISPATCHER: job (0, 0, 11) finished\n",
      "14:00:03 DISPATCHER: register_result: lock acquired\n",
      "14:00:03 DISPATCHER: job (0, 0, 11) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:00:03 job_id: (0, 0, 11)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.005740801613874142, 'loss': 'mae', 'numLayers': 2, 'numNeurons': 52, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 18.799872635458495, 'info': {'L1': 18.799872635458495, 'L2': 26.477858493076102, 'MAX': 2.4082158788874954, 'TrainTime': 57.8125}}\n",
      "exception: None\n",
      "\n",
      "14:00:03 job_callback for (0, 0, 11) started\n",
      "14:00:03 DISPATCHER: Trying to submit another job.\n",
      "14:00:03 job_callback for (0, 0, 11) got condition\n",
      "14:00:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:00:03 HBMASTER: Trying to run another job!\n",
      "14:00:03 job_callback for (0, 0, 11) finished\n",
      "14:00:03 start sampling a new configuration.\n",
      "14:00:03 done sampling a new configuration.\n",
      "14:00:03 HBMASTER: schedule new run for iteration 0\n",
      "14:00:03 HBMASTER: trying submitting job (0, 0, 15) to dispatcher\n",
      "14:00:03 HBMASTER: submitting job (0, 0, 15) to dispatcher\n",
      "14:00:03 DISPATCHER: trying to submit job (0, 0, 15)\n",
      "14:00:03 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:00:03 HBMASTER: job (0, 0, 15) submitted to dispatcher\n",
      "14:00:03 DISPATCHER: Trying to submit another job.\n",
      "14:00:03 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:00:03 DISPATCHER: starting job (0, 0, 15) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:00:03 DISPATCHER: job (0, 0, 15) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:00:03 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:00:03 WORKER: start processing job (0, 0, 15)\n",
      "14:00:03 WORKER: args: ()\n",
      "14:00:03 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.008896592364971057, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 79, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0028337726835161448.\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0014168863417580724.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.002777328947558999.\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0013886644737794995.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0048852539621293545.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:00:14 WORKER: done with job (0, 0, 10), trying to register it.\n",
      "14:00:14 WORKER: registered result for job (0, 0, 10) with dispatcher\n",
      "14:00:14 DISPATCHER: job (0, 0, 10) finished\n",
      "14:00:14 DISPATCHER: register_result: lock acquired\n",
      "14:00:14 DISPATCHER: job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:00:14 job_id: (0, 0, 10)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 4.581238737272087, 'info': {'L1': 4.581238737272087, 'L2': 3.015747792272403, 'MAX': 1.1453823835031383, 'TrainTime': 142.78125}}\n",
      "exception: None\n",
      "\n",
      "14:00:14 job_callback for (0, 0, 10) started\n",
      "14:00:14 DISPATCHER: Trying to submit another job.\n",
      "14:00:14 job_callback for (0, 0, 10) got condition\n",
      "14:00:14 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:00:14 HBMASTER: Trying to run another job!\n",
      "14:00:14 job_callback for (0, 0, 10) finished\n",
      "14:00:14 start sampling a new configuration.\n",
      "14:00:14 done sampling a new configuration.\n",
      "14:00:14 HBMASTER: schedule new run for iteration 0\n",
      "14:00:14 HBMASTER: trying submitting job (0, 0, 16) to dispatcher\n",
      "14:00:14 HBMASTER: submitting job (0, 0, 16) to dispatcher\n",
      "14:00:14 DISPATCHER: trying to submit job (0, 0, 16)\n",
      "14:00:14 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:00:14 HBMASTER: job (0, 0, 16) submitted to dispatcher\n",
      "14:00:14 DISPATCHER: Trying to submit another job.\n",
      "14:00:14 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:00:14 DISPATCHER: starting job (0, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:00:14 DISPATCHER: job (0, 0, 16) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:00:14 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:00:14 WORKER: start processing job (0, 0, 16)\n",
      "14:00:14 WORKER: args: ()\n",
      "14:00:14 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.002786573274003666, 'loss': 'mse', 'numLayers': 48, 'numNeurons': 85, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0006943322368897498.\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0003471661184448749.\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0024426269810646772.\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.004448296036571264.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:00:22 WORKER: done with job (0, 0, 12), trying to register it.\n",
      "14:00:22 WORKER: registered result for job (0, 0, 12) with dispatcher\n",
      "14:00:22 DISPATCHER: job (0, 0, 12) finished\n",
      "14:00:22 DISPATCHER: register_result: lock acquired\n",
      "14:00:22 DISPATCHER: job (0, 0, 12) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:00:22 job_id: (0, 0, 12)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0055546581014190724, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 93, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.245415253048243, 'info': {'L1': 8.245415253048243, 'L2': 13.960987997651173, 'MAX': 4.980350296389398, 'TrainTime': 88.03125}}\n",
      "exception: None\n",
      "\n",
      "14:00:22 job_callback for (0, 0, 12) started\n",
      "14:00:22 DISPATCHER: Trying to submit another job.\n",
      "14:00:22 job_callback for (0, 0, 12) got condition\n",
      "14:00:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:00:22 HBMASTER: Trying to run another job!\n",
      "14:00:22 job_callback for (0, 0, 12) finished\n",
      "14:00:22 start sampling a new configuration.\n",
      "14:00:22 done sampling a new configuration.\n",
      "14:00:22 HBMASTER: schedule new run for iteration 0\n",
      "14:00:22 HBMASTER: trying submitting job (0, 0, 17) to dispatcher\n",
      "14:00:22 HBMASTER: submitting job (0, 0, 17) to dispatcher\n",
      "14:00:22 DISPATCHER: trying to submit job (0, 0, 17)\n",
      "14:00:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:00:22 HBMASTER: job (0, 0, 17) submitted to dispatcher\n",
      "14:00:22 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:00:22 DISPATCHER: Trying to submit another job.\n",
      "14:00:22 DISPATCHER: starting job (0, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:00:22 DISPATCHER: job (0, 0, 17) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:00:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:00:22 WORKER: start processing job (0, 0, 17)\n",
      "14:00:22 WORKER: args: ()\n",
      "14:00:22 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.006772872141870129, 'loss': 'mse', 'numLayers': 42, 'numNeurons': 87, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.002224148018285632.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0012213134905323386.\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.001112074009142816.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:00:26 WORKER: done with job (0, 0, 13), trying to register it.\n",
      "14:00:26 WORKER: registered result for job (0, 0, 13) with dispatcher\n",
      "14:00:26 DISPATCHER: job (0, 0, 13) finished\n",
      "14:00:26 DISPATCHER: register_result: lock acquired\n",
      "14:00:26 DISPATCHER: job (0, 0, 13) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:00:26 job_id: (0, 0, 13)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 10, 'initial_lr': 0.009770508096866717, 'loss': 'mse', 'numLayers': 16, 'numNeurons': 96, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 29.12516303002162, 'info': {'L1': 29.12516303002162, 'L2': 46.34366234946878, 'MAX': 2.7054768012874693, 'TrainTime': 76.34375}}\n",
      "exception: None\n",
      "\n",
      "14:00:26 job_callback for (0, 0, 13) started\n",
      "14:00:26 DISPATCHER: Trying to submit another job.\n",
      "14:00:26 job_callback for (0, 0, 13) got condition\n",
      "14:00:26 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:00:26 HBMASTER: Trying to run another job!\n",
      "14:00:26 job_callback for (0, 0, 13) finished\n",
      "14:00:26 start sampling a new configuration.\n",
      "14:00:26 done sampling a new configuration.\n",
      "14:00:26 HBMASTER: schedule new run for iteration 0\n",
      "14:00:26 HBMASTER: trying submitting job (0, 0, 18) to dispatcher\n",
      "14:00:26 HBMASTER: submitting job (0, 0, 18) to dispatcher\n",
      "14:00:26 DISPATCHER: trying to submit job (0, 0, 18)\n",
      "14:00:26 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:00:26 HBMASTER: job (0, 0, 18) submitted to dispatcher\n",
      "14:00:26 DISPATCHER: Trying to submit another job.\n",
      "14:00:26 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:00:26 DISPATCHER: starting job (0, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:00:26 DISPATCHER: job (0, 0, 18) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:00:26 WORKER: start processing job (0, 0, 18)\n",
      "14:00:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:00:26 WORKER: args: ()\n",
      "14:00:26 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.00019037455282385694, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 24, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "14:00:29 DISPATCHER: Starting worker discovery\n",
      "14:00:29 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:00:29 DISPATCHER: Finished worker discovery\n",
      "14:00:31 WORKER: done with job (0, 0, 15), trying to register it.\n",
      "14:00:31 WORKER: registered result for job (0, 0, 15) with dispatcher\n",
      "14:00:31 DISPATCHER: job (0, 0, 15) finished\n",
      "14:00:31 DISPATCHER: register_result: lock acquired\n",
      "14:00:31 DISPATCHER: job (0, 0, 15) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:00:31 job_id: (0, 0, 15)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.008896592364971057, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 79, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 33.69326213635841, 'info': {'L1': 33.69326213635841, 'L2': 69.5072611032868, 'MAX': 3.0763051790976004, 'TrainTime': 58.875}}\n",
      "exception: None\n",
      "\n",
      "14:00:31 job_callback for (0, 0, 15) started\n",
      "14:00:31 DISPATCHER: Trying to submit another job.\n",
      "14:00:31 job_callback for (0, 0, 15) got condition\n",
      "14:00:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:00:31 HBMASTER: Trying to run another job!\n",
      "14:00:31 job_callback for (0, 0, 15) finished\n",
      "14:00:31 start sampling a new configuration.\n",
      "14:00:31 done sampling a new configuration.\n",
      "14:00:31 HBMASTER: schedule new run for iteration 0\n",
      "14:00:31 HBMASTER: trying submitting job (0, 0, 19) to dispatcher\n",
      "14:00:31 HBMASTER: submitting job (0, 0, 19) to dispatcher\n",
      "14:00:31 DISPATCHER: trying to submit job (0, 0, 19)\n",
      "14:00:31 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:00:31 HBMASTER: job (0, 0, 19) submitted to dispatcher\n",
      "14:00:31 DISPATCHER: Trying to submit another job.\n",
      "14:00:31 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:00:31 DISPATCHER: starting job (0, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:00:31 DISPATCHER: job (0, 0, 19) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:00:31 WORKER: start processing job (0, 0, 19)\n",
      "14:00:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:00:31 WORKER: args: ()\n",
      "14:00:31 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.007515325131023456, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 72, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.003757662605494261.\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0018788313027471304.\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0009394156513735652.\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0004697078256867826.\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0002348539128433913.\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0013932866277173162.\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0006966433138586581.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.003386436030268669.\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00034832165692932904.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:01:19 WORKER: done with job (0, 0, 19), trying to register it.\n",
      "14:01:19 WORKER: registered result for job (0, 0, 19) with dispatcher\n",
      "14:01:19 DISPATCHER: job (0, 0, 19) finished\n",
      "14:01:19 DISPATCHER: register_result: lock acquired\n",
      "14:01:19 DISPATCHER: job (0, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:01:19 job_id: (0, 0, 19)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.007515325131023456, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 72, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 5.331516291050541, 'info': {'L1': 5.331516291050541, 'L2': 7.294432577456389, 'MAX': 4.210607330691156, 'TrainTime': 98.890625}}\n",
      "exception: None\n",
      "\n",
      "14:01:19 job_callback for (0, 0, 19) started\n",
      "14:01:19 DISPATCHER: Trying to submit another job.\n",
      "14:01:19 job_callback for (0, 0, 19) got condition\n",
      "14:01:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:01:19 HBMASTER: Trying to run another job!\n",
      "14:01:19 job_callback for (0, 0, 19) finished\n",
      "14:01:19 start sampling a new configuration.\n",
      "14:01:19 done sampling a new configuration.\n",
      "14:01:19 HBMASTER: schedule new run for iteration 0\n",
      "14:01:19 HBMASTER: trying submitting job (0, 0, 20) to dispatcher\n",
      "14:01:19 HBMASTER: submitting job (0, 0, 20) to dispatcher\n",
      "14:01:19 DISPATCHER: trying to submit job (0, 0, 20)\n",
      "14:01:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:01:19 HBMASTER: job (0, 0, 20) submitted to dispatcher\n",
      "14:01:19 DISPATCHER: Trying to submit another job.\n",
      "14:01:19 DISPATCHER: starting job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:01:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:01:19 DISPATCHER: job (0, 0, 20) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:01:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:01:19 WORKER: start processing job (0, 0, 20)\n",
      "14:01:19 WORKER: args: ()\n",
      "14:01:19 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0016932180151343346.\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.00017416082846466452.\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0008466090075671673.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.00042330450378358364.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:01:29 DISPATCHER: Starting worker discovery\n",
      "14:01:29 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:01:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.00021165225189179182.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:01:33 WORKER: done with job (0, 0, 16), trying to register it.\n",
      "14:01:33 WORKER: registered result for job (0, 0, 16) with dispatcher\n",
      "14:01:33 DISPATCHER: job (0, 0, 16) finished\n",
      "14:01:33 DISPATCHER: register_result: lock acquired\n",
      "14:01:33 DISPATCHER: job (0, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:01:33 job_id: (0, 0, 16)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.002786573274003666, 'loss': 'mse', 'numLayers': 48, 'numNeurons': 85, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 14.007838154703107, 'info': {'L1': 14.007838154703107, 'L2': 12.436274745754503, 'MAX': 3.6429605888240904, 'TrainTime': 165.875}}\n",
      "exception: None\n",
      "\n",
      "14:01:33 job_callback for (0, 0, 16) started\n",
      "14:01:33 DISPATCHER: Trying to submit another job.\n",
      "14:01:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:01:33 job_callback for (0, 0, 16) got condition\n",
      "14:01:33 HBMASTER: Trying to run another job!\n",
      "14:01:33 job_callback for (0, 0, 16) finished\n",
      "14:01:33 start sampling a new configuration.\n",
      "14:01:33 done sampling a new configuration.\n",
      "14:01:33 HBMASTER: schedule new run for iteration 0\n",
      "14:01:33 HBMASTER: trying submitting job (0, 0, 21) to dispatcher\n",
      "14:01:33 HBMASTER: submitting job (0, 0, 21) to dispatcher\n",
      "14:01:33 DISPATCHER: trying to submit job (0, 0, 21)\n",
      "14:01:33 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:01:33 HBMASTER: job (0, 0, 21) submitted to dispatcher\n",
      "14:01:33 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:01:33 DISPATCHER: Trying to submit another job.\n",
      "14:01:33 DISPATCHER: starting job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:01:33 DISPATCHER: job (0, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:01:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:01:33 WORKER: start processing job (0, 0, 21)\n",
      "14:01:33 WORKER: args: ()\n",
      "14:01:33 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.00010582612594589591.\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 5.2913062972947955e-05.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00110: ReduceLROnPlateau reducing learning rate to 2.6456531486473978e-05.\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0031705207657068968.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:01:48 WORKER: done with job (0, 0, 17), trying to register it.\n",
      "14:01:48 WORKER: registered result for job (0, 0, 17) with dispatcher\n",
      "14:01:48 DISPATCHER: job (0, 0, 17) finished\n",
      "14:01:48 DISPATCHER: register_result: lock acquired\n",
      "14:01:48 DISPATCHER: job (0, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:01:48 job_id: (0, 0, 17)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.006772872141870129, 'loss': 'mse', 'numLayers': 42, 'numNeurons': 87, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 14.700970768006222, 'info': {'L1': 14.700970768006222, 'L2': 12.828813317081657, 'MAX': 3.577165882479486, 'TrainTime': 188.078125}}\n",
      "exception: None\n",
      "\n",
      "14:01:48 job_callback for (0, 0, 17) started\n",
      "14:01:48 DISPATCHER: Trying to submit another job.\n",
      "14:01:48 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:01:48 job_callback for (0, 0, 17) got condition\n",
      "14:01:48 done building a new model for budget 111.111111 based on 9/15 split\n",
      "Best loss for this budget:4.581239\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:01:48 HBMASTER: Trying to run another job!\n",
      "14:01:48 job_callback for (0, 0, 17) finished\n",
      "14:01:48 start sampling a new configuration.\n",
      "14:01:48 best_vector: [3, 0, 0.914646927098664, 0.6348121714579107, 1, 0.5765529372975002, 0.39300273062075325, 4], 2.2485748911800592e-31, 0.044472612583305905, -0.018145367016246482\n",
      "14:01:48 done sampling a new configuration.\n",
      "14:01:48 HBMASTER: schedule new run for iteration 0\n",
      "14:01:48 HBMASTER: trying submitting job (0, 0, 22) to dispatcher\n",
      "14:01:48 HBMASTER: submitting job (0, 0, 22) to dispatcher\n",
      "14:01:48 DISPATCHER: trying to submit job (0, 0, 22)\n",
      "14:01:48 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:01:48 HBMASTER: job (0, 0, 22) submitted to dispatcher\n",
      "14:01:48 DISPATCHER: Trying to submit another job.\n",
      "14:01:48 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:01:48 DISPATCHER: starting job (0, 0, 22) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:01:48 DISPATCHER: job (0, 0, 22) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:01:48 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:01:48 WORKER: start processing job (0, 0, 22)\n",
      "14:01:48 WORKER: args: ()\n",
      "14:01:48 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.006384640497433316, 'loss': 'mse', 'numLayers': 30, 'numNeurons': 45, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:01:56 WORKER: done with job (0, 0, 18), trying to register it.\n",
      "14:01:56 WORKER: registered result for job (0, 0, 18) with dispatcher\n",
      "14:01:56 DISPATCHER: job (0, 0, 18) finished\n",
      "14:01:56 DISPATCHER: register_result: lock acquired\n",
      "14:01:56 DISPATCHER: job (0, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:01:56 job_id: (0, 0, 18)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.00019037455282385694, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 24, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 91.94594395565396, 'info': {'L1': 91.94594395565396, 'L2': 433.29434437549435, 'MAX': 5.017270900309086, 'TrainTime': 200.296875}}\n",
      "exception: None\n",
      "\n",
      "14:01:56 job_callback for (0, 0, 18) started\n",
      "14:01:56 DISPATCHER: Trying to submit another job.\n",
      "14:01:56 job_callback for (0, 0, 18) got condition\n",
      "14:01:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:01:56 done building a new model for budget 111.111111 based on 9/16 split\n",
      "Best loss for this budget:4.581239\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:01:56 HBMASTER: Trying to run another job!\n",
      "14:01:56 job_callback for (0, 0, 18) finished\n",
      "14:01:56 start sampling a new configuration.\n",
      "14:01:56 best_vector: [3, 3, 0.06983692839715339, 0.8608944889720105, 0, 0.5709142826975405, 0.8829446883664456, 0], 0.008851631928601774, 0.01717770523843638, 0.0001520507241486534\n",
      "14:01:56 done sampling a new configuration.\n",
      "14:01:56 HBMASTER: schedule new run for iteration 0\n",
      "14:01:56 HBMASTER: trying submitting job (0, 0, 23) to dispatcher\n",
      "14:01:56 HBMASTER: submitting job (0, 0, 23) to dispatcher\n",
      "14:01:56 DISPATCHER: trying to submit job (0, 0, 23)\n",
      "14:01:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:01:56 HBMASTER: job (0, 0, 23) submitted to dispatcher\n",
      "14:01:56 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:01:56 DISPATCHER: Trying to submit another job.\n",
      "14:01:56 DISPATCHER: starting job (0, 0, 23) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:01:56 DISPATCHER: job (0, 0, 23) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:01:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:01:56 WORKER: start processing job (0, 0, 23)\n",
      "14:01:56 WORKER: args: ()\n",
      "14:01:56 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.008622855440822905, 'loss': 'mae', 'numLayers': 29, 'numNeurons': 90, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0015852603828534484.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0007926301914267242.\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0012206739047542214.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:02:19 WORKER: done with job (0, 0, 14), trying to register it.\n",
      "14:02:19 WORKER: registered result for job (0, 0, 14) with dispatcher\n",
      "14:02:19 DISPATCHER: job (0, 0, 14) finished\n",
      "14:02:19 DISPATCHER: register_result: lock acquired\n",
      "14:02:19 DISPATCHER: job (0, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:02:19 job_id: (0, 0, 14)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.00634104168576129, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 2.897903432803775, 'info': {'L1': 2.897903432803775, 'L2': 0.8927437395399311, 'MAX': 1.0292064355724424, 'TrainTime': 311.734375}}\n",
      "exception: None\n",
      "\n",
      "14:02:19 job_callback for (0, 0, 14) started\n",
      "14:02:19 DISPATCHER: Trying to submit another job.\n",
      "14:02:19 job_callback for (0, 0, 14) got condition\n",
      "14:02:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:02:19 done building a new model for budget 111.111111 based on 9/17 split\n",
      "Best loss for this budget:2.897903\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:02:19 HBMASTER: Trying to run another job!\n",
      "14:02:19 job_callback for (0, 0, 14) finished\n",
      "14:02:19 start sampling a new configuration.\n",
      "14:02:19 done sampling a new configuration.\n",
      "14:02:19 HBMASTER: schedule new run for iteration 0\n",
      "14:02:19 HBMASTER: trying submitting job (0, 0, 24) to dispatcher\n",
      "14:02:19 HBMASTER: submitting job (0, 0, 24) to dispatcher\n",
      "14:02:19 DISPATCHER: trying to submit job (0, 0, 24)\n",
      "14:02:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:02:19 HBMASTER: job (0, 0, 24) submitted to dispatcher\n",
      "14:02:19 DISPATCHER: Trying to submit another job.\n",
      "14:02:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:02:19 DISPATCHER: starting job (0, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:02:19 DISPATCHER: job (0, 0, 24) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:02:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:02:19 WORKER: start processing job (0, 0, 24)\n",
      "14:02:19 WORKER: args: ()\n",
      "14:02:19 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.006592839352991139, 'loss': 'mse', 'numLayers': 49, 'numNeurons': 75, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.0006103369523771107.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:02:22 WORKER: done with job (0, 0, 21), trying to register it.\n",
      "14:02:22 WORKER: registered result for job (0, 0, 21) with dispatcher\n",
      "14:02:22 DISPATCHER: job (0, 0, 21) finished\n",
      "14:02:22 DISPATCHER: register_result: lock acquired\n",
      "14:02:22 DISPATCHER: job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:02:22 job_id: (0, 0, 21)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 0.8369142741286918, 'info': {'L1': 0.8369142741286918, 'L2': 0.0597433991150579, 'MAX': 0.20483918059998452, 'TrainTime': 116.40625}}\n",
      "exception: None\n",
      "\n",
      "14:02:22 job_callback for (0, 0, 21) started\n",
      "14:02:22 DISPATCHER: Trying to submit another job.\n",
      "14:02:22 job_callback for (0, 0, 21) got condition\n",
      "14:02:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:02:22 done building a new model for budget 111.111111 based on 9/17 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:02:22 HBMASTER: Trying to run another job!\n",
      "14:02:22 job_callback for (0, 0, 21) finished\n",
      "14:02:22 start sampling a new configuration.\n",
      "14:02:22 done sampling a new configuration.\n",
      "14:02:22 HBMASTER: schedule new run for iteration 0\n",
      "14:02:22 HBMASTER: trying submitting job (0, 0, 25) to dispatcher\n",
      "14:02:22 HBMASTER: submitting job (0, 0, 25) to dispatcher\n",
      "14:02:22 DISPATCHER: trying to submit job (0, 0, 25)\n",
      "14:02:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:02:22 HBMASTER: job (0, 0, 25) submitted to dispatcher\n",
      "14:02:22 DISPATCHER: Trying to submit another job.\n",
      "14:02:22 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:02:22 DISPATCHER: starting job (0, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:02:22 DISPATCHER: job (0, 0, 25) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:02:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:02:22 WORKER: start processing job (0, 0, 25)\n",
      "14:02:22 WORKER: args: ()\n",
      "14:02:22 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 10, 'initial_lr': 0.009115439145465235, 'loss': 'mse', 'numLayers': 44, 'numNeurons': 54, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "14:02:22 WORKER: done with job (0, 0, 20), trying to register it.\n",
      "14:02:22 WORKER: registered result for job (0, 0, 20) with dispatcher\n",
      "14:02:22 DISPATCHER: job (0, 0, 20) finished\n",
      "14:02:22 DISPATCHER: register_result: lock acquired\n",
      "14:02:22 DISPATCHER: job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:02:22 job_id: (0, 0, 20)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 3.3191085314821764, 'info': {'L1': 3.3191085314821764, 'L2': 1.015752913226953, 'MAX': 0.5262438569710728, 'TrainTime': 165.8125}}\n",
      "exception: None\n",
      "\n",
      "14:02:22 job_callback for (0, 0, 20) started\n",
      "14:02:22 DISPATCHER: Trying to submit another job.\n",
      "14:02:22 job_callback for (0, 0, 20) got condition\n",
      "14:02:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:02:22 done building a new model for budget 111.111111 based on 9/18 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:02:22 HBMASTER: Trying to run another job!\n",
      "14:02:22 job_callback for (0, 0, 20) finished\n",
      "14:02:22 start sampling a new configuration.\n",
      "14:02:22 best_vector: [3, 0, 0.7479095567615609, 0.4504916414849409, 1, 0.5125905340047915, 0.6663440282403957, 0], 4.066937448123702e-31, 0.024588526692520294, -0.019068207148344635\n",
      "14:02:22 done sampling a new configuration.\n",
      "14:02:22 HBMASTER: schedule new run for iteration 0\n",
      "14:02:22 HBMASTER: trying submitting job (0, 0, 26) to dispatcher\n",
      "14:02:22 HBMASTER: submitting job (0, 0, 26) to dispatcher\n",
      "14:02:22 DISPATCHER: trying to submit job (0, 0, 26)\n",
      "14:02:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:02:23 HBMASTER: job (0, 0, 26) submitted to dispatcher\n",
      "14:02:23 DISPATCHER: Trying to submit another job.\n",
      "14:02:23 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:02:23 DISPATCHER: starting job (0, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:02:23 DISPATCHER: job (0, 0, 26) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:02:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:02:23 WORKER: start processing job (0, 0, 26)\n",
      "14:02:23 WORKER: args: ()\n",
      "14:02:23 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.004559867250700915, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 70, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "14:02:30 DISPATCHER: Starting worker discovery\n",
      "14:02:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:02:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:02:52 WORKER: done with job (0, 0, 22), trying to register it.\n",
      "14:02:52 WORKER: registered result for job (0, 0, 22) with dispatcher\n",
      "14:02:52 DISPATCHER: job (0, 0, 22) finished\n",
      "14:02:52 DISPATCHER: register_result: lock acquired\n",
      "14:02:52 DISPATCHER: job (0, 0, 22) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:02:52 job_id: (0, 0, 22)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.006384640497433316, 'loss': 'mse', 'numLayers': 30, 'numNeurons': 45, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 75.25709279980403, 'info': {'L1': 75.25709279980403, 'L2': 293.51928485739427, 'MAX': 4.181142210960388, 'TrainTime': 147.484375}}\n",
      "exception: None\n",
      "\n",
      "14:02:52 job_callback for (0, 0, 22) started\n",
      "14:02:52 DISPATCHER: Trying to submit another job.\n",
      "14:02:53 job_callback for (0, 0, 22) got condition\n",
      "14:02:53 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:02:53 done building a new model for budget 111.111111 based on 9/19 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:02:53 HBMASTER: Trying to run another job!\n",
      "14:02:53 job_callback for (0, 0, 22) finished\n",
      "14:02:53 start sampling a new configuration.\n",
      "14:02:53 best_vector: [3, 0, 0.8180099057531643, 0.7568535264429759, 1, 0.3356102798003011, 0.32554161942298676, 0], 1.6838949670660898e-31, 0.059386126780955686, -0.0024152453042389603\n",
      "14:02:53 done sampling a new configuration.\n",
      "14:02:53 HBMASTER: schedule new run for iteration 1\n",
      "14:02:53 HBMASTER: trying submitting job (1, 0, 0) to dispatcher\n",
      "14:02:53 HBMASTER: submitting job (1, 0, 0) to dispatcher\n",
      "14:02:53 DISPATCHER: trying to submit job (1, 0, 0)\n",
      "14:02:53 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:02:53 HBMASTER: job (1, 0, 0) submitted to dispatcher\n",
      "14:02:53 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:02:53 DISPATCHER: Trying to submit another job.\n",
      "14:02:53 DISPATCHER: starting job (1, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:02:53 DISPATCHER: job (1, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:02:53 WORKER: start processing job (1, 0, 0)\n",
      "14:02:53 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:02:53 WORKER: args: ()\n",
      "14:02:53 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.007592849911785462, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 39, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.004311427939683199.\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0021557139698415995.\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0010778569849207997.\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0005389284924603999.\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.00026946424623019993.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.00013473212311509997.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:03:04 WORKER: done with job (0, 0, 23), trying to register it.\n",
      "14:03:04 WORKER: registered result for job (0, 0, 23) with dispatcher\n",
      "14:03:04 DISPATCHER: job (0, 0, 23) finished\n",
      "14:03:04 DISPATCHER: register_result: lock acquired\n",
      "14:03:04 DISPATCHER: job (0, 0, 23) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:03:04 job_id: (0, 0, 23)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.008622855440822905, 'loss': 'mae', 'numLayers': 29, 'numNeurons': 90, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.320694042444117, 'info': {'L1': 8.320694042444117, 'L2': 14.043244236066181, 'MAX': 4.985329906832513, 'TrainTime': 155.109375}}\n",
      "exception: None\n",
      "\n",
      "14:03:04 job_callback for (0, 0, 23) started\n",
      "14:03:04 DISPATCHER: Trying to submit another job.\n",
      "14:03:04 job_callback for (0, 0, 23) got condition\n",
      "14:03:04 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:03:04 done building a new model for budget 111.111111 based on 9/20 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:03:04 HBMASTER: Trying to run another job!\n",
      "14:03:04 job_callback for (0, 0, 23) finished\n",
      "14:03:04 start sampling a new configuration.\n",
      "14:03:05 best_vector: [2, 3, 0.2878769334628061, 0.8221589374743508, 1, 0.5901586082590107, 0.036380777888618085, 2], 0.1409487595401858, 0.008386079574472353, 0.0011820075134271672\n",
      "14:03:05 done sampling a new configuration.\n",
      "14:03:05 HBMASTER: schedule new run for iteration 1\n",
      "14:03:05 HBMASTER: trying submitting job (1, 0, 1) to dispatcher\n",
      "14:03:05 HBMASTER: submitting job (1, 0, 1) to dispatcher\n",
      "14:03:05 DISPATCHER: trying to submit job (1, 0, 1)\n",
      "14:03:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:03:05 HBMASTER: job (1, 0, 1) submitted to dispatcher\n",
      "14:03:05 DISPATCHER: Trying to submit another job.\n",
      "14:03:05 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:03:05 DISPATCHER: starting job (1, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:03:05 DISPATCHER: job (1, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:03:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:03:05 WORKER: start processing job (1, 0, 1)\n",
      "14:03:05 WORKER: args: ()\n",
      "14:03:05 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.008239373480996072, 'loss': 'mse', 'numLayers': 30, 'numNeurons': 13, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.003296419745311141.\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0016482098726555705.\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.0008241049363277853.\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0004120524681638926.\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0022799335420131683.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:03:25 WORKER: done with job (0, 0, 24), trying to register it.\n",
      "14:03:25 WORKER: registered result for job (0, 0, 24) with dispatcher\n",
      "14:03:25 DISPATCHER: job (0, 0, 24) finished\n",
      "14:03:25 DISPATCHER: register_result: lock acquired\n",
      "14:03:25 DISPATCHER: job (0, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:03:25 job_id: (0, 0, 24)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.006592839352991139, 'loss': 'mse', 'numLayers': 49, 'numNeurons': 75, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 19.26947979530215, 'info': {'L1': 19.26947979530215, 'L2': 21.05545779615463, 'MAX': 3.4556889938228696, 'TrainTime': 135.0}}\n",
      "exception: None\n",
      "\n",
      "14:03:25 job_callback for (0, 0, 24) started\n",
      "14:03:25 DISPATCHER: Trying to submit another job.\n",
      "14:03:25 job_callback for (0, 0, 24) got condition\n",
      "14:03:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:03:25 done building a new model for budget 111.111111 based on 9/21 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:03:25 HBMASTER: Trying to run another job!\n",
      "14:03:25 job_callback for (0, 0, 24) finished\n",
      "14:03:25 start sampling a new configuration.\n",
      "14:03:26 best_vector: [2, 1, 0.768087560558045, 0.49211307731783227, 1, 0.3659584928124182, 0.24050504177452253, 2], 0.04141607740398498, 0.05112858802928552, 0.002117545559377349\n",
      "14:03:26 done sampling a new configuration.\n",
      "14:03:26 HBMASTER: schedule new run for iteration 1\n",
      "14:03:26 HBMASTER: trying submitting job (1, 0, 2) to dispatcher\n",
      "14:03:26 HBMASTER: submitting job (1, 0, 2) to dispatcher\n",
      "14:03:26 DISPATCHER: trying to submit job (1, 0, 2)\n",
      "14:03:26 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:03:26 HBMASTER: job (1, 0, 2) submitted to dispatcher\n",
      "14:03:26 DISPATCHER: Trying to submit another job.\n",
      "14:03:26 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:03:26 DISPATCHER: starting job (1, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:03:26 DISPATCHER: job (1, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:03:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:03:26 WORKER: start processing job (1, 0, 2)\n",
      "14:03:26 WORKER: args: ()\n",
      "14:03:26 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.00497191946544654, 'loss': 'mse', 'numLayers': 19, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0011399667710065842.\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0005699833855032921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:03:30 DISPATCHER: Starting worker discovery\n",
      "14:03:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:03:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.00028499169275164604.\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "Batch 0: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2347: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1664: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "14:03:35 WORKER: done with job (1, 0, 1), trying to register it.\n",
      "14:03:35 WORKER: registered result for job (1, 0, 1) with dispatcher\n",
      "14:03:35 DISPATCHER: job (1, 0, 1) finished\n",
      "14:03:35 DISPATCHER: register_result: lock acquired\n",
      "14:03:35 DISPATCHER: job (1, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:03:35 job_id: (1, 0, 1)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.008239373480996072, 'loss': 'mse', 'numLayers': 30, 'numNeurons': 13, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': nan, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 57.078125}}\n",
      "exception: None\n",
      "\n",
      "14:03:35 job_callback for (1, 0, 1) started\n",
      "14:03:35 DISPATCHER: Trying to submit another job.\n",
      "14:03:35 job_callback for (1, 0, 1) got condition\n",
      "14:03:35 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:03:35 Only 1 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:03:35 HBMASTER: Trying to run another job!\n",
      "14:03:35 job_callback for (1, 0, 1) finished\n",
      "14:03:35 start sampling a new configuration.\n",
      "14:03:35 best_vector: [3, 1, 0.7413879396723223, 0.12476599267287114, 0, 0.10978750542726196, 0.6806708398030965, 1], 1.1859985627057002e-30, 0.008431713422304924, -0.0037809015783110697\n",
      "14:03:35 done sampling a new configuration.\n",
      "14:03:35 HBMASTER: schedule new run for iteration 1\n",
      "14:03:35 HBMASTER: trying submitting job (1, 0, 3) to dispatcher\n",
      "14:03:35 HBMASTER: submitting job (1, 0, 3) to dispatcher\n",
      "14:03:35 DISPATCHER: trying to submit job (1, 0, 3)\n",
      "14:03:35 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:03:35 HBMASTER: job (1, 0, 3) submitted to dispatcher\n",
      "14:03:35 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:03:35 DISPATCHER: Trying to submit another job.\n",
      "14:03:35 DISPATCHER: starting job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:03:35 DISPATCHER: job (1, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:03:35 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:03:35 WORKER: start processing job (1, 0, 3)\n",
      "14:03:35 WORKER: args: ()\n",
      "14:03:35 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "Batch 0: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:03:40 WORKER: done with job (1, 0, 2), trying to register it.\n",
      "14:03:40 WORKER: registered result for job (1, 0, 2) with dispatcher\n",
      "14:03:40 DISPATCHER: job (1, 0, 2) finished\n",
      "14:03:40 DISPATCHER: register_result: lock acquired\n",
      "14:03:40 DISPATCHER: job (1, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:03:40 job_id: (1, 0, 2)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.00497191946544654, 'loss': 'mse', 'numLayers': 19, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': nan, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 29.859375}}\n",
      "exception: None\n",
      "\n",
      "14:03:40 job_callback for (1, 0, 2) started\n",
      "14:03:40 DISPATCHER: Trying to submit another job.\n",
      "14:03:40 job_callback for (1, 0, 2) got condition\n",
      "14:03:40 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:03:40 Only 2 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:03:40 HBMASTER: Trying to run another job!\n",
      "14:03:40 job_callback for (1, 0, 2) finished\n",
      "14:03:40 start sampling a new configuration.\n",
      "14:03:40 best_vector: [3, 2, 0.5776060984141347, 0.39353766777574073, 0, 0.08124665751054072, 0.081801661491863, 1], 1.2585407037090363e-29, 0.000794571043314616, -0.005824577533564692\n",
      "14:03:40 done sampling a new configuration.\n",
      "14:03:40 HBMASTER: schedule new run for iteration 1\n",
      "14:03:40 HBMASTER: trying submitting job (1, 0, 4) to dispatcher\n",
      "14:03:40 HBMASTER: submitting job (1, 0, 4) to dispatcher\n",
      "14:03:40 DISPATCHER: trying to submit job (1, 0, 4)\n",
      "14:03:40 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:03:40 HBMASTER: job (1, 0, 4) submitted to dispatcher\n",
      "14:03:40 DISPATCHER: Trying to submit another job.\n",
      "14:03:40 DISPATCHER: starting job (1, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:03:40 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:03:40 DISPATCHER: job (1, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:03:40 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:03:40 WORKER: start processing job (1, 0, 4)\n",
      "14:03:40 WORKER: args: ()\n",
      "14:03:40 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.003996022910979833, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 17, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:03:43 WORKER: done with job (0, 0, 26), trying to register it.\n",
      "14:03:43 WORKER: registered result for job (0, 0, 26) with dispatcher\n",
      "14:03:43 DISPATCHER: job (0, 0, 26) finished\n",
      "14:03:43 DISPATCHER: register_result: lock acquired\n",
      "14:03:43 DISPATCHER: job (0, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:03:43 job_id: (0, 0, 26)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.004559867250700915, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 70, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 3.927137941498262, 'info': {'L1': 3.927137941498262, 'L2': 2.001197276849122, 'MAX': 2.606676380526361, 'TrainTime': 183.640625}}\n",
      "exception: None\n",
      "\n",
      "14:03:43 job_callback for (0, 0, 26) started\n",
      "14:03:43 DISPATCHER: Trying to submit another job.\n",
      "14:03:43 job_callback for (0, 0, 26) got condition\n",
      "14:03:43 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:03:43 done building a new model for budget 111.111111 based on 9/22 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:03:43 HBMASTER: Trying to run another job!\n",
      "14:03:43 job_callback for (0, 0, 26) finished\n",
      "14:03:43 start sampling a new configuration.\n",
      "14:03:43 best_vector: [4, 2, 0.5921068402718261, 0.3916932482734464, 1, 0.8478868325403353, 0.822553298759809, 4], 5.313658237698672e-31, 0.018819426377581574, -0.0018013827389677791\n",
      "14:03:43 done sampling a new configuration.\n",
      "14:03:43 HBMASTER: schedule new run for iteration 1\n",
      "14:03:43 HBMASTER: trying submitting job (1, 0, 5) to dispatcher\n",
      "14:03:43 HBMASTER: submitting job (1, 0, 5) to dispatcher\n",
      "14:03:43 DISPATCHER: trying to submit job (1, 0, 5)\n",
      "14:03:43 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:03:43 HBMASTER: job (1, 0, 5) submitted to dispatcher\n",
      "14:03:43 DISPATCHER: Trying to submit another job.\n",
      "14:03:43 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:03:43 DISPATCHER: starting job (1, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:03:43 DISPATCHER: job (1, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:03:43 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:03:43 WORKER: start processing job (1, 0, 5)\n",
      "14:03:43 WORKER: args: ()\n",
      "14:03:43 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.00397776315790712, 'loss': 'mse', 'numLayers': 43, 'numNeurons': 84, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.0006675916374661028.\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 0.0003337958187330514.\n",
      "\n",
      "Epoch 00222: ReduceLROnPlateau reducing learning rate to 0.0001668979093665257.\n",
      "\n",
      "Epoch 00306: ReduceLROnPlateau reducing learning rate to 0.003796424949541688.\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.001998011488467455.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:04:19 WORKER: done with job (1, 0, 0), trying to register it.\n",
      "14:04:19 WORKER: registered result for job (1, 0, 0) with dispatcher\n",
      "14:04:19 DISPATCHER: job (1, 0, 0) finished\n",
      "14:04:19 DISPATCHER: register_result: lock acquired\n",
      "14:04:19 DISPATCHER: job (1, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:04:19 job_id: (1, 0, 0)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.007592849911785462, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 39, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.22910567520513417, 'info': {'L1': 0.22910567520513417, 'L2': 0.008203910361939779, 'MAX': 0.23109069038563668, 'TrainTime': 214.859375}}\n",
      "exception: None\n",
      "\n",
      "14:04:19 job_callback for (1, 0, 0) started\n",
      "14:04:19 DISPATCHER: Trying to submit another job.\n",
      "14:04:19 job_callback for (1, 0, 0) got condition\n",
      "14:04:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:04:19 Only 3 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:04:19 HBMASTER: Trying to run another job!\n",
      "14:04:19 job_callback for (1, 0, 0) finished\n",
      "14:04:19 start sampling a new configuration.\n",
      "14:04:19 done sampling a new configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 8.344895468326285e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:04:19 HBMASTER: schedule new run for iteration 1\n",
      "14:04:19 HBMASTER: trying submitting job (1, 0, 6) to dispatcher\n",
      "14:04:19 HBMASTER: submitting job (1, 0, 6) to dispatcher\n",
      "14:04:19 DISPATCHER: trying to submit job (1, 0, 6)\n",
      "14:04:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:04:19 HBMASTER: job (1, 0, 6) submitted to dispatcher\n",
      "14:04:19 DISPATCHER: Trying to submit another job.\n",
      "14:04:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:04:19 DISPATCHER: starting job (1, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:04:19 DISPATCHER: job (1, 0, 6) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:04:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:04:19 WORKER: start processing job (1, 0, 6)\n",
      "14:04:19 WORKER: args: ()\n",
      "14:04:19 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.004917984872254104, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 94, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00233: ReduceLROnPlateau reducing learning rate to 0.0009990057442337275.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00317: ReduceLROnPlateau reducing learning rate to 4.172447734163143e-05.\n",
      "\n",
      "Epoch 00273: ReduceLROnPlateau reducing learning rate to 0.0004995028721168637.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:04:26 WORKER: done with job (1, 0, 3), trying to register it.\n",
      "14:04:26 WORKER: registered result for job (1, 0, 3) with dispatcher\n",
      "14:04:26 DISPATCHER: job (1, 0, 3) finished\n",
      "14:04:26 DISPATCHER: register_result: lock acquired\n",
      "14:04:26 DISPATCHER: job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:04:26 job_id: (1, 0, 3)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.18974255062552067, 'info': {'L1': 0.18974255062552067, 'L2': 0.005536541899219436, 'MAX': 0.26800681222611367, 'TrainTime': 137.421875}}\n",
      "exception: None\n",
      "\n",
      "14:04:26 job_callback for (1, 0, 3) started\n",
      "14:04:26 DISPATCHER: Trying to submit another job.\n",
      "14:04:26 job_callback for (1, 0, 3) got condition\n",
      "14:04:26 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:04:26 Only 4 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:04:26 HBMASTER: Trying to run another job!\n",
      "14:04:26 job_callback for (1, 0, 3) finished\n",
      "14:04:26 start sampling a new configuration.\n",
      "14:04:26 best_vector: [2, 1, 0.4045002658535109, 0.3638948411982896, 1, 0.1712161726177287, 0.623940980101483, 2], 3.908893291891545e-31, 0.025582688636560144, -0.00033167043019561963\n",
      "14:04:26 done sampling a new configuration.\n",
      "14:04:26 HBMASTER: schedule new run for iteration 1\n",
      "14:04:26 HBMASTER: trying submitting job (1, 0, 7) to dispatcher\n",
      "14:04:26 HBMASTER: submitting job (1, 0, 7) to dispatcher\n",
      "14:04:26 DISPATCHER: trying to submit job (1, 0, 7)\n",
      "14:04:26 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:04:26 HBMASTER: job (1, 0, 7) submitted to dispatcher\n",
      "14:04:26 DISPATCHER: Trying to submit another job.\n",
      "14:04:26 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:04:26 DISPATCHER: starting job (1, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:04:26 DISPATCHER: job (1, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:04:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:04:26 WORKER: start processing job (1, 0, 7)\n",
      "14:04:26 WORKER: args: ()\n",
      "14:04:26 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 8, 'initial_lr': 0.003702558927863067, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 66, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:04:30 DISPATCHER: Starting worker discovery\n",
      "14:04:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:04:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:04:34 WORKER: done with job (1, 0, 4), trying to register it.\n",
      "14:04:34 WORKER: registered result for job (1, 0, 4) with dispatcher\n",
      "14:04:34 DISPATCHER: job (1, 0, 4) finished\n",
      "14:04:34 DISPATCHER: register_result: lock acquired\n",
      "14:04:34 DISPATCHER: job (1, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:04:34 job_id: (1, 0, 4)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.003996022910979833, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 17, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.47020952792024345, 'info': {'L1': 0.47020952792024345, 'L2': 0.021750703830932062, 'MAX': 0.27252317537003456, 'TrainTime': 140.890625}}\n",
      "exception: None\n",
      "\n",
      "14:04:34 job_callback for (1, 0, 4) started\n",
      "14:04:34 DISPATCHER: Trying to submit another job.\n",
      "14:04:34 job_callback for (1, 0, 4) got condition\n",
      "14:04:34 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:04:34 Only 5 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:04:34 HBMASTER: Trying to run another job!\n",
      "14:04:34 job_callback for (1, 0, 4) finished\n",
      "14:04:34 start sampling a new configuration.\n",
      "14:04:34 best_vector: [0, 1, 0.7888153309822793, 0.760638208549385, 0, 0.9829467371657401, 0.46715729035328235, 3], 1.4517683970912452e-30, 0.006888151043951599, -0.001347993438031858\n",
      "14:04:34 done sampling a new configuration.\n",
      "14:04:34 HBMASTER: schedule new run for iteration 1\n",
      "14:04:34 HBMASTER: trying submitting job (1, 0, 8) to dispatcher\n",
      "14:04:34 HBMASTER: submitting job (1, 0, 8) to dispatcher\n",
      "14:04:34 DISPATCHER: trying to submit job (1, 0, 8)\n",
      "14:04:34 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:04:34 HBMASTER: job (1, 0, 8) submitted to dispatcher\n",
      "14:04:34 DISPATCHER: Trying to submit another job.\n",
      "14:04:34 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:04:34 DISPATCHER: starting job (1, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:04:34 DISPATCHER: job (1, 0, 8) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:04:34 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:04:34 WORKER: start processing job (1, 0, 8)\n",
      "14:04:34 WORKER: args: ()\n",
      "14:04:34 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.007630318264638913, 'loss': 'mae', 'numLayers': 50, 'numNeurons': 52, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.0019888815004378557.\n",
      "Batch 0: Invalid loss, terminating training\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0009944407502189279.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:04:40 WORKER: done with job (1, 0, 7), trying to register it.\n",
      "14:04:40 WORKER: registered result for job (1, 0, 7) with dispatcher\n",
      "14:04:40 DISPATCHER: job (1, 0, 7) finished\n",
      "14:04:40 DISPATCHER: register_result: lock acquired\n",
      "14:04:40 DISPATCHER: job (1, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:04:40 job_id: (1, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 8, 'initial_lr': 0.003702558927863067, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 66, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': nan, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 22.828125}}\n",
      "exception: None\n",
      "\n",
      "14:04:40 job_callback for (1, 0, 7) started\n",
      "14:04:40 DISPATCHER: Trying to submit another job.\n",
      "14:04:40 job_callback for (1, 0, 7) got condition\n",
      "14:04:40 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:04:40 Only 6 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:04:40 HBMASTER: Trying to run another job!\n",
      "14:04:40 job_callback for (1, 0, 7) finished\n",
      "14:04:40 start sampling a new configuration.\n",
      "14:04:40 best_vector: [5, 3, 0.3424657876459445, 0.45655518073100826, 1, 0.1708168883799899, 0.1151269084242389, 3], 1.2217359647760856e-30, 0.008185074589199605, -0.003445513659000917\n",
      "14:04:40 done sampling a new configuration.\n",
      "14:04:40 HBMASTER: schedule new run for iteration 2\n",
      "14:04:40 HBMASTER: trying submitting job (2, 0, 0) to dispatcher\n",
      "14:04:40 HBMASTER: submitting job (2, 0, 0) to dispatcher\n",
      "14:04:40 DISPATCHER: trying to submit job (2, 0, 0)\n",
      "14:04:40 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:04:40 HBMASTER: job (2, 0, 0) submitted to dispatcher\n",
      "14:04:40 DISPATCHER: Trying to submit another job.\n",
      "14:04:40 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:04:40 DISPATCHER: starting job (2, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:04:40 DISPATCHER: job (2, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:04:40 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:04:40 WORKER: start processing job (2, 0, 0)\n",
      "14:04:40 WORKER: args: ()\n",
      "14:04:40 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.0046198962892369825, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 20, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.0004972203751094639.\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 0.00024861018755473197.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.00012430509377736598.\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 6.215254688868299e-05.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00309: ReduceLROnPlateau reducing learning rate to 3.1076273444341496e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:05:05 WORKER: done with job (1, 0, 5), trying to register it.\n",
      "14:05:05 WORKER: registered result for job (1, 0, 5) with dispatcher\n",
      "14:05:05 DISPATCHER: job (1, 0, 5) finished\n",
      "14:05:05 DISPATCHER: register_result: lock acquired\n",
      "14:05:05 DISPATCHER: job (1, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:05:05 job_id: (1, 0, 5)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.00397776315790712, 'loss': 'mse', 'numLayers': 43, 'numNeurons': 84, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 91.57962096142178, 'info': {'L1': 91.57962096142178, 'L2': 429.9328626525655, 'MAX': 4.998954750597477, 'TrainTime': 191.640625}}\n",
      "exception: None\n",
      "\n",
      "14:05:05 job_callback for (1, 0, 5) started\n",
      "14:05:05 DISPATCHER: Trying to submit another job.\n",
      "14:05:05 job_callback for (1, 0, 5) got condition\n",
      "14:05:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:05:05 Only 7 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:05:05 HBMASTER: Trying to run another job!\n",
      "14:05:05 job_callback for (1, 0, 5) finished\n",
      "14:05:05 start sampling a new configuration.\n",
      "14:05:05 best_vector: [3, 2, 0.5925065916730812, 0.7639808453382306, 0, 0.33395741548739144, 0.44979329875676244, 3], 1.9027576150999736e-31, 0.052555301424845886, -0.009193674280911899\n",
      "14:05:05 done sampling a new configuration.\n",
      "14:05:05 HBMASTER: schedule new run for iteration 2\n",
      "14:05:05 HBMASTER: trying submitting job (2, 0, 1) to dispatcher\n",
      "14:05:05 HBMASTER: submitting job (2, 0, 1) to dispatcher\n",
      "14:05:05 DISPATCHER: trying to submit job (2, 0, 1)\n",
      "14:05:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:05:05 HBMASTER: job (2, 0, 1) submitted to dispatcher\n",
      "14:05:05 DISPATCHER: Trying to submit another job.\n",
      "14:05:05 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:05:05 DISPATCHER: starting job (2, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:05:05 DISPATCHER: job (2, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:05:05 WORKER: start processing job (2, 0, 1)\n",
      "14:05:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:05:05 WORKER: args: ()\n",
      "14:05:05 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.007663410368848484, 'loss': 'mae', 'numLayers': 18, 'numNeurons': 50, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:05:16 WORKER: done with job (1, 0, 6), trying to register it.\n",
      "14:05:16 WORKER: registered result for job (1, 0, 6) with dispatcher\n",
      "14:05:16 DISPATCHER: job (1, 0, 6) finished\n",
      "14:05:16 DISPATCHER: register_result: lock acquired\n",
      "14:05:16 DISPATCHER: job (1, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:05:16 job_id: (1, 0, 6)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.004917984872254104, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 94, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 66.62856964286851, 'info': {'L1': 66.62856964286851, 'L2': 231.93156771054913, 'MAX': 3.746684193611145, 'TrainTime': 131.71875}}\n",
      "exception: None\n",
      "\n",
      "14:05:16 job_callback for (1, 0, 6) started\n",
      "14:05:16 DISPATCHER: Trying to submit another job.\n",
      "14:05:16 job_callback for (1, 0, 6) got condition\n",
      "14:05:16 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:05:16 Only 8 run(s) for budget 333.333333 available, need more than 10 -> can't build model!\n",
      "14:05:16 HBMASTER: Trying to run another job!\n",
      "14:05:16 job_callback for (1, 0, 6) finished\n",
      "14:05:16 start sampling a new configuration.\n",
      "14:05:16 done sampling a new configuration.\n",
      "14:05:16 HBMASTER: schedule new run for iteration 2\n",
      "14:05:16 HBMASTER: trying submitting job (2, 0, 2) to dispatcher\n",
      "14:05:16 HBMASTER: submitting job (2, 0, 2) to dispatcher\n",
      "14:05:17 DISPATCHER: trying to submit job (2, 0, 2)\n",
      "14:05:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:05:17 HBMASTER: job (2, 0, 2) submitted to dispatcher\n",
      "14:05:17 DISPATCHER: Trying to submit another job.\n",
      "14:05:17 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:05:17 DISPATCHER: starting job (2, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:05:17 DISPATCHER: job (2, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:05:17 WORKER: start processing job (2, 0, 2)\n",
      "14:05:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:05:17 WORKER: args: ()\n",
      "14:05:17 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.003757002732446554, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 90, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:05:30 DISPATCHER: Starting worker discovery\n",
      "14:05:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:05:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0038151592016220093.\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.0019075796008110046.\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0009537898004055023.\n",
      "\n",
      "Epoch 00218: ReduceLROnPlateau reducing learning rate to 0.00047689490020275116.\n",
      "\n",
      "Epoch 00252: ReduceLROnPlateau reducing learning rate to 0.00023844745010137558.\n",
      "\n",
      "Epoch 00289: ReduceLROnPlateau reducing learning rate to 0.00011922372505068779.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:06:30 DISPATCHER: Starting worker discovery\n",
      "14:06:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:06:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00323: ReduceLROnPlateau reducing learning rate to 5.9611862525343895e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:06:41 WORKER: done with job (1, 0, 8), trying to register it.\n",
      "14:06:41 WORKER: registered result for job (1, 0, 8) with dispatcher\n",
      "14:06:41 DISPATCHER: job (1, 0, 8) finished\n",
      "14:06:41 DISPATCHER: register_result: lock acquired\n",
      "14:06:41 DISPATCHER: job (1, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:06:41 job_id: (1, 0, 8)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.007630318264638913, 'loss': 'mae', 'numLayers': 50, 'numNeurons': 52, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 8.320452825552168, 'info': {'L1': 8.320452825552168, 'L2': 13.929303861763692, 'MAX': 4.9411480831020445, 'TrainTime': 331.515625}}\n",
      "exception: None\n",
      "\n",
      "14:06:41 job_callback for (1, 0, 8) started\n",
      "14:06:41 DISPATCHER: Trying to submit another job.\n",
      "14:06:41 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:06:41 job_callback for (1, 0, 8) got condition\n",
      "14:06:41 HBMASTER: Trying to run another job!\n",
      "14:06:41 job_callback for (1, 0, 8) finished\n",
      "14:06:41 ITERATION: Advancing config (1, 0, 0) to next budget 1000.000000\n",
      "14:06:41 ITERATION: Advancing config (1, 0, 3) to next budget 1000.000000\n",
      "14:06:41 ITERATION: Advancing config (1, 0, 4) to next budget 1000.000000\n",
      "14:06:41 HBMASTER: schedule new run for iteration 1\n",
      "14:06:41 HBMASTER: trying submitting job (1, 0, 0) to dispatcher\n",
      "14:06:41 HBMASTER: submitting job (1, 0, 0) to dispatcher\n",
      "14:06:41 DISPATCHER: trying to submit job (1, 0, 0)\n",
      "14:06:41 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:06:41 HBMASTER: job (1, 0, 0) submitted to dispatcher\n",
      "14:06:41 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:06:41 DISPATCHER: Trying to submit another job.\n",
      "14:06:41 DISPATCHER: starting job (1, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:06:41 DISPATCHER: job (1, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:06:41 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:06:41 WORKER: start processing job (1, 0, 0)\n",
      "14:06:41 WORKER: args: ()\n",
      "14:06:41 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.007592849911785462, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 39, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00553: ReduceLROnPlateau reducing learning rate to 0.0018785013817250729.\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.004557719454169273.\n",
      "\n",
      "Epoch 00404: ReduceLROnPlateau reducing learning rate to 0.003831705078482628.\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0022788597270846367.\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0011394298635423183.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:07:30 DISPATCHER: Starting worker discovery\n",
      "14:07:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:07:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0005697149317711592.\n",
      "\n",
      "Epoch 00808: ReduceLROnPlateau reducing learning rate to 0.0009392506908625364.\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.0002848574658855796.\n",
      "\n",
      "Epoch 00612: ReduceLROnPlateau reducing learning rate to 0.001915852539241314.\n",
      "\n",
      "Epoch 00949: ReduceLROnPlateau reducing learning rate to 0.0004696253454312682.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:08:03 WORKER: done with job (0, 0, 25), trying to register it.\n",
      "14:08:03 WORKER: registered result for job (0, 0, 25) with dispatcher\n",
      "14:08:03 DISPATCHER: job (0, 0, 25) finished\n",
      "14:08:03 DISPATCHER: register_result: lock acquired\n",
      "14:08:03 DISPATCHER: job (0, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:08:03 job_id: (0, 0, 25)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 10, 'initial_lr': 0.009115439145465235, 'loss': 'mse', 'numLayers': 44, 'numNeurons': 54, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 20.76623235707311, 'info': {'L1': 20.76623235707311, 'L2': 24.370426549402985, 'MAX': 3.739866773974237, 'TrainTime': 749.9375}}\n",
      "exception: None\n",
      "\n",
      "14:08:03 job_callback for (0, 0, 25) started\n",
      "14:08:03 DISPATCHER: Trying to submit another job.\n",
      "14:08:03 job_callback for (0, 0, 25) got condition\n",
      "14:08:03 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:08:03 done building a new model for budget 111.111111 based on 9/22 split\n",
      "Best loss for this budget:0.836914\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:08:03 HBMASTER: Trying to run another job!\n",
      "14:08:03 job_callback for (0, 0, 25) finished\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 0) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 4) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 9) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 10) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 14) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 19) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 20) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 21) to next budget 333.333333\n",
      "14:08:03 ITERATION: Advancing config (0, 0, 26) to next budget 333.333333\n",
      "14:08:03 HBMASTER: schedule new run for iteration 0\n",
      "14:08:03 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "14:08:03 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "14:08:03 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "14:08:03 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:08:03 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "14:08:03 DISPATCHER: Trying to submit another job.\n",
      "14:08:03 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:08:03 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:08:03 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:08:03 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:08:03 WORKER: start processing job (0, 0, 0)\n",
      "14:08:03 WORKER: args: ()\n",
      "14:08:03 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.006379384704252637, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 18, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:08:07 WORKER: done with job (2, 0, 2), trying to register it.\n",
      "14:08:07 WORKER: registered result for job (2, 0, 2) with dispatcher\n",
      "14:08:07 DISPATCHER: job (2, 0, 2) finished\n",
      "14:08:07 DISPATCHER: register_result: lock acquired\n",
      "14:08:07 DISPATCHER: job (2, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:08:07 job_id: (2, 0, 2)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 8, 'initial_lr': 0.003757002732446554, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 90, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 42.680732987234805, 'info': {'L1': 42.680732987234805, 'L2': 140.132459042576, 'MAX': 4.51085154228571, 'TrainTime': 507.734375}}\n",
      "exception: None\n",
      "\n",
      "14:08:07 job_callback for (2, 0, 2) started\n",
      "14:08:07 DISPATCHER: Trying to submit another job.\n",
      "14:08:07 job_callback for (2, 0, 2) got condition\n",
      "14:08:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:08:07 Only 1 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:08:07 HBMASTER: Trying to run another job!\n",
      "14:08:07 job_callback for (2, 0, 2) finished\n",
      "14:08:07 HBMASTER: schedule new run for iteration 0\n",
      "14:08:07 HBMASTER: trying submitting job (0, 0, 4) to dispatcher\n",
      "14:08:07 HBMASTER: submitting job (0, 0, 4) to dispatcher\n",
      "14:08:07 DISPATCHER: trying to submit job (0, 0, 4)\n",
      "14:08:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:08:07 HBMASTER: job (0, 0, 4) submitted to dispatcher\n",
      "14:08:07 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:08:07 DISPATCHER: Trying to submit another job.\n",
      "14:08:07 DISPATCHER: starting job (0, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:08:07 DISPATCHER: job (0, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:08:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:08:07 WORKER: start processing job (0, 0, 4)\n",
      "14:08:07 WORKER: args: ()\n",
      "14:08:07 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008237092039893007, 'loss': 'mse', 'numLayers': 20, 'numNeurons': 52, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00717: ReduceLROnPlateau reducing learning rate to 0.000957926269620657.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:08:30 DISPATCHER: Starting worker discovery\n",
      "14:08:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:08:30 DISPATCHER: Finished worker discovery\n",
      "14:09:05 WORKER: done with job (2, 0, 1), trying to register it.\n",
      "14:09:05 WORKER: registered result for job (2, 0, 1) with dispatcher\n",
      "14:09:05 DISPATCHER: job (2, 0, 1) finished\n",
      "14:09:05 DISPATCHER: register_result: lock acquired\n",
      "14:09:05 DISPATCHER: job (2, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:09:05 job_id: (2, 0, 1)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.007663410368848484, 'loss': 'mae', 'numLayers': 18, 'numNeurons': 50, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.9278469646237888, 'info': {'L1': 0.9278469646237888, 'L2': 0.049665588049522674, 'MAX': 0.1572892027013868, 'TrainTime': 712.625}}\n",
      "exception: None\n",
      "\n",
      "14:09:05 job_callback for (2, 0, 1) started\n",
      "14:09:05 DISPATCHER: Trying to submit another job.\n",
      "14:09:05 job_callback for (2, 0, 1) got condition\n",
      "14:09:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:09:05 Only 2 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:09:05 HBMASTER: Trying to run another job!\n",
      "14:09:05 job_callback for (2, 0, 1) finished\n",
      "14:09:05 HBMASTER: schedule new run for iteration 0\n",
      "14:09:05 HBMASTER: trying submitting job (0, 0, 9) to dispatcher\n",
      "14:09:05 HBMASTER: submitting job (0, 0, 9) to dispatcher\n",
      "14:09:05 DISPATCHER: trying to submit job (0, 0, 9)\n",
      "14:09:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:09:05 HBMASTER: job (0, 0, 9) submitted to dispatcher\n",
      "14:09:05 DISPATCHER: Trying to submit another job.\n",
      "14:09:05 DISPATCHER: starting job (0, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:09:05 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:09:05 DISPATCHER: job (0, 0, 9) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:09:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:09:05 WORKER: start processing job (0, 0, 9)\n",
      "14:09:05 WORKER: args: ()\n",
      "14:09:05 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0028890742802121614, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 97, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.004118545912206173.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:09:20 WORKER: done with job (2, 0, 0), trying to register it.\n",
      "14:09:20 WORKER: registered result for job (2, 0, 0) with dispatcher\n",
      "14:09:20 DISPATCHER: job (2, 0, 0) finished\n",
      "14:09:20 DISPATCHER: register_result: lock acquired\n",
      "14:09:20 DISPATCHER: job (2, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:09:20 job_id: (2, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.0046198962892369825, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 20, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 1.8066743494698252, 'info': {'L1': 1.8066743494698252, 'L2': 0.17692795699746156, 'MAX': 0.2132859230041504, 'TrainTime': 802.484375}}\n",
      "exception: None\n",
      "\n",
      "14:09:20 job_callback for (2, 0, 0) started\n",
      "14:09:20 DISPATCHER: Trying to submit another job.\n",
      "14:09:20 job_callback for (2, 0, 0) got condition\n",
      "14:09:20 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:09:20 Only 3 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:09:20 HBMASTER: Trying to run another job!\n",
      "14:09:20 job_callback for (2, 0, 0) finished\n",
      "14:09:20 HBMASTER: schedule new run for iteration 0\n",
      "14:09:20 HBMASTER: trying submitting job (0, 0, 10) to dispatcher\n",
      "14:09:20 HBMASTER: submitting job (0, 0, 10) to dispatcher\n",
      "14:09:20 DISPATCHER: trying to submit job (0, 0, 10)\n",
      "14:09:20 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:09:20 HBMASTER: job (0, 0, 10) submitted to dispatcher\n",
      "14:09:20 DISPATCHER: Trying to submit another job.\n",
      "14:09:20 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:09:20 DISPATCHER: starting job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:09:20 DISPATCHER: job (0, 0, 10) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:09:20 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:09:20 WORKER: start processing job (0, 0, 10)\n",
      "14:09:20 WORKER: args: ()\n",
      "14:09:20 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:09:30 DISPATCHER: Starting worker discovery\n",
      "14:09:30 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:09:30 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.0020592729561030865.\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 0.0010296364780515432.\n",
      "\n",
      "Epoch 00299: ReduceLROnPlateau reducing learning rate to 0.0005148182390257716.\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 0.001444537192583084.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:10:09 WORKER: done with job (0, 0, 4), trying to register it.\n",
      "14:10:09 WORKER: registered result for job (0, 0, 4) with dispatcher\n",
      "14:10:09 DISPATCHER: job (0, 0, 4) finished\n",
      "14:10:09 DISPATCHER: register_result: lock acquired\n",
      "14:10:09 DISPATCHER: job (0, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:10:09 job_id: (0, 0, 4)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008237092039893007, 'loss': 'mse', 'numLayers': 20, 'numNeurons': 52, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 1.5184245063651476, 'info': {'L1': 1.5184245063651476, 'L2': 0.1644151586722062, 'MAX': 0.24521116484337746, 'TrainTime': 363.09375}}\n",
      "exception: None\n",
      "\n",
      "14:10:09 job_callback for (0, 0, 4) started\n",
      "14:10:09 DISPATCHER: Trying to submit another job.\n",
      "14:10:09 job_callback for (0, 0, 4) got condition\n",
      "14:10:09 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:10:09 HBMASTER: Trying to run another job!\n",
      "14:10:09 job_callback for (0, 0, 4) finished\n",
      "14:10:09 HBMASTER: schedule new run for iteration 0\n",
      "14:10:09 HBMASTER: trying submitting job (0, 0, 14) to dispatcher\n",
      "14:10:09 HBMASTER: submitting job (0, 0, 14) to dispatcher\n",
      "14:10:09 DISPATCHER: trying to submit job (0, 0, 14)\n",
      "14:10:09 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:10:09 HBMASTER: job (0, 0, 14) submitted to dispatcher\n",
      "14:10:09 DISPATCHER: Trying to submit another job.\n",
      "14:10:09 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:10:09 DISPATCHER: starting job (0, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:10:09 DISPATCHER: job (0, 0, 14) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:10:09 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:10:09 WORKER: start processing job (0, 0, 14)\n",
      "14:10:09 WORKER: args: ()\n",
      "14:10:09 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.00634104168576129, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 0.000722268596291542.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:10:33 DISPATCHER: Starting worker discovery\n",
      "14:10:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:10:33 DISPATCHER: Finished worker discovery\n",
      "14:10:34 WORKER: done with job (1, 0, 0), trying to register it.\n",
      "14:10:34 WORKER: registered result for job (1, 0, 0) with dispatcher\n",
      "14:10:34 DISPATCHER: job (1, 0, 0) finished\n",
      "14:10:34 DISPATCHER: register_result: lock acquired\n",
      "14:10:34 DISPATCHER: job (1, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:10:34 job_id: (1, 0, 0)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.007592849911785462, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 39, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.6884440111580332, 'info': {'L1': 0.6884440111580332, 'L2': 0.02539797555316437, 'MAX': 0.242749582421121, 'TrainTime': 704.75}}\n",
      "exception: None\n",
      "\n",
      "14:10:34 job_callback for (1, 0, 0) started\n",
      "14:10:34 DISPATCHER: Trying to submit another job.\n",
      "14:10:34 job_callback for (1, 0, 0) got condition\n",
      "14:10:34 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:10:34 Only 4 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:10:34 HBMASTER: Trying to run another job!\n",
      "14:10:34 job_callback for (1, 0, 0) finished\n",
      "14:10:34 HBMASTER: schedule new run for iteration 0\n",
      "14:10:34 HBMASTER: trying submitting job (0, 0, 19) to dispatcher\n",
      "14:10:34 HBMASTER: submitting job (0, 0, 19) to dispatcher\n",
      "14:10:34 DISPATCHER: trying to submit job (0, 0, 19)\n",
      "14:10:34 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:10:34 HBMASTER: job (0, 0, 19) submitted to dispatcher\n",
      "14:10:34 DISPATCHER: Trying to submit another job.\n",
      "14:10:34 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:10:34 DISPATCHER: starting job (0, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:10:34 DISPATCHER: job (0, 0, 19) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:10:34 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:10:35 WORKER: start processing job (0, 0, 19)\n",
      "14:10:35 WORKER: args: ()\n",
      "14:10:35 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.007515325131023456, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 72, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00123: ReduceLROnPlateau reducing learning rate to 0.0028337726835161448.\n",
      "\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 0.000361134298145771.\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.003189692273736.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:10:50 WORKER: done with job (0, 0, 9), trying to register it.\n",
      "14:10:50 WORKER: registered result for job (0, 0, 9) with dispatcher\n",
      "14:10:50 DISPATCHER: job (0, 0, 9) finished\n",
      "14:10:50 DISPATCHER: register_result: lock acquired\n",
      "14:10:50 DISPATCHER: job (0, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:10:50 job_id: (0, 0, 9)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0028890742802121614, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 97, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 6.406957084343529, 'info': {'L1': 6.406957084343529, 'L2': 6.824974512951249, 'MAX': 2.2445300439767277, 'TrainTime': 315.484375}}\n",
      "exception: None\n",
      "\n",
      "14:10:50 job_callback for (0, 0, 9) started\n",
      "14:10:50 DISPATCHER: Trying to submit another job.\n",
      "14:10:50 job_callback for (0, 0, 9) got condition\n",
      "14:10:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:10:50 HBMASTER: Trying to run another job!\n",
      "14:10:50 job_callback for (0, 0, 9) finished\n",
      "14:10:50 HBMASTER: schedule new run for iteration 0\n",
      "14:10:50 HBMASTER: trying submitting job (0, 0, 20) to dispatcher\n",
      "14:10:50 HBMASTER: submitting job (0, 0, 20) to dispatcher\n",
      "14:10:50 DISPATCHER: trying to submit job (0, 0, 20)\n",
      "14:10:50 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:10:50 HBMASTER: job (0, 0, 20) submitted to dispatcher\n",
      "14:10:50 DISPATCHER: Trying to submit another job.\n",
      "14:10:50 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:10:50 DISPATCHER: starting job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:10:50 DISPATCHER: job (0, 0, 20) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:10:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:10:50 WORKER: start processing job (0, 0, 20)\n",
      "14:10:50 WORKER: args: ()\n",
      "14:10:50 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.001594846136868.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.000797423068434.\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.003757662605494261.\n",
      "\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 0.0014168863417580724.\n",
      "\n",
      "Epoch 00327: ReduceLROnPlateau reducing learning rate to 0.0007084431708790362.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:11:21 WORKER: done with job (0, 0, 10), trying to register it.\n",
      "14:11:21 WORKER: registered result for job (0, 0, 10) with dispatcher\n",
      "14:11:21 DISPATCHER: job (0, 0, 10) finished\n",
      "14:11:21 DISPATCHER: register_result: lock acquired\n",
      "14:11:21 DISPATCHER: job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:11:21 job_id: (0, 0, 10)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.789031541498542, 'info': {'L1': 0.789031541498542, 'L2': 0.03867037002968007, 'MAX': 0.1779929505699247, 'TrainTime': 360.890625}}\n",
      "exception: None\n",
      "\n",
      "14:11:21 job_callback for (0, 0, 10) started\n",
      "14:11:21 DISPATCHER: Trying to submit another job.\n",
      "14:11:21 job_callback for (0, 0, 10) got condition\n",
      "14:11:21 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:11:21 HBMASTER: Trying to run another job!\n",
      "14:11:21 job_callback for (0, 0, 10) finished\n",
      "14:11:21 HBMASTER: schedule new run for iteration 0\n",
      "14:11:21 HBMASTER: trying submitting job (0, 0, 21) to dispatcher\n",
      "14:11:21 HBMASTER: submitting job (0, 0, 21) to dispatcher\n",
      "14:11:21 DISPATCHER: trying to submit job (0, 0, 21)\n",
      "14:11:21 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:11:21 HBMASTER: job (0, 0, 21) submitted to dispatcher\n",
      "14:11:21 DISPATCHER: Trying to submit another job.\n",
      "14:11:21 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:11:21 DISPATCHER: starting job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:11:21 DISPATCHER: job (0, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:11:21 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:11:21 WORKER: start processing job (0, 0, 21)\n",
      "14:11:21 WORKER: args: ()\n",
      "14:11:21 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 0.0018788313027471304.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:11:33 DISPATCHER: Starting worker discovery\n",
      "14:11:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:11:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 0.0009394156513735652.\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0023639723658561707.\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0031705207657068968.\n",
      "\n",
      "Epoch 00329: ReduceLROnPlateau reducing learning rate to 0.0004697078256867826.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:12:07 WORKER: done with job (0, 0, 19), trying to register it.\n",
      "14:12:07 WORKER: registered result for job (0, 0, 19) with dispatcher\n",
      "14:12:07 DISPATCHER: job (0, 0, 19) finished\n",
      "14:12:07 DISPATCHER: register_result: lock acquired\n",
      "14:12:07 DISPATCHER: job (0, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:12:07 job_id: (0, 0, 19)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.007515325131023456, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 72, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 7.709705539269747, 'info': {'L1': 7.709705539269747, 'L2': 9.724764254501231, 'MAX': 2.9723638890416617, 'TrainTime': 270.890625}}\n",
      "exception: None\n",
      "\n",
      "14:12:07 job_callback for (0, 0, 19) started\n",
      "14:12:07 DISPATCHER: Trying to submit another job.\n",
      "14:12:07 job_callback for (0, 0, 19) got condition\n",
      "14:12:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:12:07 HBMASTER: Trying to run another job!\n",
      "14:12:07 job_callback for (0, 0, 19) finished\n",
      "14:12:07 HBMASTER: schedule new run for iteration 0\n",
      "14:12:07 HBMASTER: trying submitting job (0, 0, 26) to dispatcher\n",
      "14:12:07 HBMASTER: submitting job (0, 0, 26) to dispatcher\n",
      "14:12:07 DISPATCHER: trying to submit job (0, 0, 26)\n",
      "14:12:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:12:07 HBMASTER: job (0, 0, 26) submitted to dispatcher\n",
      "14:12:07 DISPATCHER: Trying to submit another job.\n",
      "14:12:07 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:12:07 DISPATCHER: starting job (0, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:12:07 DISPATCHER: job (0, 0, 26) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:12:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:12:07 WORKER: start processing job (0, 0, 26)\n",
      "14:12:07 WORKER: args: ()\n",
      "14:12:07 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.004559867250700915, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 70, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:12:22 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "14:12:22 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "14:12:22 DISPATCHER: job (0, 0, 0) finished\n",
      "14:12:22 DISPATCHER: register_result: lock acquired\n",
      "14:12:22 DISPATCHER: job (0, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:12:22 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.006379384704252637, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 18, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 1.5516616576718425, 'info': {'L1': 1.5516616576718425, 'L2': 0.18196600731778634, 'MAX': 0.31838274002075195, 'TrainTime': 717.59375}}\n",
      "exception: None\n",
      "\n",
      "14:12:22 job_callback for (0, 0, 0) started\n",
      "14:12:22 DISPATCHER: Trying to submit another job.\n",
      "14:12:22 job_callback for (0, 0, 0) got condition\n",
      "14:12:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:12:22 HBMASTER: Trying to run another job!\n",
      "14:12:22 job_callback for (0, 0, 0) finished\n",
      "14:12:22 HBMASTER: schedule new run for iteration 1\n",
      "14:12:22 HBMASTER: trying submitting job (1, 0, 3) to dispatcher\n",
      "14:12:22 HBMASTER: submitting job (1, 0, 3) to dispatcher\n",
      "14:12:22 DISPATCHER: trying to submit job (1, 0, 3)\n",
      "14:12:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:12:22 HBMASTER: job (1, 0, 3) submitted to dispatcher\n",
      "14:12:22 DISPATCHER: Trying to submit another job.\n",
      "14:12:22 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:12:22 DISPATCHER: starting job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:12:22 DISPATCHER: job (1, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:12:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:12:22 WORKER: start processing job (1, 0, 3)\n",
      "14:12:22 WORKER: args: ()\n",
      "14:12:22 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0012206739047542214.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:12:33 DISPATCHER: Starting worker discovery\n",
      "14:12:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:12:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0022799335420131683.\n",
      "\n",
      "Epoch 00251: ReduceLROnPlateau reducing learning rate to 0.0006103369523771107.\n",
      "\n",
      "Epoch 00243: ReduceLROnPlateau reducing learning rate to 0.0015852603828534484.\n",
      "\n",
      "Epoch 00286: ReduceLROnPlateau reducing learning rate to 0.00030516847618855536.\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.0011399667710065842.\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.0005699833855032921.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:13:28 WORKER: done with job (0, 0, 21), trying to register it.\n",
      "14:13:28 WORKER: registered result for job (0, 0, 21) with dispatcher\n",
      "14:13:28 DISPATCHER: job (0, 0, 21) finished\n",
      "14:13:28 DISPATCHER: register_result: lock acquired\n",
      "14:13:28 DISPATCHER: job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:13:28 job_id: (0, 0, 21)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.27841960930435145, 'info': {'L1': 0.27841960930435145, 'L2': 0.0058891257786734255, 'MAX': 0.19905910957985817, 'TrainTime': 355.734375}}\n",
      "exception: None\n",
      "\n",
      "14:13:28 job_callback for (0, 0, 21) started\n",
      "14:13:28 DISPATCHER: Trying to submit another job.\n",
      "14:13:28 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:13:28 job_callback for (0, 0, 21) got condition\n",
      "14:13:28 HBMASTER: Trying to run another job!\n",
      "14:13:28 job_callback for (0, 0, 21) finished\n",
      "14:13:28 HBMASTER: schedule new run for iteration 1\n",
      "14:13:28 HBMASTER: trying submitting job (1, 0, 4) to dispatcher\n",
      "14:13:28 HBMASTER: submitting job (1, 0, 4) to dispatcher\n",
      "14:13:28 DISPATCHER: trying to submit job (1, 0, 4)\n",
      "14:13:28 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:13:28 HBMASTER: job (1, 0, 4) submitted to dispatcher\n",
      "14:13:28 DISPATCHER: Trying to submit another job.\n",
      "14:13:28 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:13:28 DISPATCHER: starting job (1, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:13:28 DISPATCHER: job (1, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:13:28 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:13:28 WORKER: start processing job (1, 0, 4)\n",
      "14:13:28 WORKER: args: ()\n",
      "14:13:28 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.003996022910979833, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 17, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "14:13:29 WORKER: done with job (0, 0, 20), trying to register it.\n",
      "14:13:29 WORKER: registered result for job (0, 0, 20) with dispatcher\n",
      "14:13:29 DISPATCHER: job (0, 0, 20) finished\n",
      "14:13:29 DISPATCHER: register_result: lock acquired\n",
      "14:13:29 DISPATCHER: job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:13:29 job_id: (0, 0, 20)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.6028844497822456, 'info': {'L1': 0.6028844497822456, 'L2': 0.027436844587578342, 'MAX': 0.32453070152932106, 'TrainTime': 456.671875}}\n",
      "exception: None\n",
      "\n",
      "14:13:29 job_callback for (0, 0, 20) started\n",
      "14:13:29 job_callback for (0, 0, 20) got condition\n",
      "14:13:29 DISPATCHER: Trying to submit another job.\n",
      "14:13:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:13:29 HBMASTER: Trying to run another job!\n",
      "14:13:29 job_callback for (0, 0, 20) finished\n",
      "14:13:29 start sampling a new configuration.\n",
      "14:13:29 best_vector: [3, 2, 0.87766370302629, 0.5200995247023448, 0, 0.4103221063374297, 0.3341566172183224, 4], 1.0919903181079187e-31, 0.09157590350550825, -0.012898765540765132\n",
      "14:13:29 done sampling a new configuration.\n",
      "14:13:29 HBMASTER: schedule new run for iteration 2\n",
      "14:13:29 HBMASTER: trying submitting job (2, 0, 3) to dispatcher\n",
      "14:13:29 HBMASTER: submitting job (2, 0, 3) to dispatcher\n",
      "14:13:29 DISPATCHER: trying to submit job (2, 0, 3)\n",
      "14:13:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:13:29 HBMASTER: job (2, 0, 3) submitted to dispatcher\n",
      "14:13:29 DISPATCHER: Trying to submit another job.\n",
      "14:13:29 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:13:29 DISPATCHER: starting job (2, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:13:29 DISPATCHER: job (2, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:13:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:13:29 WORKER: start processing job (2, 0, 3)\n",
      "14:13:29 WORKER: args: ()\n",
      "14:13:29 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.005248985294553214, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 40, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00321: ReduceLROnPlateau reducing learning rate to 0.0007926301914267242.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:13:33 DISPATCHER: Starting worker discovery\n",
      "14:13:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:13:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 0.00028499169275164604.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:13:39 WORKER: done with job (0, 0, 14), trying to register it.\n",
      "14:13:39 WORKER: registered result for job (0, 0, 14) with dispatcher\n",
      "14:13:39 DISPATCHER: job (0, 0, 14) finished\n",
      "14:13:39 DISPATCHER: register_result: lock acquired\n",
      "14:13:39 DISPATCHER: job (0, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:13:39 job_id: (0, 0, 14)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 12, 'initial_lr': 0.00634104168576129, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 4.2948689543270735, 'info': {'L1': 4.2948689543270735, 'L2': 1.998122709353414, 'MAX': 0.9671696334448248, 'TrainTime': 600.59375}}\n",
      "exception: None\n",
      "\n",
      "14:13:39 job_callback for (0, 0, 14) started\n",
      "14:13:39 DISPATCHER: Trying to submit another job.\n",
      "14:13:39 job_callback for (0, 0, 14) got condition\n",
      "14:13:39 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:13:39 HBMASTER: Trying to run another job!\n",
      "14:13:39 job_callback for (0, 0, 14) finished\n",
      "14:13:39 start sampling a new configuration.\n",
      "14:13:39 best_vector: [7, 0, 0.16069812351336932, 0.19105723883959436, 0, 0.5343679804898493, 0.19964214103651023, 4], 6.416406932621779e-31, 0.01558504643643907, -0.0007739781661635457\n",
      "14:13:39 done sampling a new configuration.\n",
      "14:13:39 HBMASTER: schedule new run for iteration 2\n",
      "14:13:39 HBMASTER: trying submitting job (2, 0, 4) to dispatcher\n",
      "14:13:39 HBMASTER: submitting job (2, 0, 4) to dispatcher\n",
      "14:13:39 DISPATCHER: trying to submit job (2, 0, 4)\n",
      "14:13:39 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:13:39 HBMASTER: job (2, 0, 4) submitted to dispatcher\n",
      "14:13:39 DISPATCHER: Trying to submit another job.\n",
      "14:13:39 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:13:39 DISPATCHER: starting job (2, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:13:39 DISPATCHER: job (2, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:13:39 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:13:39 WORKER: start processing job (2, 0, 4)\n",
      "14:13:39 WORKER: args: ()\n",
      "14:13:39 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 6, 'initial_lr': 0.0019914666645119844, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 28, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00180: ReduceLROnPlateau reducing learning rate to 0.00014249584637582302.\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 7.124792318791151e-05.\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 3.5623961593955755e-05.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00377: ReduceLROnPlateau reducing learning rate to 0.0006675916374661028.\n",
      "\n",
      "Epoch 00282: ReduceLROnPlateau reducing learning rate to 1.7811980796977878e-05.\n",
      "\n",
      "Epoch 00316: ReduceLROnPlateau reducing learning rate to 8.905990398488939e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:14:06 WORKER: done with job (0, 0, 26), trying to register it.\n",
      "14:14:06 WORKER: registered result for job (0, 0, 26) with dispatcher\n",
      "14:14:06 DISPATCHER: job (0, 0, 26) finished\n",
      "14:14:06 DISPATCHER: register_result: lock acquired\n",
      "14:14:06 DISPATCHER: job (0, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:14:06 job_id: (0, 0, 26)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.004559867250700915, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 70, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 20.74560404777259, 'info': {'L1': 20.74560404777259, 'L2': 24.32664368336254, 'MAX': 3.74125913847619, 'TrainTime': 338.359375}}\n",
      "exception: None\n",
      "\n",
      "14:14:06 job_callback for (0, 0, 26) started\n",
      "14:14:06 DISPATCHER: Trying to submit another job.\n",
      "14:14:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:14:06 job_callback for (0, 0, 26) got condition\n",
      "14:14:06 done building a new model for budget 333.333333 based on 9/15 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:14:06 HBMASTER: Trying to run another job!\n",
      "14:14:06 job_callback for (0, 0, 26) finished\n",
      "14:14:06 ITERATION: Advancing config (0, 0, 10) to next budget 1000.000000\n",
      "14:14:06 ITERATION: Advancing config (0, 0, 20) to next budget 1000.000000\n",
      "14:14:06 ITERATION: Advancing config (0, 0, 21) to next budget 1000.000000\n",
      "14:14:06 HBMASTER: schedule new run for iteration 0\n",
      "14:14:06 HBMASTER: trying submitting job (0, 0, 10) to dispatcher\n",
      "14:14:06 HBMASTER: submitting job (0, 0, 10) to dispatcher\n",
      "14:14:06 DISPATCHER: trying to submit job (0, 0, 10)\n",
      "14:14:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:14:06 HBMASTER: job (0, 0, 10) submitted to dispatcher\n",
      "14:14:06 DISPATCHER: Trying to submit another job.\n",
      "14:14:06 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:14:06 DISPATCHER: starting job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:14:06 DISPATCHER: job (0, 0, 10) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:14:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:14:06 WORKER: start processing job (0, 0, 10)\n",
      "14:14:06 WORKER: args: ()\n",
      "14:14:06 WORKER: kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00236: ReduceLROnPlateau reducing learning rate to 0.001998011488467455.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00586: ReduceLROnPlateau reducing learning rate to 0.0003337958187330514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:14:33 DISPATCHER: Starting worker discovery\n",
      "14:14:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:14:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00761: ReduceLROnPlateau reducing learning rate to 0.0001668979093665257.\n",
      "\n",
      "Epoch 00865: ReduceLROnPlateau reducing learning rate to 8.344895468326285e-05.\n",
      "\n",
      "Epoch 00970: ReduceLROnPlateau reducing learning rate to 4.172447734163143e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:15:25 WORKER: done with job (1, 0, 3), trying to register it.\n",
      "14:15:25 WORKER: registered result for job (1, 0, 3) with dispatcher\n",
      "14:15:25 DISPATCHER: job (1, 0, 3) finished\n",
      "14:15:25 DISPATCHER: register_result: lock acquired\n",
      "14:15:25 DISPATCHER: job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:15:25 job_id: (1, 0, 3)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.17568715816533642, 'info': {'L1': 0.17568715816533642, 'L2': 0.003667222220234255, 'MAX': 0.16855002511673867, 'TrainTime': 533.984375}}\n",
      "exception: None\n",
      "\n",
      "14:15:25 job_callback for (1, 0, 3) started\n",
      "14:15:25 DISPATCHER: Trying to submit another job.\n",
      "14:15:25 job_callback for (1, 0, 3) got condition\n",
      "14:15:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:15:25 Only 5 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:15:25 HBMASTER: Trying to run another job!\n",
      "14:15:25 job_callback for (1, 0, 3) finished\n",
      "14:15:25 HBMASTER: schedule new run for iteration 0\n",
      "14:15:25 HBMASTER: trying submitting job (0, 0, 20) to dispatcher\n",
      "14:15:25 HBMASTER: submitting job (0, 0, 20) to dispatcher\n",
      "14:15:25 DISPATCHER: trying to submit job (0, 0, 20)\n",
      "14:15:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:15:25 HBMASTER: job (0, 0, 20) submitted to dispatcher\n",
      "14:15:25 DISPATCHER: Trying to submit another job.\n",
      "14:15:25 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:15:25 DISPATCHER: starting job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:15:25 DISPATCHER: job (0, 0, 20) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:15:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:15:25 WORKER: start processing job (0, 0, 20)\n",
      "14:15:25 WORKER: args: ()\n",
      "14:15:25 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:15:33 DISPATCHER: Starting worker discovery\n",
      "14:15:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:15:33 DISPATCHER: Finished worker discovery\n",
      "14:15:56 WORKER: done with job (1, 0, 4), trying to register it.\n",
      "14:15:56 WORKER: registered result for job (1, 0, 4) with dispatcher\n",
      "14:15:56 DISPATCHER: job (1, 0, 4) finished\n",
      "14:15:56 DISPATCHER: register_result: lock acquired\n",
      "14:15:56 DISPATCHER: job (1, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:15:56 job_id: (1, 0, 4)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.003996022910979833, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 17, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.8407887104334789, 'info': {'L1': 0.8407887104334789, 'L2': 0.04664326907287013, 'MAX': 0.27706448305779396, 'TrainTime': 449.4375}}\n",
      "exception: None\n",
      "\n",
      "14:15:56 job_callback for (1, 0, 4) started\n",
      "14:15:56 DISPATCHER: Trying to submit another job.\n",
      "14:15:56 job_callback for (1, 0, 4) got condition\n",
      "14:15:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:15:56 Only 6 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:15:56 HBMASTER: Trying to run another job!\n",
      "14:15:56 job_callback for (1, 0, 4) finished\n",
      "14:15:56 HBMASTER: schedule new run for iteration 0\n",
      "14:15:56 HBMASTER: trying submitting job (0, 0, 21) to dispatcher\n",
      "14:15:56 HBMASTER: submitting job (0, 0, 21) to dispatcher\n",
      "14:15:56 DISPATCHER: trying to submit job (0, 0, 21)\n",
      "14:15:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:15:56 HBMASTER: job (0, 0, 21) submitted to dispatcher\n",
      "14:15:56 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:15:56 DISPATCHER: Trying to submit another job.\n",
      "14:15:56 DISPATCHER: starting job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:15:56 DISPATCHER: job (0, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:15:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:15:56 WORKER: start processing job (0, 0, 21)\n",
      "14:15:56 WORKER: args: ()\n",
      "14:15:56 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00431: ReduceLROnPlateau reducing learning rate to 0.0028337726835161448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:16:33 DISPATCHER: Starting worker discovery\n",
      "14:16:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:16:33 DISPATCHER: Finished worker discovery\n",
      "14:16:58 WORKER: done with job (2, 0, 4), trying to register it.\n",
      "14:16:58 WORKER: registered result for job (2, 0, 4) with dispatcher\n",
      "14:16:58 DISPATCHER: job (2, 0, 4) finished\n",
      "14:16:58 DISPATCHER: register_result: lock acquired\n",
      "14:16:58 DISPATCHER: job (2, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:16:58 job_id: (2, 0, 4)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 5000, 'denspt': 6, 'initial_lr': 0.0019914666645119844, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 28, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 88.67697057941345, 'info': {'L1': 88.67697057941345, 'L2': 403.74761740909616, 'MAX': 4.853686049580574, 'TrainTime': 556.390625}}\n",
      "exception: None\n",
      "\n",
      "14:16:58 job_callback for (2, 0, 4) started\n",
      "14:16:58 DISPATCHER: Trying to submit another job.\n",
      "14:16:58 job_callback for (2, 0, 4) got condition\n",
      "14:16:58 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:16:58 Only 7 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:16:58 HBMASTER: Trying to run another job!\n",
      "14:16:58 job_callback for (2, 0, 4) finished\n",
      "14:16:58 ITERATION: Advancing config (1, 0, 3) to next budget 3000.000000\n",
      "14:16:58 HBMASTER: schedule new run for iteration 1\n",
      "14:16:58 HBMASTER: trying submitting job (1, 0, 3) to dispatcher\n",
      "14:16:58 HBMASTER: submitting job (1, 0, 3) to dispatcher\n",
      "14:16:58 DISPATCHER: trying to submit job (1, 0, 3)\n",
      "14:16:58 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:16:58 HBMASTER: job (1, 0, 3) submitted to dispatcher\n",
      "14:16:58 DISPATCHER: Trying to submit another job.\n",
      "14:16:58 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:16:58 DISPATCHER: starting job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:16:58 DISPATCHER: job (1, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:16:58 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:16:58 WORKER: start processing job (1, 0, 3)\n",
      "14:16:58 WORKER: args: ()\n",
      "14:16:58 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:17:33 WORKER: done with job (2, 0, 3), trying to register it.\n",
      "14:17:33 WORKER: registered result for job (2, 0, 3) with dispatcher\n",
      "14:17:33 DISPATCHER: job (2, 0, 3) finished\n",
      "14:17:33 DISPATCHER: register_result: lock acquired\n",
      "14:17:33 DISPATCHER: job (2, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:17:33 job_id: (2, 0, 3)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.005248985294553214, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 40, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 35.80450485747386, 'info': {'L1': 35.80450485747386, 'L2': 69.47555090083819, 'MAX': 2.854026596438226, 'TrainTime': 704.390625}}\n",
      "exception: None\n",
      "\n",
      "14:17:33 job_callback for (2, 0, 3) started\n",
      "14:17:33 DISPATCHER: Trying to submit another job.\n",
      "14:17:33 job_callback for (2, 0, 3) got condition\n",
      "14:17:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:17:33 Only 8 run(s) for budget 1000.000000 available, need more than 10 -> can't build model!\n",
      "14:17:33 HBMASTER: Trying to run another job!\n",
      "14:17:33 job_callback for (2, 0, 3) finished\n",
      "14:17:33 start sampling a new configuration.\n",
      "14:17:33 best_vector: [2, 1, 0.3076113807337819, 0.20106097629562708, 1, 0.21023819051259818, 0.8783575657910103, 4], 2.8451626097076135e-31, 0.035147375991376684, -0.001083872987725754\n",
      "14:17:33 done sampling a new configuration.\n",
      "14:17:33 HBMASTER: schedule new run for iteration 2\n",
      "14:17:33 HBMASTER: trying submitting job (2, 0, 5) to dispatcher\n",
      "14:17:33 HBMASTER: submitting job (2, 0, 5) to dispatcher\n",
      "14:17:33 DISPATCHER: trying to submit job (2, 0, 5)\n",
      "14:17:33 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:17:33 HBMASTER: job (2, 0, 5) submitted to dispatcher\n",
      "14:17:33 DISPATCHER: Trying to submit another job.\n",
      "14:17:33 DISPATCHER: starting job (2, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:17:33 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:17:33 DISPATCHER: job (2, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:17:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:17:33 WORKER: start processing job (2, 0, 5)\n",
      "14:17:33 WORKER: args: ()\n",
      "14:17:33 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.0020905036653267083, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "14:17:33 DISPATCHER: Starting worker discovery\n",
      "14:17:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:17:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00729: ReduceLROnPlateau reducing learning rate to 0.0014168863417580724.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00419: ReduceLROnPlateau reducing learning rate to 0.0012206739047542214.\n",
      "\n",
      "Epoch 00850: ReduceLROnPlateau reducing learning rate to 0.0007084431708790362.\n",
      "\n",
      "Epoch 00619: ReduceLROnPlateau reducing learning rate to 0.0023639723658561707.\n",
      "\n",
      "Epoch 00635: ReduceLROnPlateau reducing learning rate to 0.0006103369523771107.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:18:33 DISPATCHER: Starting worker discovery\n",
      "14:18:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:18:33 DISPATCHER: Finished worker discovery\n",
      "14:18:35 WORKER: done with job (0, 0, 10), trying to register it.\n",
      "14:18:35 WORKER: registered result for job (0, 0, 10) with dispatcher\n",
      "14:18:35 DISPATCHER: job (0, 0, 10) finished\n",
      "14:18:35 DISPATCHER: register_result: lock acquired\n",
      "14:18:35 DISPATCHER: job (0, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:18:35 job_id: (0, 0, 10)\n",
      "kwargs: {'config': {'activator': 'Addons>mish', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.0056675455594506615, 'loss': 'mae', 'numLayers': 25, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.34091099252309853, 'info': {'L1': 0.34091099252309853, 'L2': 0.009420701893040177, 'MAX': 0.18039370645218789, 'TrainTime': 767.453125}}\n",
      "exception: None\n",
      "\n",
      "14:18:35 job_callback for (0, 0, 10) started\n",
      "14:18:35 job_callback for (0, 0, 10) got condition\n",
      "14:18:35 DISPATCHER: Trying to submit another job.\n",
      "14:18:35 HBMASTER: Trying to run another job!\n",
      "14:18:35 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:18:35 job_callback for (0, 0, 10) finished\n",
      "14:18:35 start sampling a new configuration.\n",
      "14:18:35 best_vector: [2, 3, 0.7940469272943943, 0.8144757376938278, 0, 0.4054708815696641, 0.2917649007812772, 1], 1.0790250046098269e-31, 0.09267625826350502, -0.0009456797387900124\n",
      "14:18:35 done sampling a new configuration.\n",
      "14:18:35 HBMASTER: schedule new run for iteration 3\n",
      "14:18:35 HBMASTER: trying submitting job (3, 0, 0) to dispatcher\n",
      "14:18:35 HBMASTER: submitting job (3, 0, 0) to dispatcher\n",
      "14:18:35 DISPATCHER: trying to submit job (3, 0, 0)\n",
      "14:18:35 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:18:35 HBMASTER: job (3, 0, 0) submitted to dispatcher\n",
      "14:18:35 DISPATCHER: Trying to submit another job.\n",
      "14:18:35 DISPATCHER: starting job (3, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:18:35 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:18:35 DISPATCHER: job (3, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:18:35 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:18:35 WORKER: start processing job (3, 0, 0)\n",
      "14:18:35 WORKER: args: ()\n",
      "14:18:35 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.008163309803168895, 'loss': 'mae', 'numLayers': 21, 'numNeurons': 36, 'optimizer': 'RMSprop'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00583: ReduceLROnPlateau reducing learning rate to 0.0006675916374661028.\n",
      "\n",
      "Epoch 00808: ReduceLROnPlateau reducing learning rate to 0.00030516847618855536.\n",
      "\n",
      "Epoch 00914: ReduceLROnPlateau reducing learning rate to 0.00015258423809427768.\n",
      "\n",
      "Epoch 00713: ReduceLROnPlateau reducing learning rate to 0.0010452518472447991.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:19:19 WORKER: done with job (0, 0, 21), trying to register it.\n",
      "14:19:19 WORKER: registered result for job (0, 0, 21) with dispatcher\n",
      "14:19:19 DISPATCHER: job (0, 0, 21) finished\n",
      "14:19:19 DISPATCHER: register_result: lock acquired\n",
      "14:19:19 DISPATCHER: job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:19:19 job_id: (0, 0, 21)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.18335659137692217, 'info': {'L1': 0.18335659137692217, 'L2': 0.0024813682264962895, 'MAX': 0.18215803373986184, 'TrainTime': 577.640625}}\n",
      "exception: None\n",
      "\n",
      "14:19:19 job_callback for (0, 0, 21) started\n",
      "14:19:19 job_callback for (0, 0, 21) got condition\n",
      "14:19:19 DISPATCHER: Trying to submit another job.\n",
      "14:19:19 HBMASTER: Trying to run another job!\n",
      "14:19:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:19:19 job_callback for (0, 0, 21) finished\n",
      "14:19:19 start sampling a new configuration.\n",
      "14:19:19 best_vector: [2, 1, 0.714804216263184, 0.5335592901691789, 1, 0.39077703868056823, 0.18255582295222428, 0], 1.366814880830232e-31, 0.07316279724673316, -0.003823938640061799\n",
      "14:19:19 done sampling a new configuration.\n",
      "14:19:19 HBMASTER: schedule new run for iteration 3\n",
      "14:19:19 HBMASTER: trying submitting job (3, 0, 1) to dispatcher\n",
      "14:19:19 HBMASTER: submitting job (3, 0, 1) to dispatcher\n",
      "14:19:19 DISPATCHER: trying to submit job (3, 0, 1)\n",
      "14:19:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:19:19 HBMASTER: job (3, 0, 1) submitted to dispatcher\n",
      "14:19:19 DISPATCHER: Trying to submit another job.\n",
      "14:19:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:19:19 DISPATCHER: starting job (3, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:19:19 DISPATCHER: job (3, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:19:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:19:19 WORKER: start processing job (3, 0, 1)\n",
      "14:19:19 WORKER: args: ()\n",
      "14:19:19 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0053822369726748715, 'loss': 'mse', 'numLayers': 21, 'numNeurons': 26, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:19:27 WORKER: done with job (0, 0, 20), trying to register it.\n",
      "14:19:27 WORKER: registered result for job (0, 0, 20) with dispatcher\n",
      "14:19:27 DISPATCHER: job (0, 0, 20) finished\n",
      "14:19:27 DISPATCHER: register_result: lock acquired\n",
      "14:19:27 DISPATCHER: job (0, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:19:27 job_id: (0, 0, 20)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004727944928451344, 'loss': 'mse', 'numLayers': 26, 'numNeurons': 14, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.5377593646565495, 'info': {'L1': 0.5377593646565495, 'L2': 0.016164159001144207, 'MAX': 0.15629200209313332, 'TrainTime': 697.625}}\n",
      "exception: None\n",
      "\n",
      "14:19:27 job_callback for (0, 0, 20) started\n",
      "14:19:27 DISPATCHER: Trying to submit another job.\n",
      "14:19:27 job_callback for (0, 0, 20) got condition\n",
      "14:19:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:19:27 HBMASTER: Trying to run another job!\n",
      "14:19:27 job_callback for (0, 0, 20) finished\n",
      "14:19:27 ITERATION: Advancing config (0, 0, 21) to next budget 3000.000000\n",
      "14:19:27 HBMASTER: schedule new run for iteration 0\n",
      "14:19:27 HBMASTER: trying submitting job (0, 0, 21) to dispatcher\n",
      "14:19:27 HBMASTER: submitting job (0, 0, 21) to dispatcher\n",
      "14:19:27 DISPATCHER: trying to submit job (0, 0, 21)\n",
      "14:19:27 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:19:27 HBMASTER: job (0, 0, 21) submitted to dispatcher\n",
      "14:19:27 DISPATCHER: Trying to submit another job.\n",
      "14:19:27 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:19:27 DISPATCHER: starting job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:19:27 DISPATCHER: job (0, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:19:27 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:19:27 WORKER: start processing job (0, 0, 21)\n",
      "14:19:27 WORKER: args: ()\n",
      "14:19:27 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00813: ReduceLROnPlateau reducing learning rate to 0.0005226259236223996.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:19:33 DISPATCHER: Starting worker discovery\n",
      "14:19:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:19:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00913: ReduceLROnPlateau reducing learning rate to 0.0002613129618111998.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:19:50 WORKER: done with job (2, 0, 5), trying to register it.\n",
      "14:19:50 WORKER: registered result for job (2, 0, 5) with dispatcher\n",
      "14:19:50 DISPATCHER: job (2, 0, 5) finished\n",
      "14:19:50 DISPATCHER: register_result: lock acquired\n",
      "14:19:50 DISPATCHER: job (2, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:19:50 job_id: (2, 0, 5)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.0020905036653267083, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 91.18766242287471, 'info': {'L1': 91.18766242287471, 'L2': 426.34908390793544, 'MAX': 4.979346305131912, 'TrainTime': 401.421875}}\n",
      "exception: None\n",
      "\n",
      "14:19:50 job_callback for (2, 0, 5) started\n",
      "14:19:50 DISPATCHER: Trying to submit another job.\n",
      "14:19:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:19:50 job_callback for (2, 0, 5) got condition\n",
      "14:19:50 HBMASTER: Trying to run another job!\n",
      "14:19:50 job_callback for (2, 0, 5) finished\n",
      "14:19:50 ITERATION: Advancing config (2, 0, 0) to next budget 3000.000000\n",
      "14:19:50 ITERATION: Advancing config (2, 0, 1) to next budget 3000.000000\n",
      "14:19:50 HBMASTER: schedule new run for iteration 2\n",
      "14:19:50 HBMASTER: trying submitting job (2, 0, 0) to dispatcher\n",
      "14:19:50 HBMASTER: submitting job (2, 0, 0) to dispatcher\n",
      "14:19:50 DISPATCHER: trying to submit job (2, 0, 0)\n",
      "14:19:50 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:19:50 HBMASTER: job (2, 0, 0) submitted to dispatcher\n",
      "14:19:50 DISPATCHER: Trying to submit another job.\n",
      "14:19:50 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:19:50 DISPATCHER: starting job (2, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:19:50 DISPATCHER: job (2, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:19:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:19:50 WORKER: start processing job (2, 0, 0)\n",
      "14:19:50 WORKER: args: ()\n",
      "14:19:50 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.0046198962892369825, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 20, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 01432: ReduceLROnPlateau reducing learning rate to 0.0003337958187330514.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:20:33 DISPATCHER: Starting worker discovery\n",
      "14:20:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:20:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00887: ReduceLROnPlateau reducing learning rate to 0.004081654828041792.\n",
      "\n",
      "Epoch 01187: ReduceLROnPlateau reducing learning rate to 0.002040827414020896.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:21:33 DISPATCHER: Starting worker discovery\n",
      "14:21:33 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:21:33 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01487: ReduceLROnPlateau reducing learning rate to 0.001020413707010448.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:22:33 DISPATCHER: Starting worker discovery\n",
      "14:22:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:22:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01787: ReduceLROnPlateau reducing learning rate to 0.000510206853505224.\n",
      "\n",
      "Epoch 02624: ReduceLROnPlateau reducing learning rate to 0.0001668979093665257.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:23:34 DISPATCHER: Starting worker discovery\n",
      "14:23:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:23:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02087: ReduceLROnPlateau reducing learning rate to 0.000255103426752612.\n",
      "\n",
      "Epoch 01634: ReduceLROnPlateau reducing learning rate to 0.0012206739047542214.\n",
      "\n",
      "Epoch 02993: ReduceLROnPlateau reducing learning rate to 8.344895468326285e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:23:57 WORKER: done with job (1, 0, 3), trying to register it.\n",
      "14:23:57 WORKER: registered result for job (1, 0, 3) with dispatcher\n",
      "14:23:57 DISPATCHER: job (1, 0, 3) finished\n",
      "14:23:57 DISPATCHER: register_result: lock acquired\n",
      "14:23:57 DISPATCHER: job (1, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:23:57 job_id: (1, 0, 3)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0013351833274614245, 'loss': 'mae', 'numLayers': 7, 'numNeurons': 71, 'optimizer': 'RMSprop'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.1791283158481637, 'info': {'L1': 0.1791283158481637, 'L2': 0.002609698159279119, 'MAX': 0.10238757301503121, 'TrainTime': 1215.640625}}\n",
      "exception: None\n",
      "\n",
      "14:23:57 job_callback for (1, 0, 3) started\n",
      "14:23:57 DISPATCHER: Trying to submit another job.\n",
      "14:23:57 job_callback for (1, 0, 3) got condition\n",
      "14:23:57 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:23:57 Only 1 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:23:57 HBMASTER: Trying to run another job!\n",
      "14:23:57 job_callback for (1, 0, 3) finished\n",
      "14:23:57 HBMASTER: schedule new run for iteration 2\n",
      "14:23:57 HBMASTER: trying submitting job (2, 0, 1) to dispatcher\n",
      "14:23:57 HBMASTER: submitting job (2, 0, 1) to dispatcher\n",
      "14:23:57 DISPATCHER: trying to submit job (2, 0, 1)\n",
      "14:23:57 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:23:57 HBMASTER: job (2, 0, 1) submitted to dispatcher\n",
      "14:23:57 DISPATCHER: Trying to submit another job.\n",
      "14:23:57 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:23:57 DISPATCHER: starting job (2, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:23:57 DISPATCHER: job (2, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:23:57 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:23:57 WORKER: start processing job (2, 0, 1)\n",
      "14:23:57 WORKER: args: ()\n",
      "14:23:57 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.007663410368848484, 'loss': 'mae', 'numLayers': 18, 'numNeurons': 50, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02387: ReduceLROnPlateau reducing learning rate to 0.000127551713376306.\n",
      "\n",
      "Epoch 01937: ReduceLROnPlateau reducing learning rate to 0.0006103369523771107.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:24:34 DISPATCHER: Starting worker discovery\n",
      "14:24:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:24:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02687: ReduceLROnPlateau reducing learning rate to 6.3775856688153e-05.\n",
      "\n",
      "Epoch 02243: ReduceLROnPlateau reducing learning rate to 0.00030516847618855536.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:25:34 DISPATCHER: Starting worker discovery\n",
      "14:25:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:25:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01495: ReduceLROnPlateau reducing learning rate to 0.002309948205947876.\n",
      "\n",
      "Epoch 02987: ReduceLROnPlateau reducing learning rate to 3.18879283440765e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:25:52 WORKER: done with job (3, 0, 0), trying to register it.\n",
      "14:25:52 WORKER: registered result for job (3, 0, 0) with dispatcher\n",
      "14:25:52 DISPATCHER: job (3, 0, 0) finished\n",
      "14:25:52 DISPATCHER: register_result: lock acquired\n",
      "14:25:52 DISPATCHER: job (3, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:25:52 job_id: (3, 0, 0)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.008163309803168895, 'loss': 'mae', 'numLayers': 21, 'numNeurons': 36, 'optimizer': 'RMSprop'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 7.763674008266937, 'info': {'L1': 7.763674008266937, 'L2': 10.491370784092855, 'MAX': 3.7347775765326015, 'TrainTime': 1272.28125}}\n",
      "exception: None\n",
      "\n",
      "14:25:52 job_callback for (3, 0, 0) started\n",
      "14:25:52 DISPATCHER: Trying to submit another job.\n",
      "14:25:52 job_callback for (3, 0, 0) got condition\n",
      "14:25:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:25:52 Only 2 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:25:52 HBMASTER: Trying to run another job!\n",
      "14:25:52 job_callback for (3, 0, 0) finished\n",
      "14:25:52 start sampling a new configuration.\n",
      "14:25:52 best_vector: [2, 2, 0.8944985472999868, 0.7510300512058348, 1, 0.47565848093434226, 0.1923101044579285, 0], 1.0522140418862837e-31, 0.09503769767292969, -0.0035362577609404835\n",
      "14:25:52 done sampling a new configuration.\n",
      "14:25:52 HBMASTER: schedule new run for iteration 3\n",
      "14:25:52 HBMASTER: trying submitting job (3, 0, 2) to dispatcher\n",
      "14:25:52 HBMASTER: submitting job (3, 0, 2) to dispatcher\n",
      "14:25:52 DISPATCHER: trying to submit job (3, 0, 2)\n",
      "14:25:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:25:52 HBMASTER: job (3, 0, 2) submitted to dispatcher\n",
      "14:25:52 DISPATCHER: Trying to submit another job.\n",
      "14:25:52 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:25:52 DISPATCHER: starting job (3, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:25:52 DISPATCHER: job (3, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:25:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:25:52 WORKER: start processing job (3, 0, 2)\n",
      "14:25:52 WORKER: args: ()\n",
      "14:25:52 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.007535197506937765, 'loss': 'mse', 'numLayers': 25, 'numNeurons': 27, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02549: ReduceLROnPlateau reducing learning rate to 0.00015258423809427768.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:26:34 DISPATCHER: Starting worker discovery\n",
      "14:26:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:26:34 DISPATCHER: Finished worker discovery\n",
      "14:26:45 WORKER: done with job (3, 0, 1), trying to register it.\n",
      "14:26:45 WORKER: registered result for job (3, 0, 1) with dispatcher\n",
      "14:26:45 DISPATCHER: job (3, 0, 1) finished\n",
      "14:26:45 DISPATCHER: register_result: lock acquired\n",
      "14:26:45 DISPATCHER: job (3, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:26:45 job_id: (3, 0, 1)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0053822369726748715, 'loss': 'mse', 'numLayers': 21, 'numNeurons': 26, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 1.928119138281491, 'info': {'L1': 1.928119138281491, 'L2': 0.2916066972551467, 'MAX': 0.31440626850712317, 'TrainTime': 1295.625}}\n",
      "exception: None\n",
      "\n",
      "14:26:45 job_callback for (3, 0, 1) started\n",
      "14:26:45 DISPATCHER: Trying to submit another job.\n",
      "14:26:45 job_callback for (3, 0, 1) got condition\n",
      "14:26:45 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:26:45 Only 3 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:26:45 HBMASTER: Trying to run another job!\n",
      "14:26:45 job_callback for (3, 0, 1) finished\n",
      "14:26:45 start sampling a new configuration.\n",
      "14:26:45 best_vector: [0, 0, 0.8671793462878348, 0.9761399349262974, 0, 0.5134531383441902, 0.6026054115399864, 2], 2.7666159194538586e-31, 0.03614524130250086, -0.0005609463323562026\n",
      "14:26:45 done sampling a new configuration.\n",
      "14:26:45 HBMASTER: schedule new run for iteration 3\n",
      "14:26:45 HBMASTER: trying submitting job (3, 0, 3) to dispatcher\n",
      "14:26:45 HBMASTER: submitting job (3, 0, 3) to dispatcher\n",
      "14:26:45 DISPATCHER: trying to submit job (3, 0, 3)\n",
      "14:26:45 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:26:45 HBMASTER: job (3, 0, 3) submitted to dispatcher\n",
      "14:26:45 DISPATCHER: Trying to submit another job.\n",
      "14:26:45 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:26:45 DISPATCHER: starting job (3, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:26:45 DISPATCHER: job (3, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:26:45 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:26:45 WORKER: start processing job (3, 0, 3)\n",
      "14:26:45 WORKER: args: ()\n",
      "14:26:45 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009763785355770344, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 64, 'optimizer': 'SGD'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02911: ReduceLROnPlateau reducing learning rate to 7.629211904713884e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:27:34 DISPATCHER: Starting worker discovery\n",
      "14:27:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:27:34 DISPATCHER: Finished worker discovery\n",
      "14:27:41 WORKER: done with job (0, 0, 21), trying to register it.\n",
      "14:27:41 WORKER: registered result for job (0, 0, 21) with dispatcher\n",
      "14:27:41 DISPATCHER: job (0, 0, 21) finished\n",
      "14:27:41 DISPATCHER: register_result: lock acquired\n",
      "14:27:41 DISPATCHER: job (0, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:27:41 job_id: (0, 0, 21)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.002441347874061673, 'loss': 'mae', 'numLayers': 5, 'numNeurons': 78, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.22355948691528438, 'info': {'L1': 0.22355948691528438, 'L2': 0.007853749646245715, 'MAX': 0.17764815646623688, 'TrainTime': 1436.015625}}\n",
      "exception: None\n",
      "\n",
      "14:27:41 job_callback for (0, 0, 21) started\n",
      "14:27:41 DISPATCHER: Trying to submit another job.\n",
      "14:27:41 job_callback for (0, 0, 21) got condition\n",
      "14:27:41 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:27:41 Only 4 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:27:41 HBMASTER: Trying to run another job!\n",
      "14:27:41 job_callback for (0, 0, 21) finished\n",
      "14:27:41 start sampling a new configuration.\n",
      "14:27:41 done sampling a new configuration.\n",
      "14:27:41 HBMASTER: schedule new run for iteration 4\n",
      "14:27:41 HBMASTER: trying submitting job (4, 0, 0) to dispatcher\n",
      "14:27:41 HBMASTER: submitting job (4, 0, 0) to dispatcher\n",
      "14:27:41 DISPATCHER: trying to submit job (4, 0, 0)\n",
      "14:27:41 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:27:41 HBMASTER: job (4, 0, 0) submitted to dispatcher\n",
      "14:27:41 DISPATCHER: Trying to submit another job.\n",
      "14:27:41 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:27:41 DISPATCHER: starting job (4, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:27:41 DISPATCHER: job (4, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:27:41 WORKER: start processing job (4, 0, 0)\n",
      "14:27:41 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:27:41 WORKER: args: ()\n",
      "14:27:41 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0028114216644085964, 'loss': 'mse', 'numLayers': 11, 'numNeurons': 77, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0014057108201086521.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0007028554100543261.\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.00035142770502716303.\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00017571385251358151.\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 8.785692625679076e-05.\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 4.392846312839538e-05.\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 2.196423156419769e-05.\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.0982115782098845e-05.\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 5.491057891049422e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:28:02 WORKER: done with job (4, 0, 0), trying to register it.\n",
      "14:28:02 WORKER: registered result for job (4, 0, 0) with dispatcher\n",
      "14:28:02 DISPATCHER: job (4, 0, 0) finished\n",
      "14:28:02 DISPATCHER: register_result: lock acquired\n",
      "14:28:02 DISPATCHER: job (4, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:28:02 job_id: (4, 0, 0)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0028114216644085964, 'loss': 'mse', 'numLayers': 11, 'numNeurons': 77, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 91.59883807995534, 'info': {'L1': 91.59883807995534, 'L2': 430.1088707605753, 'MAX': 4.999915606524155, 'TrainTime': 52.25}}\n",
      "exception: None\n",
      "\n",
      "14:28:02 job_callback for (4, 0, 0) started\n",
      "14:28:02 DISPATCHER: Trying to submit another job.\n",
      "14:28:02 job_callback for (4, 0, 0) got condition\n",
      "14:28:02 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:28:02 HBMASTER: Trying to run another job!\n",
      "14:28:02 job_callback for (4, 0, 0) finished\n",
      "14:28:02 start sampling a new configuration.\n",
      "14:28:02 done sampling a new configuration.\n",
      "14:28:02 HBMASTER: schedule new run for iteration 4\n",
      "14:28:02 HBMASTER: trying submitting job (4, 0, 1) to dispatcher\n",
      "14:28:02 HBMASTER: submitting job (4, 0, 1) to dispatcher\n",
      "14:28:02 DISPATCHER: trying to submit job (4, 0, 1)\n",
      "14:28:02 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:28:02 HBMASTER: job (4, 0, 1) submitted to dispatcher\n",
      "14:28:02 DISPATCHER: Trying to submit another job.\n",
      "14:28:02 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:28:02 DISPATCHER: starting job (4, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:28:02 WORKER: start processing job (4, 0, 1)\n",
      "14:28:02 DISPATCHER: job (4, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:28:02 WORKER: args: ()\n",
      "14:28:02 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:28:02 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.006393995563887887, 'loss': 'mae', 'numLayers': 45, 'numNeurons': 73, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02148: ReduceLROnPlateau reducing learning rate to 0.001154974102973938.\n",
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:28:34 DISPATCHER: Starting worker discovery\n",
      "14:28:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:28:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01521: ReduceLROnPlateau reducing learning rate to 0.003831705078482628.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:29:34 DISPATCHER: Starting worker discovery\n",
      "14:29:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:29:34 DISPATCHER: Finished worker discovery\n",
      "14:29:40 WORKER: done with job (4, 0, 1), trying to register it.\n",
      "14:29:40 WORKER: registered result for job (4, 0, 1) with dispatcher\n",
      "14:29:40 DISPATCHER: job (4, 0, 1) finished\n",
      "14:29:40 DISPATCHER: register_result: lock acquired\n",
      "14:29:40 DISPATCHER: job (4, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:29:40 job_id: (4, 0, 1)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.006393995563887887, 'loss': 'mae', 'numLayers': 45, 'numNeurons': 73, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 50.96381406040422, 'info': {'L1': 50.96381406040422, 'L2': 138.15969612222776, 'MAX': 2.9455251693725586, 'TrainTime': 259.578125}}\n",
      "exception: None\n",
      "\n",
      "14:29:40 job_callback for (4, 0, 1) started\n",
      "14:29:40 DISPATCHER: Trying to submit another job.\n",
      "14:29:40 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:29:40 job_callback for (4, 0, 1) got condition\n",
      "14:29:40 HBMASTER: Trying to run another job!\n",
      "14:29:40 job_callback for (4, 0, 1) finished\n",
      "14:29:40 start sampling a new configuration.\n",
      "14:29:40 best_vector: [4, 0, 0.6385512690365569, 0.9833589425162849, 0, 0.4403719060986833, 0.9734331712037549, 3], 3.2094299950261984e-30, 0.003115818078443045, -7.117778366324488e-05\n",
      "14:29:40 done sampling a new configuration.\n",
      "14:29:40 HBMASTER: schedule new run for iteration 4\n",
      "14:29:40 HBMASTER: trying submitting job (4, 0, 2) to dispatcher\n",
      "14:29:40 HBMASTER: submitting job (4, 0, 2) to dispatcher\n",
      "14:29:40 DISPATCHER: trying to submit job (4, 0, 2)\n",
      "14:29:40 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:29:40 HBMASTER: job (4, 0, 2) submitted to dispatcher\n",
      "14:29:40 DISPATCHER: Trying to submit another job.\n",
      "14:29:40 DISPATCHER: starting job (4, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:29:40 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:29:40 DISPATCHER: job (4, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:29:40 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:29:41 WORKER: start processing job (4, 0, 2)\n",
      "14:29:41 WORKER: args: ()\n",
      "14:29:41 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.00983525353091122, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 98, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0049176267348229885.\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0024588133674114943.\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0012294066837057471.\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0006147033418528736.\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.0003073516709264368.\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0001536758354632184.\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 7.68379177316092e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:30:23 WORKER: done with job (4, 0, 2), trying to register it.\n",
      "14:30:23 WORKER: registered result for job (4, 0, 2) with dispatcher\n",
      "14:30:23 DISPATCHER: job (4, 0, 2) finished\n",
      "14:30:23 DISPATCHER: register_result: lock acquired\n",
      "14:30:23 DISPATCHER: job (4, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:30:23 job_id: (4, 0, 2)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.00983525353091122, 'loss': 'mae', 'numLayers': 23, 'numNeurons': 98, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.229928531635855, 'info': {'L1': 8.229928531635855, 'L2': 13.941773388941831, 'MAX': 4.9791777538173765, 'TrainTime': 112.703125}}\n",
      "exception: None\n",
      "\n",
      "14:30:23 job_callback for (4, 0, 2) started\n",
      "14:30:23 DISPATCHER: Trying to submit another job.\n",
      "14:30:23 job_callback for (4, 0, 2) got condition\n",
      "14:30:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:30:23 HBMASTER: Trying to run another job!\n",
      "14:30:23 job_callback for (4, 0, 2) finished\n",
      "14:30:23 start sampling a new configuration.\n",
      "14:30:23 done sampling a new configuration.\n",
      "14:30:23 HBMASTER: schedule new run for iteration 4\n",
      "14:30:23 HBMASTER: trying submitting job (4, 0, 3) to dispatcher\n",
      "14:30:23 HBMASTER: submitting job (4, 0, 3) to dispatcher\n",
      "14:30:23 DISPATCHER: trying to submit job (4, 0, 3)\n",
      "14:30:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:30:23 HBMASTER: job (4, 0, 3) submitted to dispatcher\n",
      "14:30:23 DISPATCHER: Trying to submit another job.\n",
      "14:30:23 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:30:23 DISPATCHER: starting job (4, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:30:23 DISPATCHER: job (4, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:30:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:30:23 WORKER: start processing job (4, 0, 3)\n",
      "14:30:23 WORKER: args: ()\n",
      "14:30:23 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.0028871446357014397, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 01415: ReduceLROnPlateau reducing learning rate to 0.0037675986532121897.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:30:34 DISPATCHER: Starting worker discovery\n",
      "14:30:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:30:34 DISPATCHER: Finished worker discovery\n",
      "14:30:58 WORKER: done with job (4, 0, 3), trying to register it.\n",
      "14:30:58 WORKER: registered result for job (4, 0, 3) with dispatcher\n",
      "14:30:58 DISPATCHER: job (4, 0, 3) finished\n",
      "14:30:58 DISPATCHER: register_result: lock acquired\n",
      "14:30:58 DISPATCHER: job (4, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:30:58 job_id: (4, 0, 3)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.0028871446357014397, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 31, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 38.79483035405536, 'info': {'L1': 38.79483035405536, 'L2': 81.27352748137656, 'MAX': 2.6899738715999693, 'TrainTime': 85.828125}}\n",
      "exception: None\n",
      "\n",
      "14:30:58 job_callback for (4, 0, 3) started\n",
      "14:30:58 DISPATCHER: Trying to submit another job.\n",
      "14:30:58 job_callback for (4, 0, 3) got condition\n",
      "14:30:58 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:30:58 HBMASTER: Trying to run another job!\n",
      "14:30:58 job_callback for (4, 0, 3) finished\n",
      "14:30:58 start sampling a new configuration.\n",
      "14:30:58 done sampling a new configuration.\n",
      "14:30:58 HBMASTER: schedule new run for iteration 4\n",
      "14:30:58 HBMASTER: trying submitting job (4, 0, 4) to dispatcher\n",
      "14:30:58 HBMASTER: submitting job (4, 0, 4) to dispatcher\n",
      "14:30:58 DISPATCHER: trying to submit job (4, 0, 4)\n",
      "14:30:58 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:30:58 HBMASTER: job (4, 0, 4) submitted to dispatcher\n",
      "14:30:58 DISPATCHER: Trying to submit another job.\n",
      "14:30:58 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:30:58 DISPATCHER: starting job (4, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:30:58 DISPATCHER: job (4, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:30:58 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:30:58 WORKER: start processing job (4, 0, 4)\n",
      "14:30:58 WORKER: args: ()\n",
      "14:30:58 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.0021317022535715558, 'loss': 'mae', 'numLayers': 42, 'numNeurons': 80, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02915: ReduceLROnPlateau reducing learning rate to 0.000577487051486969.\n",
      "\n",
      "Epoch 01715: ReduceLROnPlateau reducing learning rate to 0.0018837993266060948.\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.001065851072780788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:31:32 WORKER: done with job (2, 0, 0), trying to register it.\n",
      "14:31:32 WORKER: registered result for job (2, 0, 0) with dispatcher\n",
      "14:31:32 DISPATCHER: job (2, 0, 0) finished\n",
      "14:31:32 DISPATCHER: register_result: lock acquired\n",
      "14:31:32 DISPATCHER: job (2, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:31:32 job_id: (2, 0, 0)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.0046198962892369825, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 20, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.18147856081305314, 'info': {'L1': 0.18147856081305314, 'L2': 0.00646691946080915, 'MAX': 0.13634527478052716, 'TrainTime': 2041.890625}}\n",
      "exception: None\n",
      "\n",
      "14:31:32 job_callback for (2, 0, 0) started\n",
      "14:31:32 job_callback for (2, 0, 0) got condition\n",
      "14:31:32 Only 5 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:31:32 HBMASTER: Trying to run another job!\n",
      "14:31:32 job_callback for (2, 0, 0) finished\n",
      "14:31:32 DISPATCHER: Trying to submit another job.\n",
      "14:31:32 start sampling a new configuration.\n",
      "14:31:32 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:31:32 best_vector: [0, 0, 0.903872021908098, 0.9916040842495876, 1, 0.7527794319937575, 0.5628781357903214, 0], 6.542983576591715e-31, 0.015283547456509236, -0.0023285818999776386\n",
      "14:31:32 done sampling a new configuration.\n",
      "14:31:32 HBMASTER: schedule new run for iteration 4\n",
      "14:31:32 HBMASTER: trying submitting job (4, 0, 5) to dispatcher\n",
      "14:31:32 HBMASTER: submitting job (4, 0, 5) to dispatcher\n",
      "14:31:32 DISPATCHER: trying to submit job (4, 0, 5)\n",
      "14:31:32 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:31:32 HBMASTER: job (4, 0, 5) submitted to dispatcher\n",
      "14:31:32 DISPATCHER: Trying to submit another job.\n",
      "14:31:32 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:31:32 DISPATCHER: starting job (4, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:31:32 DISPATCHER: job (4, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:31:32 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:31:32 WORKER: start processing job (4, 0, 5)\n",
      "14:31:32 WORKER: args: ()\n",
      "14:31:32 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.009916880434070918, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 61, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.000532925536390394.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:31:34 DISPATCHER: Starting worker discovery\n",
      "14:31:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:31:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.000266462768195197.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0001332313840975985.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:31:49 WORKER: done with job (4, 0, 4), trying to register it.\n",
      "14:31:49 WORKER: registered result for job (4, 0, 4) with dispatcher\n",
      "14:31:49 DISPATCHER: job (4, 0, 4) finished\n",
      "14:31:49 DISPATCHER: register_result: lock acquired\n",
      "14:31:49 DISPATCHER: job (4, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:31:49 job_id: (4, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.0021317022535715558, 'loss': 'mae', 'numLayers': 42, 'numNeurons': 80, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 7.530989379942818, 'info': {'L1': 7.530989379942818, 'L2': 12.530446594803042, 'MAX': 4.787520210635003, 'TrainTime': 141.359375}}\n",
      "exception: None\n",
      "\n",
      "14:31:49 job_callback for (4, 0, 4) started\n",
      "14:31:49 job_callback for (4, 0, 4) got condition\n",
      "14:31:49 DISPATCHER: Trying to submit another job.\n",
      "14:31:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:31:49 HBMASTER: Trying to run another job!\n",
      "14:31:49 job_callback for (4, 0, 4) finished\n",
      "14:31:49 start sampling a new configuration.\n",
      "14:31:49 best_vector: [3, 2, 0.6913713499982876, 0.2684944532070716, 0, 0.36151075837051344, 0.47862629172738297, 2], 1.88721272665869e-30, 0.005298819713718761, -0.001709618050711947\n",
      "14:31:49 done sampling a new configuration.\n",
      "14:31:49 HBMASTER: schedule new run for iteration 4\n",
      "14:31:49 HBMASTER: trying submitting job (4, 0, 6) to dispatcher\n",
      "14:31:49 HBMASTER: submitting job (4, 0, 6) to dispatcher\n",
      "14:31:49 DISPATCHER: trying to submit job (4, 0, 6)\n",
      "14:31:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:31:49 HBMASTER: job (4, 0, 6) submitted to dispatcher\n",
      "14:31:49 DISPATCHER: Trying to submit another job.\n",
      "14:31:49 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:31:49 DISPATCHER: starting job (4, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:31:49 DISPATCHER: job (4, 0, 6) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:31:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:31:49 WORKER: start processing job (4, 0, 6)\n",
      "14:31:49 WORKER: args: ()\n",
      "14:31:49 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.002758095086750009, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 53, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.004958440084010363.\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0024792200420051813.\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0012396100210025907.\n",
      "\n",
      "Epoch 01568: ReduceLROnPlateau reducing learning rate to 0.004881892818957567.\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0006198050105012953.\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.00030990250525064766.\n",
      "\n",
      "Epoch 02426: ReduceLROnPlateau reducing learning rate to 0.001915852539241314.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:32:22 WORKER: done with job (4, 0, 5), trying to register it.\n",
      "14:32:22 WORKER: registered result for job (4, 0, 5) with dispatcher\n",
      "14:32:22 DISPATCHER: job (4, 0, 5) finished\n",
      "14:32:22 DISPATCHER: register_result: lock acquired\n",
      "14:32:22 DISPATCHER: job (4, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:32:22 job_id: (4, 0, 5)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.009916880434070918, 'loss': 'mse', 'numLayers': 38, 'numNeurons': 61, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 20.589348365687748, 'info': {'L1': 20.589348365687748, 'L2': 23.936554737683483, 'MAX': 3.7486784862392515, 'TrainTime': 139.078125}}\n",
      "exception: None\n",
      "\n",
      "14:32:22 job_callback for (4, 0, 5) started\n",
      "14:32:22 DISPATCHER: Trying to submit another job.\n",
      "14:32:22 job_callback for (4, 0, 5) got condition\n",
      "14:32:22 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:32:22 HBMASTER: Trying to run another job!\n",
      "14:32:22 job_callback for (4, 0, 5) finished\n",
      "14:32:22 start sampling a new configuration.\n",
      "14:32:22 best_vector: [2, 1, 0.6392276488884947, 0.030349763079994596, 0, 0.2485149535788611, 0.9560254119622921, 3], 0.00767113910612903, 0.021396398304870962, 0.0001641347477668085\n",
      "14:32:22 done sampling a new configuration.\n",
      "14:32:22 HBMASTER: schedule new run for iteration 4\n",
      "14:32:22 HBMASTER: trying submitting job (4, 0, 7) to dispatcher\n",
      "14:32:22 HBMASTER: submitting job (4, 0, 7) to dispatcher\n",
      "14:32:22 DISPATCHER: trying to submit job (4, 0, 7)\n",
      "14:32:22 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:32:22 HBMASTER: job (4, 0, 7) submitted to dispatcher\n",
      "14:32:22 DISPATCHER: Trying to submit another job.\n",
      "14:32:22 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:32:22 DISPATCHER: starting job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:32:22 DISPATCHER: job (4, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:32:22 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:32:22 WORKER: start processing job (4, 0, 7)\n",
      "14:32:22 WORKER: args: ()\n",
      "14:32:22 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02015: ReduceLROnPlateau reducing learning rate to 0.0009418996633030474.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:32:34 DISPATCHER: Starting worker discovery\n",
      "14:32:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:32:34 DISPATCHER: Finished worker discovery\n",
      "14:32:37 WORKER: done with job (4, 0, 6), trying to register it.\n",
      "14:32:37 WORKER: registered result for job (4, 0, 6) with dispatcher\n",
      "14:32:37 DISPATCHER: job (4, 0, 6) finished\n",
      "14:32:37 DISPATCHER: register_result: lock acquired\n",
      "14:32:37 DISPATCHER: job (4, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:32:37 job_id: (4, 0, 6)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.002758095086750009, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 53, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 71.19335442332762, 'info': {'L1': 71.19335442332762, 'L2': 263.62294809655157, 'MAX': 3.9768905639648438, 'TrainTime': 130.421875}}\n",
      "exception: None\n",
      "\n",
      "14:32:37 job_callback for (4, 0, 6) started\n",
      "14:32:37 DISPATCHER: Trying to submit another job.\n",
      "14:32:37 job_callback for (4, 0, 6) got condition\n",
      "14:32:37 HBMASTER: Trying to run another job!\n",
      "14:32:37 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:32:37 job_callback for (4, 0, 6) finished\n",
      "14:32:37 start sampling a new configuration.\n",
      "14:32:37 done sampling a new configuration.\n",
      "14:32:37 HBMASTER: schedule new run for iteration 4\n",
      "14:32:37 HBMASTER: trying submitting job (4, 0, 8) to dispatcher\n",
      "14:32:37 HBMASTER: submitting job (4, 0, 8) to dispatcher\n",
      "14:32:37 DISPATCHER: trying to submit job (4, 0, 8)\n",
      "14:32:37 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:32:37 HBMASTER: job (4, 0, 8) submitted to dispatcher\n",
      "14:32:37 DISPATCHER: Trying to submit another job.\n",
      "14:32:37 DISPATCHER: starting job (4, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:32:37 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:32:37 DISPATCHER: job (4, 0, 8) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:32:37 WORKER: start processing job (4, 0, 8)\n",
      "14:32:37 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:32:37 WORKER: args: ()\n",
      "14:32:37 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.006780767493201843, 'loss': 'mae', 'numLayers': 30, 'numNeurons': 58, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.00020023132674396038.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.00010011566337198019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:32:52 WORKER: done with job (4, 0, 7), trying to register it.\n",
      "14:32:52 WORKER: registered result for job (4, 0, 7) with dispatcher\n",
      "14:32:52 DISPATCHER: job (4, 0, 7) finished\n",
      "14:32:52 DISPATCHER: register_result: lock acquired\n",
      "14:32:52 DISPATCHER: job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:32:52 job_id: (4, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 5.613319957864058, 'info': {'L1': 5.613319957864058, 'L2': 5.60849983432664, 'MAX': 2.246652605085069, 'TrainTime': 83.546875}}\n",
      "exception: None\n",
      "\n",
      "14:32:52 job_callback for (4, 0, 7) started\n",
      "14:32:52 DISPATCHER: Trying to submit another job.\n",
      "14:32:52 job_callback for (4, 0, 7) got condition\n",
      "14:32:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:32:52 HBMASTER: Trying to run another job!\n",
      "14:32:52 job_callback for (4, 0, 7) finished\n",
      "14:32:52 start sampling a new configuration.\n",
      "14:32:52 best_vector: [2, 1, 0.10721777621628525, 0.06187731715104061, 1, 0.5261695373081727, 0.8270880357272746, 0], 0.19794474379818564, 0.0019124610688798138, 0.000378561616303419\n",
      "14:32:52 done sampling a new configuration.\n",
      "14:32:52 HBMASTER: schedule new run for iteration 4\n",
      "14:32:52 HBMASTER: trying submitting job (4, 0, 9) to dispatcher\n",
      "14:32:52 HBMASTER: submitting job (4, 0, 9) to dispatcher\n",
      "14:32:52 DISPATCHER: trying to submit job (4, 0, 9)\n",
      "14:32:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:32:52 HBMASTER: job (4, 0, 9) submitted to dispatcher\n",
      "14:32:52 DISPATCHER: Trying to submit another job.\n",
      "14:32:52 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:32:52 DISPATCHER: starting job (4, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:32:52 DISPATCHER: job (4, 0, 9) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:32:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:32:52 WORKER: start processing job (4, 0, 9)\n",
      "14:32:52 WORKER: args: ()\n",
      "14:32:52 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0007125854397953021, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 85, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0033903836738318205.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0016951918369159102.\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0008475959184579551.\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.00042379795922897756.\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.00021189897961448878.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:33:13 WORKER: done with job (4, 0, 8), trying to register it.\n",
      "14:33:13 WORKER: registered result for job (4, 0, 8) with dispatcher\n",
      "14:33:13 DISPATCHER: job (4, 0, 8) finished\n",
      "14:33:13 DISPATCHER: register_result: lock acquired\n",
      "14:33:13 DISPATCHER: job (4, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:33:13 job_id: (4, 0, 8)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.006780767493201843, 'loss': 'mae', 'numLayers': 30, 'numNeurons': 58, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.042192354432826, 'info': {'L1': 8.042192354432826, 'L2': 13.61044069720383, 'MAX': 4.958336155306634, 'TrainTime': 87.140625}}\n",
      "exception: None\n",
      "\n",
      "14:33:13 job_callback for (4, 0, 8) started\n",
      "14:33:13 DISPATCHER: Trying to submit another job.\n",
      "14:33:13 job_callback for (4, 0, 8) got condition\n",
      "14:33:13 HBMASTER: Trying to run another job!\n",
      "14:33:13 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:33:13 job_callback for (4, 0, 8) finished\n",
      "14:33:13 start sampling a new configuration.\n",
      "14:33:13 done sampling a new configuration.\n",
      "14:33:13 HBMASTER: schedule new run for iteration 4\n",
      "14:33:13 HBMASTER: trying submitting job (4, 0, 10) to dispatcher\n",
      "14:33:13 HBMASTER: submitting job (4, 0, 10) to dispatcher\n",
      "14:33:13 DISPATCHER: trying to submit job (4, 0, 10)\n",
      "14:33:13 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:33:13 HBMASTER: job (4, 0, 10) submitted to dispatcher\n",
      "14:33:13 DISPATCHER: Trying to submit another job.\n",
      "14:33:13 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:33:13 DISPATCHER: starting job (4, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:33:13 DISPATCHER: job (4, 0, 10) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:33:13 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:33:13 WORKER: start processing job (4, 0, 10)\n",
      "14:33:13 WORKER: args: ()\n",
      "14:33:13 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.008968027577715127, 'loss': 'mae', 'numLayers': 4, 'numNeurons': 68, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00448401365429163.\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.002242006827145815.\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.0003562927304301411.\n",
      "\n",
      "Epoch 02315: ReduceLROnPlateau reducing learning rate to 0.0004709498316515237.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:33:31 WORKER: done with job (4, 0, 10), trying to register it.\n",
      "14:33:31 WORKER: registered result for job (4, 0, 10) with dispatcher\n",
      "14:33:31 DISPATCHER: job (4, 0, 10) finished\n",
      "14:33:31 DISPATCHER: register_result: lock acquired\n",
      "14:33:31 DISPATCHER: job (4, 0, 10) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:33:31 job_id: (4, 0, 10)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.008968027577715127, 'loss': 'mae', 'numLayers': 4, 'numNeurons': 68, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 34.38619840216599, 'info': {'L1': 34.38619840216599, 'L2': 72.2066535197144, 'MAX': 3.1240967292299047, 'TrainTime': 47.171875}}\n",
      "exception: None\n",
      "\n",
      "14:33:31 job_callback for (4, 0, 10) started\n",
      "14:33:31 DISPATCHER: Trying to submit another job.\n",
      "14:33:31 job_callback for (4, 0, 10) got condition\n",
      "14:33:31 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:33:31 HBMASTER: Trying to run another job!\n",
      "14:33:31 job_callback for (4, 0, 10) finished\n",
      "14:33:31 start sampling a new configuration.\n",
      "14:33:31 done sampling a new configuration.\n",
      "14:33:31 HBMASTER: schedule new run for iteration 4\n",
      "14:33:31 HBMASTER: trying submitting job (4, 0, 11) to dispatcher\n",
      "14:33:31 HBMASTER: submitting job (4, 0, 11) to dispatcher\n",
      "14:33:31 DISPATCHER: trying to submit job (4, 0, 11)\n",
      "14:33:31 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:33:31 HBMASTER: job (4, 0, 11) submitted to dispatcher\n",
      "14:33:31 DISPATCHER: Trying to submit another job.\n",
      "14:33:31 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:33:31 DISPATCHER: starting job (4, 0, 11) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:33:31 DISPATCHER: job (4, 0, 11) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:33:31 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:33:31 WORKER: start processing job (4, 0, 11)\n",
      "14:33:31 WORKER: args: ()\n",
      "14:33:31 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0014801163340335398, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.00017814636521507055.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:33:33 WORKER: done with job (4, 0, 9), trying to register it.\n",
      "14:33:33 WORKER: registered result for job (4, 0, 9) with dispatcher\n",
      "14:33:33 DISPATCHER: job (4, 0, 9) finished\n",
      "14:33:33 DISPATCHER: register_result: lock acquired\n",
      "14:33:33 DISPATCHER: job (4, 0, 9) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:33:33 job_id: (4, 0, 9)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0007125854397953021, 'loss': 'mse', 'numLayers': 27, 'numNeurons': 85, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 16.82146763333514, 'info': {'L1': 16.82146763333514, 'L2': 17.582627475300143, 'MAX': 1.978388098245711, 'TrainTime': 94.640625}}\n",
      "exception: None\n",
      "\n",
      "14:33:33 job_callback for (4, 0, 9) started\n",
      "14:33:33 DISPATCHER: Trying to submit another job.\n",
      "14:33:33 job_callback for (4, 0, 9) got condition\n",
      "14:33:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:33:33 HBMASTER: Trying to run another job!\n",
      "14:33:33 job_callback for (4, 0, 9) finished\n",
      "14:33:33 start sampling a new configuration.\n",
      "14:33:33 best_vector: [1, 3, 0.260791632313902, 0.45746599244642283, 1, 0.43557589218195225, 0.5241967631738726, 2], 8.012969542202635e-31, 0.012479767890458201, -0.001441350786562214\n",
      "14:33:33 done sampling a new configuration.\n",
      "14:33:33 HBMASTER: schedule new run for iteration 4\n",
      "14:33:33 HBMASTER: trying submitting job (4, 0, 12) to dispatcher\n",
      "14:33:33 HBMASTER: submitting job (4, 0, 12) to dispatcher\n",
      "14:33:33 DISPATCHER: trying to submit job (4, 0, 12)\n",
      "14:33:33 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:33:33 HBMASTER: job (4, 0, 12) submitted to dispatcher\n",
      "14:33:33 DISPATCHER: Trying to submit another job.\n",
      "14:33:33 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:33:33 DISPATCHER: starting job (4, 0, 12) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:33:33 DISPATCHER: job (4, 0, 12) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:33:33 WORKER: start processing job (4, 0, 12)\n",
      "14:33:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:33:33 WORKER: args: ()\n",
      "14:33:33 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.004628913325219587, 'loss': 'mse', 'numLayers': 23, 'numNeurons': 57, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "14:33:34 DISPATCHER: Starting worker discovery\n",
      "14:33:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:33:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02886: ReduceLROnPlateau reducing learning rate to 0.000957926269620657.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:02 WORKER: done with job (4, 0, 12), trying to register it.\n",
      "14:34:02 WORKER: registered result for job (4, 0, 12) with dispatcher\n",
      "14:34:02 DISPATCHER: job (4, 0, 12) finished\n",
      "14:34:02 DISPATCHER: register_result: lock acquired\n",
      "14:34:02 DISPATCHER: job (4, 0, 12) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:34:02 job_id: (4, 0, 12)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 7, 'initial_lr': 0.004628913325219587, 'loss': 'mse', 'numLayers': 23, 'numNeurons': 57, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 23.56701511087754, 'info': {'L1': 23.56701511087754, 'L2': 29.90444542887582, 'MAX': 1.8659309553020567, 'TrainTime': 73.5}}\n",
      "exception: None\n",
      "\n",
      "14:34:02 job_callback for (4, 0, 12) started\n",
      "14:34:02 DISPATCHER: Trying to submit another job.\n",
      "14:34:02 job_callback for (4, 0, 12) got condition\n",
      "14:34:02 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:34:02 HBMASTER: Trying to run another job!\n",
      "14:34:02 job_callback for (4, 0, 12) finished\n",
      "14:34:02 start sampling a new configuration.\n",
      "14:34:02 done sampling a new configuration.\n",
      "14:34:02 HBMASTER: schedule new run for iteration 4\n",
      "14:34:02 HBMASTER: trying submitting job (4, 0, 13) to dispatcher\n",
      "14:34:02 HBMASTER: submitting job (4, 0, 13) to dispatcher\n",
      "14:34:02 DISPATCHER: trying to submit job (4, 0, 13)\n",
      "14:34:02 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:34:02 HBMASTER: job (4, 0, 13) submitted to dispatcher\n",
      "14:34:02 DISPATCHER: Trying to submit another job.\n",
      "14:34:02 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:34:02 DISPATCHER: starting job (4, 0, 13) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:34:02 DISPATCHER: job (4, 0, 13) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:34:02 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:34:02 WORKER: start processing job (4, 0, 13)\n",
      "14:34:02 WORKER: args: ()\n",
      "14:34:02 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 9, 'initial_lr': 0.0031608799034339367, 'loss': 'mse', 'numLayers': 48, 'numNeurons': 55, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:08 WORKER: done with job (2, 0, 1), trying to register it.\n",
      "14:34:08 WORKER: registered result for job (2, 0, 1) with dispatcher\n",
      "14:34:08 DISPATCHER: job (2, 0, 1) finished\n",
      "14:34:08 DISPATCHER: register_result: lock acquired\n",
      "14:34:08 DISPATCHER: job (2, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:34:08 job_id: (2, 0, 1)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 15000, 'denspt': 9, 'initial_lr': 0.007663410368848484, 'loss': 'mae', 'numLayers': 18, 'numNeurons': 50, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 8.176061991898047, 'info': {'L1': 8.176061991898047, 'L2': 12.637860203324827, 'MAX': 4.117814816349198, 'TrainTime': 1820.25}}\n",
      "exception: None\n",
      "\n",
      "14:34:08 job_callback for (2, 0, 1) started\n",
      "14:34:08 job_callback for (2, 0, 1) got condition\n",
      "14:34:08 Only 6 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:34:08 DISPATCHER: Trying to submit another job.\n",
      "14:34:08 HBMASTER: Trying to run another job!\n",
      "14:34:08 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:34:08 job_callback for (2, 0, 1) finished\n",
      "14:34:08 start sampling a new configuration.\n",
      "14:34:08 best_vector: [0, 2, 0.6491596226791049, 0.8289115788655638, 0, 0.6646156314385008, 0.9931265792827817, 1], 2.916649913252652e-30, 0.0034285911224936788, -0.00010345225194783473\n",
      "14:34:08 done sampling a new configuration.\n",
      "14:34:09 HBMASTER: schedule new run for iteration 4\n",
      "14:34:09 HBMASTER: trying submitting job (4, 0, 14) to dispatcher\n",
      "14:34:09 HBMASTER: submitting job (4, 0, 14) to dispatcher\n",
      "14:34:09 DISPATCHER: trying to submit job (4, 0, 14)\n",
      "14:34:09 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:34:09 HBMASTER: job (4, 0, 14) submitted to dispatcher\n",
      "14:34:09 DISPATCHER: Trying to submit another job.\n",
      "14:34:09 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:34:09 DISPATCHER: starting job (4, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:34:09 DISPATCHER: job (4, 0, 14) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:34:09 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:34:09 WORKER: start processing job (4, 0, 14)\n",
      "14:34:09 WORKER: args: ()\n",
      "14:34:09 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.008306224630769082, 'loss': 'mae', 'numLayers': 34, 'numNeurons': 100, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02615: ReduceLROnPlateau reducing learning rate to 0.00023547491582576185.\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:34:34 DISPATCHER: Starting worker discovery\n",
      "14:34:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:34:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02520: ReduceLROnPlateau reducing learning rate to 0.0024409464094787836.\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0015804399736225605.\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.004153112415224314.\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0007902199868112803.\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.002076556207612157.\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0003951099934056401.\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.00019755499670282006.\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0010382781038060784.\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 9.877749835141003e-05.\n",
      "\n",
      "Epoch 02915: ReduceLROnPlateau reducing learning rate to 0.00011773745791288093.\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0005191390519030392.\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 4.9388749175705016e-05.\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0002595695259515196.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:12 WORKER: done with job (4, 0, 13), trying to register it.\n",
      "14:35:12 WORKER: registered result for job (4, 0, 13) with dispatcher\n",
      "14:35:12 DISPATCHER: job (4, 0, 13) finished\n",
      "14:35:12 DISPATCHER: register_result: lock acquired\n",
      "14:35:12 DISPATCHER: job (4, 0, 13) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:35:12 job_id: (4, 0, 13)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 9, 'initial_lr': 0.0031608799034339367, 'loss': 'mse', 'numLayers': 48, 'numNeurons': 55, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 20.582144823694065, 'info': {'L1': 20.582144823694065, 'L2': 23.945868497401147, 'MAX': 3.7488775657528013, 'TrainTime': 183.546875}}\n",
      "exception: None\n",
      "\n",
      "14:35:12 job_callback for (4, 0, 13) started\n",
      "14:35:12 DISPATCHER: Trying to submit another job.\n",
      "14:35:12 job_callback for (4, 0, 13) got condition\n",
      "14:35:12 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:35:12 HBMASTER: Trying to run another job!\n",
      "14:35:12 job_callback for (4, 0, 13) finished\n",
      "14:35:12 start sampling a new configuration.\n",
      "14:35:12 done sampling a new configuration.\n",
      "14:35:12 HBMASTER: schedule new run for iteration 4\n",
      "14:35:12 HBMASTER: trying submitting job (4, 0, 15) to dispatcher\n",
      "14:35:12 HBMASTER: submitting job (4, 0, 15) to dispatcher\n",
      "14:35:12 DISPATCHER: trying to submit job (4, 0, 15)\n",
      "14:35:12 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:35:12 HBMASTER: job (4, 0, 15) submitted to dispatcher\n",
      "14:35:12 DISPATCHER: Trying to submit another job.\n",
      "14:35:12 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:35:12 DISPATCHER: starting job (4, 0, 15) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:35:12 DISPATCHER: job (4, 0, 15) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:35:12 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:35:12 WORKER: start processing job (4, 0, 15)\n",
      "14:35:12 WORKER: args: ()\n",
      "14:35:12 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.005890432995680939, 'loss': 'mae', 'numLayers': 29, 'numNeurons': 50, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "14:35:13 WORKER: done with job (4, 0, 14), trying to register it.\n",
      "14:35:13 WORKER: registered result for job (4, 0, 14) with dispatcher\n",
      "14:35:13 DISPATCHER: job (4, 0, 14) finished\n",
      "14:35:13 DISPATCHER: register_result: lock acquired\n",
      "14:35:13 DISPATCHER: job (4, 0, 14) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:35:13 job_id: (4, 0, 14)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.008306224630769082, 'loss': 'mae', 'numLayers': 34, 'numNeurons': 100, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.707777259724562, 'info': {'L1': 8.707777259724562, 'L2': 9.953858597053852, 'MAX': 4.417614261996087, 'TrainTime': 161.703125}}\n",
      "exception: None\n",
      "\n",
      "14:35:13 job_callback for (4, 0, 14) started\n",
      "14:35:13 DISPATCHER: Trying to submit another job.\n",
      "14:35:13 job_callback for (4, 0, 14) got condition\n",
      "14:35:13 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:35:13 HBMASTER: Trying to run another job!\n",
      "14:35:13 job_callback for (4, 0, 14) finished\n",
      "14:35:13 start sampling a new configuration.\n",
      "14:35:13 done sampling a new configuration.\n",
      "14:35:13 HBMASTER: schedule new run for iteration 4\n",
      "14:35:13 HBMASTER: trying submitting job (4, 0, 16) to dispatcher\n",
      "14:35:13 HBMASTER: submitting job (4, 0, 16) to dispatcher\n",
      "14:35:13 DISPATCHER: trying to submit job (4, 0, 16)\n",
      "14:35:13 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:35:13 HBMASTER: job (4, 0, 16) submitted to dispatcher\n",
      "14:35:13 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:35:13 DISPATCHER: Trying to submit another job.\n",
      "14:35:13 DISPATCHER: starting job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:35:13 DISPATCHER: job (4, 0, 16) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:35:13 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:35:13 WORKER: start processing job (4, 0, 16)\n",
      "14:35:13 WORKER: args: ()\n",
      "14:35:13 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:19 WORKER: done with job (3, 0, 2), trying to register it.\n",
      "14:35:19 WORKER: registered result for job (3, 0, 2) with dispatcher\n",
      "14:35:19 DISPATCHER: job (3, 0, 2) finished\n",
      "14:35:19 DISPATCHER: register_result: lock acquired\n",
      "14:35:19 DISPATCHER: job (3, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:35:19 job_id: (3, 0, 2)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.007535197506937765, 'loss': 'mse', 'numLayers': 25, 'numNeurons': 27, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 5.45591724157116, 'info': {'L1': 5.45591724157116, 'L2': 2.54944986195527, 'MAX': 1.2721320794933408, 'TrainTime': 1701.34375}}\n",
      "exception: None\n",
      "\n",
      "14:35:19 job_callback for (3, 0, 2) started\n",
      "14:35:19 job_callback for (3, 0, 2) got condition\n",
      "14:35:19 DISPATCHER: Trying to submit another job.\n",
      "14:35:19 Only 7 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:35:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:35:19 HBMASTER: Trying to run another job!\n",
      "14:35:19 job_callback for (3, 0, 2) finished\n",
      "14:35:19 start sampling a new configuration.\n",
      "14:35:19 best_vector: [2, 0, 0.6952437202478977, 0.5399852089984739, 0, 0.23209299681461004, 0.6135054839348788, 0], 3.7460407356479333e-31, 0.026694851192722954, -0.0037056921971941542\n",
      "14:35:19 done sampling a new configuration.\n",
      "14:35:20 HBMASTER: schedule new run for iteration 4\n",
      "14:35:20 HBMASTER: trying submitting job (4, 0, 17) to dispatcher\n",
      "14:35:20 HBMASTER: submitting job (4, 0, 17) to dispatcher\n",
      "14:35:20 DISPATCHER: trying to submit job (4, 0, 17)\n",
      "14:35:20 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:35:20 HBMASTER: job (4, 0, 17) submitted to dispatcher\n",
      "14:35:20 DISPATCHER: Trying to submit another job.\n",
      "14:35:20 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:35:20 DISPATCHER: starting job (4, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:35:20 DISPATCHER: job (4, 0, 17) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:35:20 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:35:20 WORKER: start processing job (4, 0, 17)\n",
      "14:35:20 WORKER: args: ()\n",
      "14:35:20 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0054458535690848926, 'loss': 'mae', 'numLayers': 13, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:34 DISPATCHER: Starting worker discovery\n",
      "14:35:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:35:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0027229266706854105.\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0013614633353427052.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0018838440300896764.\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0009419220150448382.\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0004709610075224191.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:48 WORKER: done with job (4, 0, 17), trying to register it.\n",
      "14:35:48 WORKER: registered result for job (4, 0, 17) with dispatcher\n",
      "14:35:48 DISPATCHER: job (4, 0, 17) finished\n",
      "14:35:48 DISPATCHER: register_result: lock acquired\n",
      "14:35:48 DISPATCHER: job (4, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:35:48 job_id: (4, 0, 17)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0054458535690848926, 'loss': 'mae', 'numLayers': 13, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 6.408756044046521, 'info': {'L1': 6.408756044046521, 'L2': 7.312686655074948, 'MAX': 2.850381500330209, 'TrainTime': 79.453125}}\n",
      "exception: None\n",
      "\n",
      "14:35:48 job_callback for (4, 0, 17) started\n",
      "14:35:48 DISPATCHER: Trying to submit another job.\n",
      "14:35:48 job_callback for (4, 0, 17) got condition\n",
      "14:35:48 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:35:48 HBMASTER: Trying to run another job!\n",
      "14:35:48 job_callback for (4, 0, 17) finished\n",
      "14:35:48 start sampling a new configuration.\n",
      "14:35:49 best_vector: [4, 0, 0.7795892265433202, 0.06139183913934622, 0, 0.3062055792736821, 0.7171736925325765, 3], 0.015605464493462043, 0.03015895393230442, 0.0004706444847505341\n",
      "14:35:49 done sampling a new configuration.\n",
      "14:35:49 HBMASTER: schedule new run for iteration 4\n",
      "14:35:49 HBMASTER: trying submitting job (4, 0, 18) to dispatcher\n",
      "14:35:49 HBMASTER: submitting job (4, 0, 18) to dispatcher\n",
      "14:35:49 DISPATCHER: trying to submit job (4, 0, 18)\n",
      "14:35:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:35:49 HBMASTER: job (4, 0, 18) submitted to dispatcher\n",
      "14:35:49 DISPATCHER: Trying to submit another job.\n",
      "14:35:49 DISPATCHER: starting job (4, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:35:49 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:35:49 DISPATCHER: job (4, 0, 18) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:35:49 WORKER: start processing job (4, 0, 18)\n",
      "14:35:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:35:49 WORKER: args: ()\n",
      "14:35:49 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.0007077792074795277, 'loss': 'mae', 'numLayers': 17, 'numNeurons': 75, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00023548050376120955.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:35:59 WORKER: done with job (4, 0, 16), trying to register it.\n",
      "14:35:59 WORKER: registered result for job (4, 0, 16) with dispatcher\n",
      "14:35:59 DISPATCHER: job (4, 0, 16) finished\n",
      "14:35:59 DISPATCHER: register_result: lock acquired\n",
      "14:35:59 DISPATCHER: job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:35:59 job_id: (4, 0, 16)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 6.288620970912899, 'info': {'L1': 6.288620970912899, 'L2': 6.6646968795303865, 'MAX': 2.3681519523464214, 'TrainTime': 129.234375}}\n",
      "exception: None\n",
      "\n",
      "14:35:59 job_callback for (4, 0, 16) started\n",
      "14:35:59 DISPATCHER: Trying to submit another job.\n",
      "14:35:59 job_callback for (4, 0, 16) got condition\n",
      "14:35:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:35:59 HBMASTER: Trying to run another job!\n",
      "14:35:59 job_callback for (4, 0, 16) finished\n",
      "14:35:59 start sampling a new configuration.\n",
      "14:35:59 best_vector: [5, 3, 0.07290781217045253, 0.5647763616046964, 0, 0.9879336326953967, 0.8782542727717741, 4], 3.945220158848412e-28, 2.5347127910141636e-05, -3.2190614190566036e-05\n",
      "14:35:59 done sampling a new configuration.\n",
      "14:35:59 HBMASTER: schedule new run for iteration 4\n",
      "14:35:59 HBMASTER: trying submitting job (4, 0, 19) to dispatcher\n",
      "14:35:59 HBMASTER: submitting job (4, 0, 19) to dispatcher\n",
      "14:35:59 DISPATCHER: trying to submit job (4, 0, 19)\n",
      "14:35:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:35:59 HBMASTER: job (4, 0, 19) submitted to dispatcher\n",
      "14:35:59 DISPATCHER: Trying to submit another job.\n",
      "14:35:59 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:35:59 DISPATCHER: starting job (4, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:35:59 DISPATCHER: job (4, 0, 19) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:35:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:35:59 WORKER: start processing job (4, 0, 19)\n",
      "14:35:59 WORKER: args: ()\n",
      "14:35:59 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.005691285979886495, 'loss': 'mae', 'numLayers': 50, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00035388959804549813.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:05 WORKER: done with job (4, 0, 15), trying to register it.\n",
      "14:36:05 WORKER: registered result for job (4, 0, 15) with dispatcher\n",
      "14:36:05 DISPATCHER: job (4, 0, 15) finished\n",
      "14:36:05 DISPATCHER: register_result: lock acquired\n",
      "14:36:05 DISPATCHER: job (4, 0, 15) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:36:05 job_id: (4, 0, 15)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.005890432995680939, 'loss': 'mae', 'numLayers': 29, 'numNeurons': 50, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 79.74354189408616, 'info': {'L1': 79.74354189408616, 'L2': 328.386572440234, 'MAX': 4.40616899728775, 'TrainTime': 127.28125}}\n",
      "exception: None\n",
      "\n",
      "14:36:05 job_callback for (4, 0, 15) started\n",
      "14:36:05 DISPATCHER: Trying to submit another job.\n",
      "14:36:05 job_callback for (4, 0, 15) got condition\n",
      "14:36:05 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:36:05 HBMASTER: Trying to run another job!\n",
      "14:36:05 job_callback for (4, 0, 15) finished\n",
      "14:36:05 start sampling a new configuration.\n",
      "14:36:05 best_vector: [2, 2, 0.19480512023858546, 0.48103377148543813, 1, 0.22483169342242915, 0.11931305065689923, 2], 0.018647827487672654, 0.03198646865800951, 0.0005964781494744096\n",
      "14:36:05 done sampling a new configuration.\n",
      "14:36:05 HBMASTER: schedule new run for iteration 4\n",
      "14:36:05 HBMASTER: trying submitting job (4, 0, 20) to dispatcher\n",
      "14:36:05 HBMASTER: submitting job (4, 0, 20) to dispatcher\n",
      "14:36:05 DISPATCHER: trying to submit job (4, 0, 20)\n",
      "14:36:05 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:36:05 HBMASTER: job (4, 0, 20) submitted to dispatcher\n",
      "14:36:05 DISPATCHER: Trying to submit another job.\n",
      "14:36:05 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:36:05 DISPATCHER: starting job (4, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:36:05 DISPATCHER: job (4, 0, 20) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:36:05 WORKER: start processing job (4, 0, 20)\n",
      "14:36:05 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:36:05 WORKER: args: ()\n",
      "14:36:05 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004862234337705838, 'loss': 'mse', 'numLayers': 13, 'numNeurons': 20, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00017694479902274907.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:07 WORKER: done with job (3, 0, 3), trying to register it.\n",
      "14:36:07 WORKER: registered result for job (3, 0, 3) with dispatcher\n",
      "14:36:07 DISPATCHER: job (3, 0, 3) finished\n",
      "14:36:07 DISPATCHER: register_result: lock acquired\n",
      "14:36:07 DISPATCHER: job (3, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:36:07 job_id: (3, 0, 3)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009763785355770344, 'loss': 'mae', 'numLayers': 27, 'numNeurons': 64, 'optimizer': 'SGD'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 7.611114291696759, 'info': {'L1': 7.611114291696759, 'L2': 12.031622259210854, 'MAX': 4.078606881015946, 'TrainTime': 1704.609375}}\n",
      "exception: None\n",
      "\n",
      "14:36:07 job_callback for (3, 0, 3) started\n",
      "14:36:07 DISPATCHER: Trying to submit another job.\n",
      "14:36:07 job_callback for (3, 0, 3) got condition\n",
      "14:36:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:36:07 Only 8 run(s) for budget 3000.000000 available, need more than 10 -> can't build model!\n",
      "14:36:07 HBMASTER: Trying to run another job!\n",
      "14:36:07 job_callback for (3, 0, 3) finished\n",
      "14:36:07 start sampling a new configuration.\n",
      "14:36:07 best_vector: [0, 2, 0.7541937666574969, 0.9449893853947968, 1, 0.7805575190012701, 0.24155379887015033, 0], 1.0368168241271594e-30, 0.00964490522076401, -0.0023342296282431976\n",
      "14:36:07 done sampling a new configuration.\n",
      "14:36:07 HBMASTER: schedule new run for iteration 4\n",
      "14:36:07 HBMASTER: trying submitting job (4, 0, 21) to dispatcher\n",
      "14:36:07 HBMASTER: submitting job (4, 0, 21) to dispatcher\n",
      "14:36:07 DISPATCHER: trying to submit job (4, 0, 21)\n",
      "14:36:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:36:07 HBMASTER: job (4, 0, 21) submitted to dispatcher\n",
      "14:36:07 DISPATCHER: Trying to submit another job.\n",
      "14:36:07 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:36:07 DISPATCHER: starting job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:36:07 DISPATCHER: job (4, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:36:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:36:08 WORKER: start processing job (4, 0, 21)\n",
      "14:36:08 WORKER: args: ()\n",
      "14:36:08 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:15 WORKER: done with job (4, 0, 18), trying to register it.\n",
      "14:36:15 WORKER: registered result for job (4, 0, 18) with dispatcher\n",
      "14:36:15 DISPATCHER: job (4, 0, 18) finished\n",
      "14:36:15 DISPATCHER: register_result: lock acquired\n",
      "14:36:15 DISPATCHER: job (4, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:36:15 job_id: (4, 0, 18)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.0007077792074795277, 'loss': 'mae', 'numLayers': 17, 'numNeurons': 75, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.133192198699655, 'info': {'L1': 8.133192198699655, 'L2': 11.088659263466853, 'MAX': 3.4889518090155116, 'TrainTime': 74.5625}}\n",
      "exception: None\n",
      "\n",
      "14:36:15 job_callback for (4, 0, 18) started\n",
      "14:36:15 DISPATCHER: Trying to submit another job.\n",
      "14:36:15 job_callback for (4, 0, 18) got condition\n",
      "14:36:15 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:36:15 HBMASTER: Trying to run another job!\n",
      "14:36:15 job_callback for (4, 0, 18) finished\n",
      "14:36:15 start sampling a new configuration.\n",
      "14:36:15 best_vector: [4, 0, 0.3714047821704709, 0.30801475438993275, 0, 0.8807294100537061, 0.08343999154689052, 3], 2.6484611183346084e-30, 0.0037757775376699313, -1.896136210108799e-05\n",
      "14:36:15 done sampling a new configuration.\n",
      "14:36:15 HBMASTER: schedule new run for iteration 4\n",
      "14:36:15 HBMASTER: trying submitting job (4, 0, 22) to dispatcher\n",
      "14:36:15 HBMASTER: submitting job (4, 0, 22) to dispatcher\n",
      "14:36:15 DISPATCHER: trying to submit job (4, 0, 22)\n",
      "14:36:15 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:36:15 HBMASTER: job (4, 0, 22) submitted to dispatcher\n",
      "14:36:15 DISPATCHER: Trying to submit another job.\n",
      "14:36:15 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:36:15 DISPATCHER: starting job (4, 0, 22) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:36:15 DISPATCHER: job (4, 0, 22) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:36:15 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:36:15 WORKER: start processing job (4, 0, 22)\n",
      "14:36:15 WORKER: args: ()\n",
      "14:36:15 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.0031493460684603345, 'loss': 'mae', 'numLayers': 45, 'numNeurons': 17, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:18 WORKER: done with job (4, 0, 20), trying to register it.\n",
      "14:36:18 WORKER: registered result for job (4, 0, 20) with dispatcher\n",
      "14:36:18 DISPATCHER: job (4, 0, 20) finished\n",
      "14:36:18 DISPATCHER: register_result: lock acquired\n",
      "14:36:18 DISPATCHER: job (4, 0, 20) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:36:18 job_id: (4, 0, 20)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.004862234337705838, 'loss': 'mse', 'numLayers': 13, 'numNeurons': 20, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': nan, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 22.75}}\n",
      "exception: None\n",
      "\n",
      "14:36:18 job_callback for (4, 0, 20) started\n",
      "14:36:18 DISPATCHER: Trying to submit another job.\n",
      "14:36:18 job_callback for (4, 0, 20) got condition\n",
      "14:36:18 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:36:18 HBMASTER: Trying to run another job!\n",
      "14:36:18 job_callback for (4, 0, 20) finished\n",
      "14:36:18 start sampling a new configuration.\n",
      "14:36:18 best_vector: [2, 0, 0.9495600688151322, 0.13480882416860268, 1, 0.6934178241768072, 0.2336446094921776, 1], 4.0780779270693405e-30, 0.002452135584173688, -7.662056810869196e-05\n",
      "14:36:18 done sampling a new configuration.\n",
      "14:36:18 HBMASTER: schedule new run for iteration 4\n",
      "14:36:18 HBMASTER: trying submitting job (4, 0, 23) to dispatcher\n",
      "14:36:18 HBMASTER: submitting job (4, 0, 23) to dispatcher\n",
      "14:36:18 DISPATCHER: trying to submit job (4, 0, 23)\n",
      "14:36:18 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:36:18 HBMASTER: job (4, 0, 23) submitted to dispatcher\n",
      "14:36:18 DISPATCHER: Trying to submit another job.\n",
      "14:36:18 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:36:18 DISPATCHER: starting job (4, 0, 23) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:36:18 DISPATCHER: job (4, 0, 23) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:36:18 WORKER: start processing job (4, 0, 23)\n",
      "14:36:18 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:36:18 WORKER: args: ()\n",
      "14:36:18 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.0014346073592691668, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 31, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:36:34 DISPATCHER: Starting worker discovery\n",
      "14:36:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:36:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.004727697465568781.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000717303657438606.\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0023638487327843904.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:10 WORKER: done with job (4, 0, 11), trying to register it.\n",
      "14:37:10 WORKER: registered result for job (4, 0, 11) with dispatcher\n",
      "14:37:10 DISPATCHER: job (4, 0, 11) finished\n",
      "14:37:10 DISPATCHER: register_result: lock acquired\n",
      "14:37:10 DISPATCHER: job (4, 0, 11) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:37:10 job_id: (4, 0, 11)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 5, 'initial_lr': 0.0014801163340335398, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 90.05113121682197, 'info': {'L1': 90.05113121682197, 'L2': 416.0396858305989, 'MAX': 4.922462858259678, 'TrainTime': 551.796875}}\n",
      "exception: None\n",
      "\n",
      "14:37:10 job_callback for (4, 0, 11) started\n",
      "14:37:10 DISPATCHER: Trying to submit another job.\n",
      "14:37:10 job_callback for (4, 0, 11) got condition\n",
      "14:37:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:10 HBMASTER: Trying to run another job!\n",
      "14:37:10 job_callback for (4, 0, 11) finished\n",
      "14:37:10 start sampling a new configuration.\n",
      "14:37:10 best_vector: [2, 0, 0.8173205695934259, 0.9771905277242052, 1, 0.5326234070931349, 0.380066968301823, 3], 1.7524782686596635e-31, 0.057062048521995284, -0.0003226869149734269\n",
      "14:37:10 done sampling a new configuration.\n",
      "14:37:10 HBMASTER: schedule new run for iteration 4\n",
      "14:37:10 HBMASTER: trying submitting job (4, 0, 24) to dispatcher\n",
      "14:37:10 HBMASTER: submitting job (4, 0, 24) to dispatcher\n",
      "14:37:10 DISPATCHER: trying to submit job (4, 0, 24)\n",
      "14:37:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:10 HBMASTER: job (4, 0, 24) submitted to dispatcher\n",
      "14:37:10 DISPATCHER: Trying to submit another job.\n",
      "14:37:10 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:10 DISPATCHER: starting job (4, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:37:10 DISPATCHER: job (4, 0, 24) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:37:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:10 WORKER: start processing job (4, 0, 24)\n",
      "14:37:10 WORKER: args: ()\n",
      "14:37:10 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009774186224469633, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 44, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:16 WORKER: done with job (4, 0, 21), trying to register it.\n",
      "14:37:16 WORKER: registered result for job (4, 0, 21) with dispatcher\n",
      "14:37:16 DISPATCHER: job (4, 0, 21) finished\n",
      "14:37:16 DISPATCHER: register_result: lock acquired\n",
      "14:37:16 DISPATCHER: job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:37:16 job_id: (4, 0, 21)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 2.8063947700648817, 'info': {'L1': 2.8063947700648817, 'L2': 1.1352481167875696, 'MAX': 1.3318489002101987, 'TrainTime': 167.078125}}\n",
      "exception: None\n",
      "\n",
      "14:37:16 job_callback for (4, 0, 21) started\n",
      "14:37:16 DISPATCHER: Trying to submit another job.\n",
      "14:37:16 job_callback for (4, 0, 21) got condition\n",
      "14:37:16 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:16 HBMASTER: Trying to run another job!\n",
      "14:37:16 job_callback for (4, 0, 21) finished\n",
      "14:37:16 start sampling a new configuration.\n",
      "14:37:16 best_vector: [2, 2, 0.14799252526513318, 0.33714331665833813, 1, 0.16932952189670988, 0.9578188523556568, 4], 4.253616582318702e-31, 0.023509406187590304, -0.0025158098505832435\n",
      "14:37:16 done sampling a new configuration.\n",
      "14:37:16 HBMASTER: schedule new run for iteration 4\n",
      "14:37:16 HBMASTER: trying submitting job (4, 0, 25) to dispatcher\n",
      "14:37:16 HBMASTER: submitting job (4, 0, 25) to dispatcher\n",
      "14:37:16 DISPATCHER: trying to submit job (4, 0, 25)\n",
      "14:37:16 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:16 HBMASTER: job (4, 0, 25) submitted to dispatcher\n",
      "14:37:16 DISPATCHER: Trying to submit another job.\n",
      "14:37:16 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:16 DISPATCHER: starting job (4, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:37:16 DISPATCHER: job (4, 0, 25) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:37:16 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:16 WORKER: start processing job (4, 0, 25)\n",
      "14:37:16 WORKER: args: ()\n",
      "14:37:16 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0034377188349175476, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.000358651828719303.\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0001793259143596515.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 8.966295717982575e-05.\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0015746729914098978.\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0007873364957049489.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:25 WORKER: done with job (4, 0, 23), trying to register it.\n",
      "14:37:25 WORKER: registered result for job (4, 0, 23) with dispatcher\n",
      "14:37:25 DISPATCHER: job (4, 0, 23) finished\n",
      "14:37:25 DISPATCHER: register_result: lock acquired\n",
      "14:37:25 DISPATCHER: job (4, 0, 23) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:37:25 job_id: (4, 0, 23)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 12, 'initial_lr': 0.0014346073592691668, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 31, 'optimizer': 'RMSprop'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 107.13564066349124, 'info': {'L1': 107.13564066349124, 'L2': 585.8773836445664, 'MAX': 6.591314792633057, 'TrainTime': 160.8125}}\n",
      "exception: None\n",
      "\n",
      "14:37:25 job_callback for (4, 0, 23) started\n",
      "14:37:25 DISPATCHER: Trying to submit another job.\n",
      "14:37:25 job_callback for (4, 0, 23) got condition\n",
      "14:37:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:25 HBMASTER: Trying to run another job!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.00039366824785247445.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:25 job_callback for (4, 0, 23) finished\n",
      "14:37:25 start sampling a new configuration.\n",
      "14:37:25 best_vector: [1, 3, 0.004159436911676778, 0.04252484353147726, 0, 0.41371801106466705, 0.649948086761461, 2], 1.9642249022098314e-30, 0.0050910667046067904, -6.970521369445156e-05\n",
      "14:37:25 done sampling a new configuration.\n",
      "14:37:25 HBMASTER: schedule new run for iteration 4\n",
      "14:37:25 HBMASTER: trying submitting job (4, 0, 26) to dispatcher\n",
      "14:37:25 HBMASTER: submitting job (4, 0, 26) to dispatcher\n",
      "14:37:25 DISPATCHER: trying to submit job (4, 0, 26)\n",
      "14:37:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:25 HBMASTER: job (4, 0, 26) submitted to dispatcher\n",
      "14:37:25 DISPATCHER: Trying to submit another job.\n",
      "14:37:25 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:25 DISPATCHER: starting job (4, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:37:25 DISPATCHER: job (4, 0, 26) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:37:25 WORKER: start processing job (4, 0, 26)\n",
      "14:37:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:25 WORKER: args: ()\n",
      "14:37:25 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.0005209959509616249, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 69, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00019683412392623723.\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 9.841706196311861e-05.\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 4.9208530981559306e-05.\n",
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:33 WORKER: done with job (4, 0, 22), trying to register it.\n",
      "14:37:33 WORKER: registered result for job (4, 0, 22) with dispatcher\n",
      "14:37:33 DISPATCHER: job (4, 0, 22) finished\n",
      "14:37:33 DISPATCHER: register_result: lock acquired\n",
      "14:37:33 DISPATCHER: job (4, 0, 22) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:37:33 job_id: (4, 0, 22)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.0031493460684603345, 'loss': 'mae', 'numLayers': 45, 'numNeurons': 17, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 8.34346104825245, 'info': {'L1': 8.34346104825245, 'L2': 14.06591344757117, 'MAX': 4.98668936957055, 'TrainTime': 178.59375}}\n",
      "exception: None\n",
      "\n",
      "14:37:33 job_callback for (4, 0, 22) started\n",
      "14:37:33 DISPATCHER: Trying to submit another job.\n",
      "14:37:33 job_callback for (4, 0, 22) got condition\n",
      "14:37:33 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:33 HBMASTER: Trying to run another job!\n",
      "14:37:33 job_callback for (4, 0, 22) finished\n",
      "14:37:33 start sampling a new configuration.\n",
      "14:37:33 best_vector: [1, 1, 0.7855287121119219, 0.8722196475538989, 0, 0.34870520771676866, 0.18111998862486828, 1], 0.01571975319318292, 0.06455081416074403, 0.001014722867025913\n",
      "14:37:33 done sampling a new configuration.\n",
      "14:37:33 HBMASTER: schedule new run for iteration 5\n",
      "14:37:33 HBMASTER: trying submitting job (5, 0, 0) to dispatcher\n",
      "14:37:33 HBMASTER: submitting job (5, 0, 0) to dispatcher\n",
      "14:37:33 DISPATCHER: trying to submit job (5, 0, 0)\n",
      "14:37:33 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:33 HBMASTER: job (5, 0, 0) submitted to dispatcher\n",
      "14:37:33 DISPATCHER: Trying to submit another job.\n",
      "14:37:33 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:33 DISPATCHER: starting job (5, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:37:33 DISPATCHER: job (5, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:37:33 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:33 WORKER: start processing job (5, 0, 0)\n",
      "14:37:33 WORKER: args: ()\n",
      "14:37:33 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.008734974510783599, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 26, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:37:34 DISPATCHER: Starting worker discovery\n",
      "14:37:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:37:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:37:42 WORKER: done with job (4, 0, 25), trying to register it.\n",
      "14:37:42 WORKER: registered result for job (4, 0, 25) with dispatcher\n",
      "14:37:42 DISPATCHER: job (4, 0, 25) finished\n",
      "14:37:42 DISPATCHER: register_result: lock acquired\n",
      "14:37:42 DISPATCHER: job (4, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:37:42 job_id: (4, 0, 25)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0034377188349175476, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 2.525565044185867, 'info': {'L1': 2.525565044185867, 'L2': 0.630610718419143, 'MAX': 0.6960873603820801, 'TrainTime': 60.3125}}\n",
      "exception: None\n",
      "\n",
      "14:37:42 job_callback for (4, 0, 25) started\n",
      "14:37:42 DISPATCHER: Trying to submit another job.\n",
      "14:37:42 job_callback for (4, 0, 25) got condition\n",
      "14:37:42 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:42 HBMASTER: Trying to run another job!\n",
      "14:37:42 job_callback for (4, 0, 25) finished\n",
      "14:37:42 start sampling a new configuration.\n",
      "14:37:42 done sampling a new configuration.\n",
      "14:37:42 HBMASTER: schedule new run for iteration 5\n",
      "14:37:42 HBMASTER: trying submitting job (5, 0, 1) to dispatcher\n",
      "14:37:42 HBMASTER: submitting job (5, 0, 1) to dispatcher\n",
      "14:37:42 DISPATCHER: trying to submit job (5, 0, 1)\n",
      "14:37:42 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:42 HBMASTER: job (5, 0, 1) submitted to dispatcher\n",
      "14:37:42 DISPATCHER: Trying to submit another job.\n",
      "14:37:42 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:42 DISPATCHER: starting job (5, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:37:42 DISPATCHER: job (5, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:37:42 WORKER: start processing job (5, 0, 1)\n",
      "14:37:42 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:42 WORKER: args: ()\n",
      "14:37:42 WORKER: kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.005378602072966949, 'loss': 'mse', 'numLayers': 37, 'numNeurons': 82, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:37:50 WORKER: done with job (4, 0, 24), trying to register it.\n",
      "14:37:50 WORKER: registered result for job (4, 0, 24) with dispatcher\n",
      "14:37:50 DISPATCHER: job (4, 0, 24) finished\n",
      "14:37:50 DISPATCHER: register_result: lock acquired\n",
      "14:37:50 DISPATCHER: job (4, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:37:50 job_id: (4, 0, 24)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009774186224469633, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 44, 'optimizer': 'Nadam'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 6.512128317909514, 'info': {'L1': 6.512128317909514, 'L2': 5.14085224576656, 'MAX': 2.3165970252864927, 'TrainTime': 96.625}}\n",
      "exception: None\n",
      "\n",
      "14:37:50 job_callback for (4, 0, 24) started\n",
      "14:37:50 DISPATCHER: Trying to submit another job.\n",
      "14:37:50 job_callback for (4, 0, 24) got condition\n",
      "14:37:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:50 HBMASTER: Trying to run another job!\n",
      "14:37:50 job_callback for (4, 0, 24) finished\n",
      "14:37:50 start sampling a new configuration.\n",
      "14:37:50 best_vector: [2, 2, 0.36822247699602306, 0.723094898180273, 1, 0.2047926781297069, 0.13387091308896, 0], 5.845061394915891e-31, 0.017108460158687347, -0.0016413779471907226\n",
      "14:37:50 done sampling a new configuration.\n",
      "14:37:50 HBMASTER: schedule new run for iteration 5\n",
      "14:37:50 HBMASTER: trying submitting job (5, 0, 2) to dispatcher\n",
      "14:37:50 HBMASTER: submitting job (5, 0, 2) to dispatcher\n",
      "14:37:50 DISPATCHER: trying to submit job (5, 0, 2)\n",
      "14:37:50 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:50 HBMASTER: job (5, 0, 2) submitted to dispatcher\n",
      "14:37:50 DISPATCHER: Trying to submit another job.\n",
      "14:37:50 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:50 DISPATCHER: starting job (5, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:37:50 DISPATCHER: job (5, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:37:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:50 WORKER: start processing job (5, 0, 2)\n",
      "14:37:50 WORKER: args: ()\n",
      "14:37:50 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.007258639491984703, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 22, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:37:53 WORKER: done with job (4, 0, 26), trying to register it.\n",
      "14:37:53 WORKER: registered result for job (4, 0, 26) with dispatcher\n",
      "14:37:53 DISPATCHER: job (4, 0, 26) finished\n",
      "14:37:53 DISPATCHER: register_result: lock acquired\n",
      "14:37:53 DISPATCHER: job (4, 0, 26) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:37:53 job_id: (4, 0, 26)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.0005209959509616249, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 69, 'optimizer': 'SGD'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 86.3356880260387, 'info': {'L1': 86.3356880260387, 'L2': 383.21976281300437, 'MAX': 4.7377767860889435, 'TrainTime': 54.171875}}\n",
      "exception: None\n",
      "\n",
      "14:37:53 job_callback for (4, 0, 26) started\n",
      "14:37:53 DISPATCHER: Trying to submit another job.\n",
      "14:37:53 job_callback for (4, 0, 26) got condition\n",
      "14:37:53 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:37:53 HBMASTER: Trying to run another job!\n",
      "14:37:53 job_callback for (4, 0, 26) finished\n",
      "14:37:53 start sampling a new configuration.\n",
      "14:37:53 best_vector: [0, 2, 0.6629945878614094, 0.8713294850546824, 0, 0.2980543178471805, 0.6053367845515352, 0], 2.5873725904607883e-31, 0.038649246099569635, -0.0016246219878316365\n",
      "14:37:53 done sampling a new configuration.\n",
      "14:37:53 HBMASTER: schedule new run for iteration 5\n",
      "14:37:53 HBMASTER: trying submitting job (5, 0, 3) to dispatcher\n",
      "14:37:53 HBMASTER: submitting job (5, 0, 3) to dispatcher\n",
      "14:37:53 DISPATCHER: trying to submit job (5, 0, 3)\n",
      "14:37:53 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:37:53 HBMASTER: job (5, 0, 3) submitted to dispatcher\n",
      "14:37:53 DISPATCHER: Trying to submit another job.\n",
      "14:37:53 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:37:53 DISPATCHER: starting job (5, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:37:53 DISPATCHER: job (5, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:37:53 WORKER: start processing job (5, 0, 3)\n",
      "14:37:53 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:37:53 WORKER: args: ()\n",
      "14:37:53 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.008726161902041356, 'loss': 'mae', 'numLayers': 16, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 0.004367487039417028.\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.002183743519708514.\n",
      "\n",
      "Epoch 00288: ReduceLROnPlateau reducing learning rate to 0.001091871759854257.\n",
      "\n",
      "Epoch 00322: ReduceLROnPlateau reducing learning rate to 0.0005459358799271286.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:27 WORKER: done with job (5, 0, 0), trying to register it.\n",
      "14:38:27 WORKER: registered result for job (5, 0, 0) with dispatcher\n",
      "14:38:27 DISPATCHER: job (5, 0, 0) finished\n",
      "14:38:27 DISPATCHER: register_result: lock acquired\n",
      "14:38:27 DISPATCHER: job (5, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:38:27 job_id: (5, 0, 0)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 11, 'initial_lr': 0.008734974510783599, 'loss': 'mae', 'numLayers': 19, 'numNeurons': 26, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 35.54189634729973, 'info': {'L1': 35.54189634729973, 'L2': 90.90136980296359, 'MAX': 3.5866448425753683, 'TrainTime': 109.703125}}\n",
      "exception: None\n",
      "\n",
      "14:38:27 job_callback for (5, 0, 0) started\n",
      "14:38:27 DISPATCHER: Trying to submit another job.\n",
      "14:38:27 job_callback for (5, 0, 0) got condition\n",
      "14:38:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:38:27 done building a new model for budget 333.333333 based on 9/16 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:38:27 HBMASTER: Trying to run another job!\n",
      "14:38:27 job_callback for (5, 0, 0) finished\n",
      "14:38:27 start sampling a new configuration.\n",
      "14:38:27 best_vector: [0, 2, 0.3189346705741135, 0.8118168933190949, 1, 0.13275318872548547, 0.3618642305790174, 0], 1.3809355307281626e-30, 0.007241467669911451, -4.577408457580988e-05\n",
      "14:38:27 done sampling a new configuration.\n",
      "14:38:27 HBMASTER: schedule new run for iteration 5\n",
      "14:38:27 HBMASTER: trying submitting job (5, 0, 4) to dispatcher\n",
      "14:38:27 HBMASTER: submitting job (5, 0, 4) to dispatcher\n",
      "14:38:27 DISPATCHER: trying to submit job (5, 0, 4)\n",
      "14:38:27 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:38:27 HBMASTER: job (5, 0, 4) submitted to dispatcher\n",
      "14:38:27 DISPATCHER: Trying to submit another job.\n",
      "14:38:27 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:38:27 DISPATCHER: starting job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:38:27 DISPATCHER: job (5, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:38:27 WORKER: start processing job (5, 0, 4)\n",
      "14:38:27 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:38:27 WORKER: args: ()\n",
      "14:38:27 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:34 DISPATCHER: Starting worker discovery\n",
      "14:38:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:38:34 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 0.004363080952316523.\n",
      "\n",
      "Epoch 00287: ReduceLROnPlateau reducing learning rate to 0.0021815404761582613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:38:46 WORKER: done with job (5, 0, 2), trying to register it.\n",
      "14:38:46 WORKER: registered result for job (5, 0, 2) with dispatcher\n",
      "14:38:46 DISPATCHER: job (5, 0, 2) finished\n",
      "14:38:46 DISPATCHER: register_result: lock acquired\n",
      "14:38:46 DISPATCHER: job (5, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:38:46 job_id: (5, 0, 2)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.007258639491984703, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 22, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 1.3801820155574405, 'info': {'L1': 1.3801820155574405, 'L2': 0.22023441672295838, 'MAX': 0.39621910714521924, 'TrainTime': 129.78125}}\n",
      "exception: None\n",
      "\n",
      "14:38:46 job_callback for (5, 0, 2) started\n",
      "14:38:46 DISPATCHER: Trying to submit another job.\n",
      "14:38:46 job_callback for (5, 0, 2) got condition\n",
      "14:38:46 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:38:46 done building a new model for budget 333.333333 based on 9/17 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:38:46 HBMASTER: Trying to run another job!\n",
      "14:38:46 job_callback for (5, 0, 2) finished\n",
      "14:38:46 start sampling a new configuration.\n",
      "14:38:46 best_vector: [4, 0, 0.057664769681190164, 0.41135440689245484, 1, 0.8698709068216166, 0.34061307478159486, 1], 1.048799834411874e-29, 0.0009534707836417241, -9.325945451464146e-05\n",
      "14:38:46 done sampling a new configuration.\n",
      "14:38:46 HBMASTER: schedule new run for iteration 5\n",
      "14:38:46 HBMASTER: trying submitting job (5, 0, 5) to dispatcher\n",
      "14:38:46 HBMASTER: submitting job (5, 0, 5) to dispatcher\n",
      "14:38:46 DISPATCHER: trying to submit job (5, 0, 5)\n",
      "14:38:46 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:38:46 HBMASTER: job (5, 0, 5) submitted to dispatcher\n",
      "14:38:46 DISPATCHER: Trying to submit another job.\n",
      "14:38:46 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:38:46 DISPATCHER: starting job (5, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:38:46 DISPATCHER: job (5, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:38:46 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:38:46 WORKER: start processing job (5, 0, 5)\n",
      "14:38:46 WORKER: args: ()\n",
      "14:38:46 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 5, 'initial_lr': 0.004172408628235304, 'loss': 'mse', 'numLayers': 44, 'numNeurons': 40, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:38:51 WORKER: done with job (5, 0, 3), trying to register it.\n",
      "14:38:51 WORKER: registered result for job (5, 0, 3) with dispatcher\n",
      "14:38:51 DISPATCHER: job (5, 0, 3) finished\n",
      "14:38:51 DISPATCHER: register_result: lock acquired\n",
      "14:38:51 DISPATCHER: job (5, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:38:51 job_id: (5, 0, 3)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.008726161902041356, 'loss': 'mae', 'numLayers': 16, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 8.286115847529564, 'info': {'L1': 8.286115847529564, 'L2': 12.72165140375571, 'MAX': 4.123144902103593, 'TrainTime': 135.1875}}\n",
      "exception: None\n",
      "\n",
      "14:38:51 job_callback for (5, 0, 3) started\n",
      "14:38:51 DISPATCHER: Trying to submit another job.\n",
      "14:38:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:38:51 job_callback for (5, 0, 3) got condition\n",
      "14:38:52 done building a new model for budget 333.333333 based on 9/17 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:38:52 HBMASTER: Trying to run another job!\n",
      "14:38:52 job_callback for (5, 0, 3) finished\n",
      "14:38:52 start sampling a new configuration.\n",
      "14:38:52 done sampling a new configuration.\n",
      "14:38:52 HBMASTER: schedule new run for iteration 5\n",
      "14:38:52 HBMASTER: trying submitting job (5, 0, 6) to dispatcher\n",
      "14:38:52 HBMASTER: submitting job (5, 0, 6) to dispatcher\n",
      "14:38:52 DISPATCHER: trying to submit job (5, 0, 6)\n",
      "14:38:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:38:52 HBMASTER: job (5, 0, 6) submitted to dispatcher\n",
      "14:38:52 DISPATCHER: Trying to submit another job.\n",
      "14:38:52 DISPATCHER: starting job (5, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:38:52 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:38:52 DISPATCHER: job (5, 0, 6) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:38:52 WORKER: start processing job (5, 0, 6)\n",
      "14:38:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:38:52 WORKER: args: ()\n",
      "14:38:52 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008381636638248738, 'loss': 'mae', 'numLayers': 32, 'numNeurons': 44, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 500 \n",
      "Batch size: 500 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00304: ReduceLROnPlateau reducing learning rate to 0.004068493843078613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:39:17 WORKER: done with job (5, 0, 1), trying to register it.\n",
      "14:39:17 WORKER: registered result for job (5, 0, 1) with dispatcher\n",
      "14:39:17 DISPATCHER: job (5, 0, 1) finished\n",
      "14:39:18 DISPATCHER: register_result: lock acquired\n",
      "14:39:18 DISPATCHER: job (5, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:39:18 job_id: (5, 0, 1)\n",
      "kwargs: {'config': {'activator': 'Addons>softshrink', 'batch_size': 15000, 'denspt': 12, 'initial_lr': 0.005378602072966949, 'loss': 'mse', 'numLayers': 37, 'numNeurons': 82, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 56.13501292804589, 'info': {'L1': 56.13501292804589, 'L2': 166.5643622128901, 'MAX': 3.2125786542892456, 'TrainTime': 202.4375}}\n",
      "exception: None\n",
      "\n",
      "14:39:18 job_callback for (5, 0, 1) started\n",
      "14:39:18 DISPATCHER: Trying to submit another job.\n",
      "14:39:18 job_callback for (5, 0, 1) got condition\n",
      "14:39:18 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:39:18 done building a new model for budget 333.333333 based on 9/18 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:39:18 HBMASTER: Trying to run another job!\n",
      "14:39:18 job_callback for (5, 0, 1) finished\n",
      "14:39:18 start sampling a new configuration.\n",
      "14:39:18 done sampling a new configuration.\n",
      "14:39:18 HBMASTER: schedule new run for iteration 5\n",
      "14:39:18 HBMASTER: trying submitting job (5, 0, 7) to dispatcher\n",
      "14:39:18 HBMASTER: submitting job (5, 0, 7) to dispatcher\n",
      "14:39:18 DISPATCHER: trying to submit job (5, 0, 7)\n",
      "14:39:18 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:39:18 HBMASTER: job (5, 0, 7) submitted to dispatcher\n",
      "14:39:18 DISPATCHER: Trying to submit another job.\n",
      "14:39:18 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:39:18 DISPATCHER: starting job (5, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:39:18 DISPATCHER: job (5, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:39:18 WORKER: start processing job (5, 0, 7)\n",
      "14:39:18 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:39:18 WORKER: args: ()\n",
      "14:39:18 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0010714680121203894, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 46, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:39:19 WORKER: done with job (5, 0, 4), trying to register it.\n",
      "14:39:19 WORKER: registered result for job (5, 0, 4) with dispatcher\n",
      "14:39:19 DISPATCHER: job (5, 0, 4) finished\n",
      "14:39:19 DISPATCHER: register_result: lock acquired\n",
      "14:39:19 DISPATCHER: job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:39:19 job_id: (5, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 0.33812513189675153, 'info': {'L1': 0.33812513189675153, 'L2': 0.008194947726458212, 'MAX': 0.19441344906025826, 'TrainTime': 149.515625}}\n",
      "exception: None\n",
      "\n",
      "14:39:19 job_callback for (5, 0, 4) started\n",
      "14:39:19 DISPATCHER: Trying to submit another job.\n",
      "14:39:19 job_callback for (5, 0, 4) got condition\n",
      "14:39:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:39:19 done building a new model for budget 333.333333 based on 9/19 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:39:19 HBMASTER: Trying to run another job!\n",
      "14:39:19 job_callback for (5, 0, 4) finished\n",
      "14:39:19 start sampling a new configuration.\n",
      "14:39:19 best_vector: [1, 2, 0.3873232539121739, 0.6103941835893705, 1, 0.2618338950064072, 0.03702568085252657, 0], 0.05260772162618178, 0.03204621463509797, 0.0016858783386961066\n",
      "14:39:19 done sampling a new configuration.\n",
      "14:39:19 HBMASTER: schedule new run for iteration 5\n",
      "14:39:19 HBMASTER: trying submitting job (5, 0, 8) to dispatcher\n",
      "14:39:19 HBMASTER: submitting job (5, 0, 8) to dispatcher\n",
      "14:39:19 DISPATCHER: trying to submit job (5, 0, 8)\n",
      "14:39:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:39:19 HBMASTER: job (5, 0, 8) submitted to dispatcher\n",
      "14:39:19 DISPATCHER: Trying to submit another job.\n",
      "14:39:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:39:19 DISPATCHER: starting job (5, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:39:19 DISPATCHER: job (5, 0, 8) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:39:19 WORKER: start processing job (5, 0, 8)\n",
      "14:39:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:39:19 WORKER: args: ()\n",
      "14:39:19 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.0061429024175347685, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 13, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.002086204243823886.\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.001043102121911943.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:39:34 DISPATCHER: Starting worker discovery\n",
      "14:39:34 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:39:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.0005215510609559715.\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.00026077553047798574.\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.0005357339978218079.\n",
      "\n",
      "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.00013038776523899287.\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.00026786699891090393.\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 6.519388261949643e-05.\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 3.259694130974822e-05.\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 0.00013393349945545197.\n",
      "\n",
      "Epoch 00275: ReduceLROnPlateau reducing learning rate to 1.629847065487411e-05.\n",
      "\n",
      "Epoch 00245: ReduceLROnPlateau reducing learning rate to 0.0030714510940015316.\n",
      "\n",
      "Epoch 00285: ReduceLROnPlateau reducing learning rate to 6.696674972772598e-05.\n",
      "\n",
      "Epoch 00309: ReduceLROnPlateau reducing learning rate to 8.149235327437054e-06.\n",
      "\n",
      "Epoch 00279: ReduceLROnPlateau reducing learning rate to 0.0015357255470007658.\n",
      "\n",
      "Epoch 00322: ReduceLROnPlateau reducing learning rate to 3.348337486386299e-05.\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 0.0007678627735003829.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:40:06 WORKER: done with job (5, 0, 5), trying to register it.\n",
      "14:40:06 WORKER: registered result for job (5, 0, 5) with dispatcher\n",
      "14:40:06 DISPATCHER: job (5, 0, 5) finished\n",
      "14:40:06 DISPATCHER: register_result: lock acquired\n",
      "14:40:06 DISPATCHER: job (5, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:40:06 job_id: (5, 0, 5)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 5, 'initial_lr': 0.004172408628235304, 'loss': 'mse', 'numLayers': 44, 'numNeurons': 40, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 84.2033584319336, 'info': {'L1': 84.2033584319336, 'L2': 365.0231566686341, 'MAX': 4.629674792289734, 'TrainTime': 208.90625}}\n",
      "exception: None\n",
      "\n",
      "14:40:06 job_callback for (5, 0, 5) started\n",
      "14:40:06 DISPATCHER: Trying to submit another job.\n",
      "14:40:06 job_callback for (5, 0, 5) got condition\n",
      "14:40:06 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:40:06 done building a new model for budget 333.333333 based on 9/20 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:40:06 HBMASTER: Trying to run another job!\n",
      "14:40:06 job_callback for (5, 0, 5) finished\n",
      "14:40:06 start sampling a new configuration.\n",
      "14:40:06 done sampling a new configuration.\n",
      "14:40:06 HBMASTER: schedule new run for iteration 6\n",
      "14:40:06 HBMASTER: trying submitting job (6, 0, 0) to dispatcher\n",
      "14:40:06 HBMASTER: submitting job (6, 0, 0) to dispatcher\n",
      "14:40:06 DISPATCHER: trying to submit job (6, 0, 0)\n",
      "14:40:06 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:40:06 HBMASTER: job (6, 0, 0) submitted to dispatcher\n",
      "14:40:06 DISPATCHER: Trying to submit another job.\n",
      "14:40:06 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:40:06 DISPATCHER: starting job (6, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:40:06 DISPATCHER: job (6, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:40:06 WORKER: start processing job (6, 0, 0)\n",
      "14:40:06 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:40:06 WORKER: args: ()\n",
      "14:40:06 WORKER: kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 6, 'initial_lr': 0.005606099229266038, 'loss': 'mse', 'numLayers': 3, 'numNeurons': 40, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:40:08 WORKER: done with job (5, 0, 7), trying to register it.\n",
      "14:40:08 WORKER: registered result for job (5, 0, 7) with dispatcher\n",
      "14:40:08 DISPATCHER: job (5, 0, 7) finished\n",
      "14:40:08 DISPATCHER: register_result: lock acquired\n",
      "14:40:08 DISPATCHER: job (5, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:40:08 job_id: (5, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0010714680121203894, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 46, 'optimizer': 'RMSprop'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 3.4743482354739457, 'info': {'L1': 3.4743482354739457, 'L2': 1.6417286919689855, 'MAX': 0.915593057345998, 'TrainTime': 142.734375}}\n",
      "exception: None\n",
      "\n",
      "14:40:08 job_callback for (5, 0, 7) started\n",
      "14:40:08 DISPATCHER: Trying to submit another job.\n",
      "14:40:08 job_callback for (5, 0, 7) got condition\n",
      "14:40:08 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:40:08 done building a new model for budget 333.333333 based on 9/21 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:40:08 HBMASTER: Trying to run another job!\n",
      "14:40:08 job_callback for (5, 0, 7) finished\n",
      "14:40:08 start sampling a new configuration.\n",
      "14:40:08 best_vector: [2, 2, 0.7820725522295512, 0.2920162582583934, 1, 0.2593589791880626, 0.46617560374151934, 0], 5.193235444998084e-31, 0.01925581866239398, -0.02361276158114841\n",
      "14:40:08 done sampling a new configuration.\n",
      "14:40:08 HBMASTER: schedule new run for iteration 6\n",
      "14:40:08 HBMASTER: trying submitting job (6, 0, 1) to dispatcher\n",
      "14:40:08 HBMASTER: submitting job (6, 0, 1) to dispatcher\n",
      "14:40:08 DISPATCHER: trying to submit job (6, 0, 1)\n",
      "14:40:08 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:40:08 HBMASTER: job (6, 0, 1) submitted to dispatcher\n",
      "14:40:08 DISPATCHER: Trying to submit another job.\n",
      "14:40:08 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:40:08 DISPATCHER: starting job (6, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:40:08 DISPATCHER: job (6, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:40:08 WORKER: start processing job (6, 0, 1)\n",
      "14:40:08 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:40:08 WORKER: args: ()\n",
      "14:40:08 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0029909609567580946, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 52, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "14:40:09 WORKER: done with job (5, 0, 8), trying to register it.\n",
      "14:40:09 WORKER: registered result for job (5, 0, 8) with dispatcher\n",
      "14:40:09 DISPATCHER: job (5, 0, 8) finished\n",
      "14:40:09 DISPATCHER: register_result: lock acquired\n",
      "14:40:09 DISPATCHER: job (5, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:40:10 job_id: (5, 0, 8)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.0061429024175347685, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 13, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 34.562506794200154, 'info': {'L1': 34.562506794200154, 'L2': 82.1964565069897, 'MAX': 3.4405219129999427, 'TrainTime': 140.609375}}\n",
      "exception: None\n",
      "\n",
      "14:40:10 job_callback for (5, 0, 8) started\n",
      "14:40:10 DISPATCHER: Trying to submit another job.\n",
      "14:40:10 job_callback for (5, 0, 8) got condition\n",
      "14:40:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:40:10 done building a new model for budget 333.333333 based on 9/22 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:40:10 HBMASTER: Trying to run another job!\n",
      "14:40:10 job_callback for (5, 0, 8) finished\n",
      "14:40:10 start sampling a new configuration.\n",
      "14:40:10 best_vector: [2, 1, 0.5247673796229864, 0.13492350078218268, 1, 0.33330104094045526, 0.5307514860610936, 1], 1.7110742632405127e-30, 0.005844281697663742, -0.003949788780113981\n",
      "14:40:10 done sampling a new configuration.\n",
      "14:40:10 HBMASTER: schedule new run for iteration 6\n",
      "14:40:10 HBMASTER: trying submitting job (6, 0, 2) to dispatcher\n",
      "14:40:10 HBMASTER: submitting job (6, 0, 2) to dispatcher\n",
      "14:40:10 DISPATCHER: trying to submit job (6, 0, 2)\n",
      "14:40:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:40:10 HBMASTER: job (6, 0, 2) submitted to dispatcher\n",
      "14:40:10 DISPATCHER: Trying to submit another job.\n",
      "14:40:10 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:40:10 DISPATCHER: starting job (6, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:40:10 DISPATCHER: job (6, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:40:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:40:10 WORKER: start processing job (6, 0, 2)\n",
      "14:40:10 WORKER: args: ()\n",
      "14:40:10 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 9, 'initial_lr': 0.0014357426577436088, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 58, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:40:35 DISPATCHER: Starting worker discovery\n",
      "14:40:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:40:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00249: ReduceLROnPlateau reducing learning rate to 0.0007178713567554951.\n",
      "\n",
      "Epoch 00349: ReduceLROnPlateau reducing learning rate to 0.00035893567837774754.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:41:35 DISPATCHER: Starting worker discovery\n",
      "14:41:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:41:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00546: ReduceLROnPlateau reducing learning rate to 0.00017946783918887377.\n",
      "\n",
      "Epoch 00571: ReduceLROnPlateau reducing learning rate to 0.001495480420999229.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:41:50 WORKER: done with job (4, 0, 19), trying to register it.\n",
      "14:41:50 WORKER: registered result for job (4, 0, 19) with dispatcher\n",
      "14:41:50 DISPATCHER: job (4, 0, 19) finished\n",
      "14:41:50 DISPATCHER: register_result: lock acquired\n",
      "14:41:50 DISPATCHER: job (4, 0, 19) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:41:50 job_id: (4, 0, 19)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 25000, 'denspt': 5, 'initial_lr': 0.005691285979886495, 'loss': 'mae', 'numLayers': 50, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 111.1111111111111, 'working_directory': '.'}\n",
      "result: {'loss': 89.04017478117137, 'info': {'L1': 89.04017478117137, 'L2': 406.9781363027377, 'MAX': 4.871864438056946, 'TrainTime': 772.140625}}\n",
      "exception: None\n",
      "\n",
      "14:41:50 job_callback for (4, 0, 19) started\n",
      "14:41:50 DISPATCHER: Trying to submit another job.\n",
      "14:41:50 job_callback for (4, 0, 19) got condition\n",
      "14:41:50 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:41:50 HBMASTER: Trying to run another job!\n",
      "14:41:50 job_callback for (4, 0, 19) finished\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 4) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 7) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 8) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 16) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 17) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 18) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 21) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 24) to next budget 333.333333\n",
      "14:41:50 ITERATION: Advancing config (4, 0, 25) to next budget 333.333333\n",
      "14:41:50 HBMASTER: schedule new run for iteration 4\n",
      "14:41:50 HBMASTER: trying submitting job (4, 0, 4) to dispatcher\n",
      "14:41:50 HBMASTER: submitting job (4, 0, 4) to dispatcher\n",
      "14:41:50 DISPATCHER: trying to submit job (4, 0, 4)\n",
      "14:41:50 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:41:50 HBMASTER: job (4, 0, 4) submitted to dispatcher\n",
      "14:41:50 DISPATCHER: Trying to submit another job.\n",
      "14:41:50 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:41:50 DISPATCHER: starting job (4, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:41:50 DISPATCHER: job (4, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:41:50 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:41:50 WORKER: start processing job (4, 0, 4)\n",
      "14:41:50 WORKER: args: ()\n",
      "14:41:50 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.0021317022535715558, 'loss': 'mae', 'numLayers': 42, 'numNeurons': 80, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00657: ReduceLROnPlateau reducing learning rate to 8.973391959443688e-05.\n",
      "\n",
      "Total samples: 1620 \n",
      "Batch size: 1620 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00757: ReduceLROnPlateau reducing learning rate to 4.486695979721844e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:42:27 WORKER: done with job (6, 0, 0), trying to register it.\n",
      "14:42:27 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
      "14:42:27 DISPATCHER: job (6, 0, 0) finished\n",
      "14:42:27 DISPATCHER: register_result: lock acquired\n",
      "14:42:27 DISPATCHER: job (6, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:42:27 job_id: (6, 0, 0)\n",
      "kwargs: {'config': {'activator': 'swish', 'batch_size': 25000, 'denspt': 6, 'initial_lr': 0.005606099229266038, 'loss': 'mse', 'numLayers': 3, 'numNeurons': 40, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 5.775838302963853, 'info': {'L1': 5.775838302963853, 'L2': 2.3345863811350656, 'MAX': 1.010593085966284, 'TrainTime': 435.921875}}\n",
      "exception: None\n",
      "\n",
      "14:42:27 job_callback for (6, 0, 0) started\n",
      "14:42:27 DISPATCHER: Trying to submit another job.\n",
      "14:42:27 job_callback for (6, 0, 0) got condition\n",
      "14:42:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:42:28 HBMASTER: Trying to run another job!\n",
      "14:42:28 job_callback for (6, 0, 0) finished\n",
      "14:42:28 HBMASTER: schedule new run for iteration 4\n",
      "14:42:28 HBMASTER: trying submitting job (4, 0, 7) to dispatcher\n",
      "14:42:28 HBMASTER: submitting job (4, 0, 7) to dispatcher\n",
      "14:42:28 DISPATCHER: trying to submit job (4, 0, 7)\n",
      "14:42:28 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:42:28 HBMASTER: job (4, 0, 7) submitted to dispatcher\n",
      "14:42:28 DISPATCHER: Trying to submit another job.\n",
      "14:42:28 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:42:28 DISPATCHER: starting job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:42:28 DISPATCHER: job (4, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:42:28 WORKER: start processing job (4, 0, 7)\n",
      "14:42:28 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:42:28 WORKER: args: ()\n",
      "14:42:28 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.001065851072780788.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:42:35 DISPATCHER: Starting worker discovery\n",
      "14:42:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:42:35 DISPATCHER: Finished worker discovery\n",
      "14:42:44 WORKER: done with job (6, 0, 1), trying to register it.\n",
      "14:42:44 WORKER: registered result for job (6, 0, 1) with dispatcher\n",
      "14:42:44 DISPATCHER: job (6, 0, 1) finished\n",
      "14:42:44 DISPATCHER: register_result: lock acquired\n",
      "14:42:44 DISPATCHER: job (6, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:42:44 job_id: (6, 0, 1)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0029909609567580946, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 52, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 3.2020075329549185, 'info': {'L1': 3.2020075329549185, 'L2': 1.178893049254775, 'MAX': 0.6746153455941748, 'TrainTime': 471.828125}}\n",
      "exception: None\n",
      "\n",
      "14:42:44 job_callback for (6, 0, 1) started\n",
      "14:42:44 DISPATCHER: Trying to submit another job.\n",
      "14:42:44 job_callback for (6, 0, 1) got condition\n",
      "14:42:44 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:42:44 HBMASTER: Trying to run another job!\n",
      "14:42:44 job_callback for (6, 0, 1) finished\n",
      "14:42:44 HBMASTER: schedule new run for iteration 4\n",
      "14:42:44 HBMASTER: trying submitting job (4, 0, 8) to dispatcher\n",
      "14:42:44 HBMASTER: submitting job (4, 0, 8) to dispatcher\n",
      "14:42:44 DISPATCHER: trying to submit job (4, 0, 8)\n",
      "14:42:44 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:42:44 HBMASTER: job (4, 0, 8) submitted to dispatcher\n",
      "14:42:44 DISPATCHER: Trying to submit another job.\n",
      "14:42:44 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:42:44 DISPATCHER: starting job (4, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:42:44 DISPATCHER: job (4, 0, 8) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:42:44 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:42:44 WORKER: start processing job (4, 0, 8)\n",
      "14:42:44 WORKER: args: ()\n",
      "14:42:44 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.006780767493201843, 'loss': 'mae', 'numLayers': 30, 'numNeurons': 58, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.000532925536390394.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:42:59 WORKER: done with job (6, 0, 2), trying to register it.\n",
      "14:42:59 WORKER: registered result for job (6, 0, 2) with dispatcher\n",
      "14:42:59 DISPATCHER: job (6, 0, 2) finished\n",
      "14:42:59 DISPATCHER: register_result: lock acquired\n",
      "14:42:59 DISPATCHER: job (6, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:42:59 job_id: (6, 0, 2)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 9, 'initial_lr': 0.0014357426577436088, 'loss': 'mse', 'numLayers': 18, 'numNeurons': 58, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 4.309833122283068, 'info': {'L1': 4.309833122283068, 'L2': 2.528311132449244, 'MAX': 1.0845858688067418, 'TrainTime': 493.296875}}\n",
      "exception: None\n",
      "\n",
      "14:42:59 job_callback for (6, 0, 2) started\n",
      "14:42:59 DISPATCHER: Trying to submit another job.\n",
      "14:42:59 job_callback for (6, 0, 2) got condition\n",
      "14:42:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:42:59 HBMASTER: Trying to run another job!\n",
      "14:42:59 job_callback for (6, 0, 2) finished\n",
      "14:42:59 HBMASTER: schedule new run for iteration 4\n",
      "14:42:59 HBMASTER: trying submitting job (4, 0, 16) to dispatcher\n",
      "14:42:59 HBMASTER: submitting job (4, 0, 16) to dispatcher\n",
      "14:42:59 DISPATCHER: trying to submit job (4, 0, 16)\n",
      "14:42:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:42:59 HBMASTER: job (4, 0, 16) submitted to dispatcher\n",
      "14:42:59 DISPATCHER: Trying to submit another job.\n",
      "14:42:59 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:42:59 DISPATCHER: starting job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:42:59 DISPATCHER: job (4, 0, 16) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:42:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:42:59 WORKER: start processing job (4, 0, 16)\n",
      "14:42:59 WORKER: args: ()\n",
      "14:42:59 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 0.000266462768195197.\n",
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.00020023132674396038.\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0033903836738318205.\n",
      "\n",
      "Epoch 00191: ReduceLROnPlateau reducing learning rate to 0.00010011566337198019.\n",
      "\n",
      "Epoch 00300: ReduceLROnPlateau reducing learning rate to 0.004190818406641483.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:43:35 DISPATCHER: Starting worker discovery\n",
      "14:43:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:43:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 5.0057831685990095e-05.\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0018838440300896764.\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.0016951918369159102.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:43:50 WORKER: done with job (4, 0, 4), trying to register it.\n",
      "14:43:50 WORKER: registered result for job (4, 0, 4) with dispatcher\n",
      "14:43:50 DISPATCHER: job (4, 0, 4) finished\n",
      "14:43:50 DISPATCHER: register_result: lock acquired\n",
      "14:43:50 DISPATCHER: job (4, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:43:50 job_id: (4, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 9, 'initial_lr': 0.0021317022535715558, 'loss': 'mae', 'numLayers': 42, 'numNeurons': 80, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 7.391747981451714, 'info': {'L1': 7.391747981451714, 'L2': 12.372903729603484, 'MAX': 4.778028290163812, 'TrainTime': 331.8125}}\n",
      "exception: None\n",
      "\n",
      "14:43:50 job_callback for (4, 0, 4) started\n",
      "14:43:50 DISPATCHER: Trying to submit another job.\n",
      "14:43:50 job_callback for (4, 0, 4) got condition\n",
      "14:43:51 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:43:51 done building a new model for budget 333.333333 based on 9/22 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:43:51 HBMASTER: Trying to run another job!\n",
      "14:43:51 job_callback for (4, 0, 4) finished\n",
      "14:43:51 HBMASTER: schedule new run for iteration 4\n",
      "14:43:51 HBMASTER: trying submitting job (4, 0, 17) to dispatcher\n",
      "14:43:51 HBMASTER: submitting job (4, 0, 17) to dispatcher\n",
      "14:43:51 DISPATCHER: trying to submit job (4, 0, 17)\n",
      "14:43:51 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:43:51 HBMASTER: job (4, 0, 17) submitted to dispatcher\n",
      "14:43:51 DISPATCHER: Trying to submit another job.\n",
      "14:43:51 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:43:51 DISPATCHER: starting job (4, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:43:51 DISPATCHER: job (4, 0, 17) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:43:51 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:43:51 WORKER: start processing job (4, 0, 17)\n",
      "14:43:51 WORKER: args: ()\n",
      "14:43:51 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0054458535690848926, 'loss': 'mae', 'numLayers': 13, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 2.5028915842995048e-05.\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:43:59 WORKER: done with job (5, 0, 6), trying to register it.\n",
      "14:43:59 WORKER: registered result for job (5, 0, 6) with dispatcher\n",
      "14:43:59 DISPATCHER: job (5, 0, 6) finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 0.0009419220150448382.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:43:59 DISPATCHER: register_result: lock acquired\n",
      "14:43:59 DISPATCHER: job (5, 0, 6) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:43:59 job_id: (5, 0, 6)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.008381636638248738, 'loss': 'mae', 'numLayers': 32, 'numNeurons': 44, 'optimizer': 'SGD'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 7.929224259449309, 'info': {'L1': 7.929224259449309, 'L2': 13.203926630698962, 'MAX': 4.931309025179681, 'TrainTime': 809.34375}}\n",
      "exception: None\n",
      "\n",
      "14:43:59 job_callback for (5, 0, 6) started\n",
      "14:43:59 DISPATCHER: Trying to submit another job.\n",
      "14:43:59 job_callback for (5, 0, 6) got condition\n",
      "14:43:59 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:43:59 done building a new model for budget 333.333333 based on 9/23 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:43:59 HBMASTER: Trying to run another job!\n",
      "14:43:59 job_callback for (5, 0, 6) finished\n",
      "14:43:59 HBMASTER: schedule new run for iteration 4\n",
      "14:43:59 HBMASTER: trying submitting job (4, 0, 18) to dispatcher\n",
      "14:43:59 HBMASTER: submitting job (4, 0, 18) to dispatcher\n",
      "14:43:59 DISPATCHER: trying to submit job (4, 0, 18)\n",
      "14:43:59 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:43:59 HBMASTER: job (4, 0, 18) submitted to dispatcher\n",
      "14:43:59 DISPATCHER: Trying to submit another job.\n",
      "14:43:59 DISPATCHER: starting job (4, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:43:59 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:43:59 DISPATCHER: job (4, 0, 18) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:43:59 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:43:59 WORKER: start processing job (4, 0, 18)\n",
      "14:43:59 WORKER: args: ()\n",
      "14:43:59 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.0007077792074795277, 'loss': 'mae', 'numLayers': 17, 'numNeurons': 75, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "14:44:00 WORKER: done with job (4, 0, 7), trying to register it.\n",
      "14:44:00 WORKER: registered result for job (4, 0, 7) with dispatcher\n",
      "14:44:00 DISPATCHER: job (4, 0, 7) finished\n",
      "14:44:00 DISPATCHER: register_result: lock acquired\n",
      "14:44:00 DISPATCHER: job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:44:00 job_id: (4, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 6.051647493898673, 'info': {'L1': 6.051647493898673, 'L2': 6.214000568152539, 'MAX': 2.203922927203893, 'TrainTime': 258.671875}}\n",
      "exception: None\n",
      "\n",
      "14:44:00 job_callback for (4, 0, 7) started\n",
      "14:44:00 job_callback for (4, 0, 7) got condition\n",
      "14:44:00 DISPATCHER: Trying to submit another job.\n",
      "14:44:00 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:44:00 done building a new model for budget 333.333333 based on 9/24 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:44:00 HBMASTER: Trying to run another job!\n",
      "14:44:00 job_callback for (4, 0, 7) finished\n",
      "14:44:00 HBMASTER: schedule new run for iteration 4\n",
      "14:44:00 HBMASTER: trying submitting job (4, 0, 21) to dispatcher\n",
      "14:44:00 HBMASTER: submitting job (4, 0, 21) to dispatcher\n",
      "14:44:00 DISPATCHER: trying to submit job (4, 0, 21)\n",
      "14:44:00 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:44:00 HBMASTER: job (4, 0, 21) submitted to dispatcher\n",
      "14:44:00 DISPATCHER: Trying to submit another job.\n",
      "14:44:00 DISPATCHER: starting job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:44:00 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:44:00 DISPATCHER: job (4, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:44:00 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:44:00 WORKER: start processing job (4, 0, 21)\n",
      "14:44:00 WORKER: args: ()\n",
      "14:44:00 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 0.0008475959184579551.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.0004709610075224191.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:44:13 WORKER: done with job (4, 0, 8), trying to register it.\n",
      "14:44:13 WORKER: registered result for job (4, 0, 8) with dispatcher\n",
      "14:44:13 DISPATCHER: job (4, 0, 8) finished\n",
      "14:44:13 DISPATCHER: register_result: lock acquired\n",
      "14:44:13 DISPATCHER: job (4, 0, 8) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:44:13 job_id: (4, 0, 8)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.006780767493201843, 'loss': 'mae', 'numLayers': 30, 'numNeurons': 58, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 34.96075217542292, 'info': {'L1': 34.96075217542292, 'L2': 91.51002967453762, 'MAX': 3.925702230465376, 'TrainTime': 232.5625}}\n",
      "exception: None\n",
      "\n",
      "14:44:13 job_callback for (4, 0, 8) started\n",
      "14:44:13 DISPATCHER: Trying to submit another job.\n",
      "14:44:13 job_callback for (4, 0, 8) got condition\n",
      "14:44:13 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:44:13 done building a new model for budget 333.333333 based on 9/25 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:44:14 HBMASTER: Trying to run another job!\n",
      "14:44:14 job_callback for (4, 0, 8) finished\n",
      "14:44:14 HBMASTER: schedule new run for iteration 4\n",
      "14:44:14 HBMASTER: trying submitting job (4, 0, 24) to dispatcher\n",
      "14:44:14 HBMASTER: submitting job (4, 0, 24) to dispatcher\n",
      "14:44:14 DISPATCHER: trying to submit job (4, 0, 24)\n",
      "14:44:14 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:44:14 HBMASTER: job (4, 0, 24) submitted to dispatcher\n",
      "14:44:14 DISPATCHER: Trying to submit another job.\n",
      "14:44:14 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:44:14 DISPATCHER: starting job (4, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:44:14 DISPATCHER: job (4, 0, 24) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:44:14 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:44:14 WORKER: start processing job (4, 0, 24)\n",
      "14:44:14 WORKER: args: ()\n",
      "14:44:14 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009774186224469633, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 44, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 0.0027229266706854105.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00320: ReduceLROnPlateau reducing learning rate to 0.00023548050376120955.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:44:27 WORKER: done with job (4, 0, 16), trying to register it.\n",
      "14:44:27 WORKER: registered result for job (4, 0, 16) with dispatcher\n",
      "14:44:27 DISPATCHER: job (4, 0, 16) finished\n",
      "14:44:27 DISPATCHER: register_result: lock acquired\n",
      "14:44:27 DISPATCHER: job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:44:27 job_id: (4, 0, 16)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 6.533224707361297, 'info': {'L1': 6.533224707361297, 'L2': 7.1992026172120624, 'MAX': 2.196107155131715, 'TrainTime': 245.671875}}\n",
      "exception: None\n",
      "\n",
      "14:44:27 job_callback for (4, 0, 16) started\n",
      "14:44:27 DISPATCHER: Trying to submit another job.\n",
      "14:44:27 job_callback for (4, 0, 16) got condition\n",
      "14:44:27 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:44:27 done building a new model for budget 333.333333 based on 9/26 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:44:27 HBMASTER: Trying to run another job!\n",
      "14:44:27 job_callback for (4, 0, 16) finished\n",
      "14:44:27 HBMASTER: schedule new run for iteration 4\n",
      "14:44:27 HBMASTER: trying submitting job (4, 0, 25) to dispatcher\n",
      "14:44:27 HBMASTER: submitting job (4, 0, 25) to dispatcher\n",
      "14:44:27 DISPATCHER: trying to submit job (4, 0, 25)\n",
      "14:44:27 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:44:27 HBMASTER: job (4, 0, 25) submitted to dispatcher\n",
      "14:44:27 DISPATCHER: Trying to submit another job.\n",
      "14:44:27 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:44:27 DISPATCHER: starting job (4, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:44:27 DISPATCHER: job (4, 0, 25) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:44:27 WORKER: start processing job (4, 0, 25)\n",
      "14:44:27 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:44:27 WORKER: args: ()\n",
      "14:44:27 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0034377188349175476, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 0.0013614633353427052.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.00035388959804549813.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:44:35 DISPATCHER: Starting worker discovery\n",
      "14:44:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:44:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.004727697465568781.\n",
      "\n",
      "Epoch 00204: ReduceLROnPlateau reducing learning rate to 0.00017694479902274907.\n",
      "\n",
      "Epoch 00326: ReduceLROnPlateau reducing learning rate to 0.0006807316676713526.\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00488709332421422.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:44:49 WORKER: done with job (4, 0, 17), trying to register it.\n",
      "14:44:49 WORKER: registered result for job (4, 0, 17) with dispatcher\n",
      "14:44:49 DISPATCHER: job (4, 0, 17) finished\n",
      "14:44:49 DISPATCHER: register_result: lock acquired\n",
      "14:44:49 DISPATCHER: job (4, 0, 17) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:44:49 job_id: (4, 0, 17)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 10, 'initial_lr': 0.0054458535690848926, 'loss': 'mae', 'numLayers': 13, 'numNeurons': 65, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 6.779689746326437, 'info': {'L1': 6.779689746326437, 'L2': 7.102831460575687, 'MAX': 2.1685081997681976, 'TrainTime': 173.1875}}\n",
      "exception: None\n",
      "\n",
      "14:44:49 job_callback for (4, 0, 17) started\n",
      "14:44:49 DISPATCHER: Trying to submit another job.\n",
      "14:44:49 job_callback for (4, 0, 17) got condition\n",
      "14:44:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:44:49 done building a new model for budget 333.333333 based on 9/27 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:44:49 HBMASTER: Trying to run another job!\n",
      "14:44:49 job_callback for (4, 0, 17) finished\n",
      "14:44:49 ITERATION: Advancing config (5, 0, 2) to next budget 1000.000000\n",
      "14:44:49 ITERATION: Advancing config (5, 0, 4) to next budget 1000.000000\n",
      "14:44:49 ITERATION: Advancing config (5, 0, 7) to next budget 1000.000000\n",
      "14:44:49 HBMASTER: schedule new run for iteration 5\n",
      "14:44:49 HBMASTER: trying submitting job (5, 0, 2) to dispatcher\n",
      "14:44:49 HBMASTER: submitting job (5, 0, 2) to dispatcher\n",
      "14:44:49 DISPATCHER: trying to submit job (5, 0, 2)\n",
      "14:44:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:44:49 HBMASTER: job (5, 0, 2) submitted to dispatcher\n",
      "14:44:49 DISPATCHER: Trying to submit another job.\n",
      "14:44:49 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:44:49 DISPATCHER: starting job (5, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:44:49 DISPATCHER: job (5, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:44:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:44:49 WORKER: start processing job (5, 0, 2)\n",
      "14:44:49 WORKER: args: ()\n",
      "14:44:49 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.007258639491984703, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 22, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 0.0023638487327843904.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00240: ReduceLROnPlateau reducing learning rate to 8.847239951137453e-05.\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00244354666210711.\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 0.0011819243663921952.\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.001221773331053555.\n",
      "\n",
      "Epoch 00189: ReduceLROnPlateau reducing learning rate to 0.0017188594210892916.\n",
      "\n",
      "Epoch 00293: ReduceLROnPlateau reducing learning rate to 4.4236199755687267e-05.\n",
      "\n",
      "Epoch 00186: ReduceLROnPlateau reducing learning rate to 0.0005909621831960976.\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 0.0008594297105446458.\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.0006108866655267775.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:45:08 WORKER: done with job (4, 0, 18), trying to register it.\n",
      "14:45:08 WORKER: registered result for job (4, 0, 18) with dispatcher\n",
      "14:45:08 DISPATCHER: job (4, 0, 18) finished\n",
      "14:45:08 DISPATCHER: register_result: lock acquired\n",
      "14:45:08 DISPATCHER: job (4, 0, 18) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:45:08 job_id: (4, 0, 18)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.0007077792074795277, 'loss': 'mae', 'numLayers': 17, 'numNeurons': 75, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 7.1025046178635565, 'info': {'L1': 7.1025046178635565, 'L2': 8.426596806506977, 'MAX': 2.891516148868322, 'TrainTime': 196.671875}}\n",
      "exception: None\n",
      "\n",
      "14:45:08 job_callback for (4, 0, 18) started\n",
      "14:45:08 job_callback for (4, 0, 18) got condition\n",
      "14:45:08 DISPATCHER: Trying to submit another job.\n",
      "14:45:08 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:45:08 done building a new model for budget 333.333333 based on 9/28 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:45:08 HBMASTER: Trying to run another job!\n",
      "14:45:08 job_callback for (4, 0, 18) finished\n",
      "14:45:08 HBMASTER: schedule new run for iteration 5\n",
      "14:45:08 HBMASTER: trying submitting job (5, 0, 4) to dispatcher\n",
      "14:45:08 HBMASTER: submitting job (5, 0, 4) to dispatcher\n",
      "14:45:08 DISPATCHER: trying to submit job (5, 0, 4)\n",
      "14:45:08 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:45:08 HBMASTER: job (5, 0, 4) submitted to dispatcher\n",
      "14:45:08 DISPATCHER: Trying to submit another job.\n",
      "14:45:08 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:45:08 DISPATCHER: starting job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:45:08 DISPATCHER: job (5, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:45:08 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:45:08 WORKER: start processing job (5, 0, 4)\n",
      "14:45:08 WORKER: args: ()\n",
      "14:45:08 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00257: ReduceLROnPlateau reducing learning rate to 0.0004297148552723229.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00194: ReduceLROnPlateau reducing learning rate to 0.00030544333276338875.\n",
      "\n",
      "Epoch 00291: ReduceLROnPlateau reducing learning rate to 0.00021485742763616145.\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 0.00015272166638169438.\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 0.00010742871381808072.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:45:19 WORKER: done with job (4, 0, 25), trying to register it.\n",
      "14:45:19 WORKER: registered result for job (4, 0, 25) with dispatcher\n",
      "14:45:19 DISPATCHER: job (4, 0, 25) finished\n",
      "14:45:19 DISPATCHER: register_result: lock acquired\n",
      "14:45:19 DISPATCHER: job (4, 0, 25) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:45:19 job_id: (4, 0, 25)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0034377188349175476, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 91.56575254615923, 'info': {'L1': 91.56575254615923, 'L2': 429.80586584790467, 'MAX': 4.9982613298343495, 'TrainTime': 154.3125}}\n",
      "exception: None\n",
      "\n",
      "14:45:19 job_callback for (4, 0, 25) started\n",
      "14:45:19 DISPATCHER: Trying to submit another job.\n",
      "14:45:19 job_callback for (4, 0, 25) got condition\n",
      "14:45:19 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:45:19 done building a new model for budget 333.333333 based on 9/28 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:45:19 HBMASTER: Trying to run another job!\n",
      "14:45:19 job_callback for (4, 0, 25) finished\n",
      "14:45:19 HBMASTER: schedule new run for iteration 5\n",
      "14:45:19 HBMASTER: trying submitting job (5, 0, 7) to dispatcher\n",
      "14:45:19 HBMASTER: submitting job (5, 0, 7) to dispatcher\n",
      "14:45:19 DISPATCHER: trying to submit job (5, 0, 7)\n",
      "14:45:19 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:45:19 HBMASTER: job (5, 0, 7) submitted to dispatcher\n",
      "14:45:19 DISPATCHER: Trying to submit another job.\n",
      "14:45:19 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:45:19 DISPATCHER: starting job (5, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:45:19 DISPATCHER: job (5, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:45:19 WORKER: start processing job (5, 0, 7)\n",
      "14:45:19 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:45:19 WORKER: args: ()\n",
      "14:45:19 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0010714680121203894, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 46, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 7.636083319084719e-05.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00296: ReduceLROnPlateau reducing learning rate to 3.8180416595423594e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:45:28 WORKER: done with job (4, 0, 21), trying to register it.\n",
      "14:45:28 WORKER: registered result for job (4, 0, 21) with dispatcher\n",
      "14:45:29 DISPATCHER: job (4, 0, 21) finished\n",
      "14:45:29 DISPATCHER: register_result: lock acquired\n",
      "14:45:29 DISPATCHER: job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:45:29 job_id: (4, 0, 21)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 5.960034509810539, 'info': {'L1': 5.960034509810539, 'L2': 3.1493378111760864, 'MAX': 1.2338475373086681, 'TrainTime': 239.421875}}\n",
      "exception: None\n",
      "\n",
      "14:45:29 job_callback for (4, 0, 21) started\n",
      "14:45:29 DISPATCHER: Trying to submit another job.\n",
      "14:45:29 job_callback for (4, 0, 21) got condition\n",
      "14:45:29 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:45:29 done building a new model for budget 333.333333 based on 9/29 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:45:29 HBMASTER: Trying to run another job!\n",
      "14:45:29 job_callback for (4, 0, 21) finished\n",
      "14:45:29 start sampling a new configuration.\n",
      "14:45:29 best_vector: [5, 1, 0.29358774139632904, 0.015217977210075118, 1, 0.01242439388324841, 0.008749542905798935, 1], 0.017346418416825146, 0.001850994256962614, 3.210812086841387e-05\n",
      "14:45:29 done sampling a new configuration.\n",
      "14:45:29 HBMASTER: schedule new run for iteration 6\n",
      "14:45:29 HBMASTER: trying submitting job (6, 0, 3) to dispatcher\n",
      "14:45:29 HBMASTER: submitting job (6, 0, 3) to dispatcher\n",
      "14:45:29 DISPATCHER: trying to submit job (6, 0, 3)\n",
      "14:45:29 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:45:29 HBMASTER: job (6, 0, 3) submitted to dispatcher\n",
      "14:45:29 DISPATCHER: Trying to submit another job.\n",
      "14:45:29 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:45:29 DISPATCHER: starting job (6, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:45:29 DISPATCHER: job (6, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:45:29 WORKER: start processing job (6, 0, 3)\n",
      "14:45:29 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:45:29 WORKER: args: ()\n",
      "14:45:29 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.00025065797437974366, 'loss': 'mse', 'numLayers': 2, 'numNeurons': 10, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00330: ReduceLROnPlateau reducing learning rate to 1.9090208297711797e-05.\n",
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:45:32 WORKER: done with job (4, 0, 24), trying to register it.\n",
      "14:45:32 WORKER: registered result for job (4, 0, 24) with dispatcher\n",
      "14:45:32 DISPATCHER: job (4, 0, 24) finished\n",
      "14:45:32 DISPATCHER: register_result: lock acquired\n",
      "14:45:32 DISPATCHER: job (4, 0, 24) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:45:32 job_id: (4, 0, 24)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 5000, 'denspt': 11, 'initial_lr': 0.009774186224469633, 'loss': 'mse', 'numLayers': 28, 'numNeurons': 44, 'optimizer': 'Nadam'}, 'budget': 333.3333333333333, 'working_directory': '.'}\n",
      "result: {'loss': 18.262463942917478, 'info': {'L1': 18.262463942917478, 'L2': 19.076623923592003, 'MAX': 3.800230781924066, 'TrainTime': 212.921875}}\n",
      "exception: None\n",
      "\n",
      "14:45:32 job_callback for (4, 0, 24) started\n",
      "14:45:32 DISPATCHER: Trying to submit another job.\n",
      "14:45:32 job_callback for (4, 0, 24) got condition\n",
      "14:45:32 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:45:32 done building a new model for budget 333.333333 based on 9/30 split\n",
      "Best loss for this budget:0.189743\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:45:32 HBMASTER: Trying to run another job!\n",
      "14:45:32 job_callback for (4, 0, 24) finished\n",
      "14:45:32 ITERATION: Advancing config (4, 0, 7) to next budget 1000.000000\n",
      "14:45:32 ITERATION: Advancing config (4, 0, 16) to next budget 1000.000000\n",
      "14:45:32 ITERATION: Advancing config (4, 0, 21) to next budget 1000.000000\n",
      "14:45:32 HBMASTER: schedule new run for iteration 4\n",
      "14:45:32 HBMASTER: trying submitting job (4, 0, 7) to dispatcher\n",
      "14:45:32 HBMASTER: submitting job (4, 0, 7) to dispatcher\n",
      "14:45:32 DISPATCHER: trying to submit job (4, 0, 7)\n",
      "14:45:32 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:45:32 HBMASTER: job (4, 0, 7) submitted to dispatcher\n",
      "14:45:32 DISPATCHER: Trying to submit another job.\n",
      "14:45:32 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:45:32 DISPATCHER: starting job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:45:32 DISPATCHER: job (4, 0, 7) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:45:32 WORKER: start processing job (4, 0, 7)\n",
      "14:45:32 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:45:32 WORKER: args: ()\n",
      "14:45:32 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "14:45:35 DISPATCHER: Starting worker discovery\n",
      "14:45:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:45:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00280: ReduceLROnPlateau reducing learning rate to 0.00020023132674396038.\n",
      "\n",
      "Epoch 00536: ReduceLROnPlateau reducing learning rate to 0.004068493843078613.\n",
      "\n",
      "Epoch 00500: ReduceLROnPlateau reducing learning rate to 0.0005357339978218079.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:46:35 DISPATCHER: Starting worker discovery\n",
      "14:46:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:46:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00612: ReduceLROnPlateau reducing learning rate to 0.00026786699891090393.\n",
      "\n",
      "Epoch 00485: ReduceLROnPlateau reducing learning rate to 0.00010011566337198019.\n",
      "\n",
      "Epoch 00720: ReduceLROnPlateau reducing learning rate to 0.00013393349945545197.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:46:56 WORKER: done with job (5, 0, 2), trying to register it.\n",
      "14:46:56 WORKER: registered result for job (5, 0, 2) with dispatcher\n",
      "14:46:56 DISPATCHER: job (5, 0, 2) finished\n",
      "14:46:56 DISPATCHER: register_result: lock acquired\n",
      "14:46:56 DISPATCHER: job (5, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:46:56 job_id: (5, 0, 2)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.007258639491984703, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 22, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 3.4743106930783583, 'info': {'L1': 3.4743106930783583, 'L2': 1.2546000593701017, 'MAX': 0.665241186227775, 'TrainTime': 311.828125}}\n",
      "exception: None\n",
      "\n",
      "14:46:56 job_callback for (5, 0, 2) started\n",
      "14:46:56 DISPATCHER: Trying to submit another job.\n",
      "14:46:56 job_callback for (5, 0, 2) got condition\n",
      "14:46:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:46:56 HBMASTER: Trying to run another job!\n",
      "14:46:56 job_callback for (5, 0, 2) finished\n",
      "14:46:56 HBMASTER: schedule new run for iteration 4\n",
      "14:46:56 HBMASTER: trying submitting job (4, 0, 16) to dispatcher\n",
      "14:46:56 HBMASTER: submitting job (4, 0, 16) to dispatcher\n",
      "14:46:56 DISPATCHER: trying to submit job (4, 0, 16)\n",
      "14:46:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:46:56 HBMASTER: job (4, 0, 16) submitted to dispatcher\n",
      "14:46:56 DISPATCHER: Trying to submit another job.\n",
      "14:46:56 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:46:56 DISPATCHER: starting job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:46:56 DISPATCHER: job (4, 0, 16) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:46:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:46:56 WORKER: start processing job (4, 0, 16)\n",
      "14:46:56 WORKER: args: ()\n",
      "14:46:56 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2880 \n",
      "Batch size: 2880 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00703: ReduceLROnPlateau reducing learning rate to 5.0057831685990095e-05.\n",
      "\n",
      "Epoch 00996: ReduceLROnPlateau reducing learning rate to 0.0020342469215393066.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:47:11 WORKER: done with job (5, 0, 4), trying to register it.\n",
      "14:47:11 WORKER: registered result for job (5, 0, 4) with dispatcher\n",
      "14:47:11 DISPATCHER: job (5, 0, 4) finished\n",
      "14:47:11 DISPATCHER: register_result: lock acquired\n",
      "14:47:11 DISPATCHER: job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:47:11 job_id: (5, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.6484396094324323, 'info': {'L1': 0.6484396094324323, 'L2': 0.02375493216985689, 'MAX': 0.08365156073504387, 'TrainTime': 291.921875}}\n",
      "exception: None\n",
      "\n",
      "14:47:11 job_callback for (5, 0, 4) started\n",
      "14:47:11 DISPATCHER: Trying to submit another job.\n",
      "14:47:11 job_callback for (5, 0, 4) got condition\n",
      "14:47:11 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:47:11 HBMASTER: Trying to run another job!\n",
      "14:47:11 job_callback for (5, 0, 4) finished\n",
      "14:47:11 HBMASTER: schedule new run for iteration 4\n",
      "14:47:11 HBMASTER: trying submitting job (4, 0, 21) to dispatcher\n",
      "14:47:11 HBMASTER: submitting job (4, 0, 21) to dispatcher\n",
      "14:47:11 DISPATCHER: trying to submit job (4, 0, 21)\n",
      "14:47:11 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:47:11 HBMASTER: job (4, 0, 21) submitted to dispatcher\n",
      "14:47:11 DISPATCHER: Trying to submit another job.\n",
      "14:47:11 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:47:11 DISPATCHER: starting job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:47:11 DISPATCHER: job (4, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:47:11 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:47:11 WORKER: start processing job (4, 0, 21)\n",
      "14:47:11 WORKER: args: ()\n",
      "14:47:11 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00912: ReduceLROnPlateau reducing learning rate to 6.696674972772598e-05.\n",
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00814: ReduceLROnPlateau reducing learning rate to 2.5028915842995048e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:47:24 WORKER: done with job (5, 0, 7), trying to register it.\n",
      "14:47:24 WORKER: registered result for job (5, 0, 7) with dispatcher\n",
      "14:47:24 DISPATCHER: job (5, 0, 7) finished\n",
      "14:47:24 DISPATCHER: register_result: lock acquired\n",
      "14:47:24 DISPATCHER: job (5, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:47:24 job_id: (5, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0010714680121203894, 'loss': 'mse', 'numLayers': 10, 'numNeurons': 46, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 4.069511378207809, 'info': {'L1': 4.069511378207809, 'L2': 2.0172051926960606, 'MAX': 0.9914403776101506, 'TrainTime': 300.03125}}\n",
      "exception: None\n",
      "\n",
      "14:47:24 job_callback for (5, 0, 7) started\n",
      "14:47:24 DISPATCHER: Trying to submit another job.\n",
      "14:47:24 job_callback for (5, 0, 7) got condition\n",
      "14:47:24 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:47:24 done building a new model for budget 1000.000000 based on 9/15 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:47:24 HBMASTER: Trying to run another job!\n",
      "14:47:24 job_callback for (5, 0, 7) finished\n",
      "14:47:24 ITERATION: Advancing config (5, 0, 4) to next budget 3000.000000\n",
      "14:47:24 HBMASTER: schedule new run for iteration 5\n",
      "14:47:24 HBMASTER: trying submitting job (5, 0, 4) to dispatcher\n",
      "14:47:25 HBMASTER: submitting job (5, 0, 4) to dispatcher\n",
      "14:47:25 DISPATCHER: trying to submit job (5, 0, 4)\n",
      "14:47:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:47:25 HBMASTER: job (5, 0, 4) submitted to dispatcher\n",
      "14:47:25 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:47:25 DISPATCHER: Trying to submit another job.\n",
      "14:47:25 DISPATCHER: starting job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:47:25 DISPATCHER: job (5, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:47:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:47:25 WORKER: start processing job (5, 0, 4)\n",
      "14:47:25 WORKER: args: ()\n",
      "14:47:25 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00952: ReduceLROnPlateau reducing learning rate to 1.2514457921497524e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:47:35 DISPATCHER: Starting worker discovery\n",
      "14:47:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:47:35 DISPATCHER: Finished worker discovery\n",
      "14:47:36 WORKER: done with job (6, 0, 3), trying to register it.\n",
      "14:47:36 WORKER: registered result for job (6, 0, 3) with dispatcher\n",
      "14:47:36 DISPATCHER: job (6, 0, 3) finished\n",
      "14:47:36 DISPATCHER: register_result: lock acquired\n",
      "14:47:36 DISPATCHER: job (6, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:47:36 job_id: (6, 0, 3)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 7, 'initial_lr': 0.00025065797437974366, 'loss': 'mse', 'numLayers': 2, 'numNeurons': 10, 'optimizer': 'RMSprop'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 15.415459193066727, 'info': {'L1': 15.415459193066727, 'L2': 15.822755256603818, 'MAX': 1.6276637350672818, 'TrainTime': 315.078125}}\n",
      "exception: None\n",
      "\n",
      "14:47:36 job_callback for (6, 0, 3) started\n",
      "14:47:36 DISPATCHER: Trying to submit another job.\n",
      "14:47:36 job_callback for (6, 0, 3) got condition\n",
      "14:47:36 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:47:36 done building a new model for budget 1000.000000 based on 9/16 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:47:36 HBMASTER: Trying to run another job!\n",
      "14:47:36 job_callback for (6, 0, 3) finished\n",
      "14:47:36 start sampling a new configuration.\n",
      "14:47:36 done sampling a new configuration.\n",
      "14:47:36 HBMASTER: schedule new run for iteration 6\n",
      "14:47:36 HBMASTER: trying submitting job (6, 0, 4) to dispatcher\n",
      "14:47:36 HBMASTER: submitting job (6, 0, 4) to dispatcher\n",
      "14:47:36 DISPATCHER: trying to submit job (6, 0, 4)\n",
      "14:47:36 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:47:36 HBMASTER: job (6, 0, 4) submitted to dispatcher\n",
      "14:47:36 DISPATCHER: Trying to submit another job.\n",
      "14:47:36 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:47:36 DISPATCHER: starting job (6, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:47:36 DISPATCHER: job (6, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:47:36 WORKER: start processing job (6, 0, 4)\n",
      "14:47:36 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:47:36 WORKER: args: ()\n",
      "14:47:36 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0013312913939173394, 'loss': 'mse', 'numLayers': 15, 'numNeurons': 11, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.0018838440300896764.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:47:43 WORKER: done with job (4, 0, 7), trying to register it.\n",
      "14:47:43 WORKER: registered result for job (4, 0, 7) with dispatcher\n",
      "14:47:43 DISPATCHER: job (4, 0, 7) finished\n",
      "14:47:43 DISPATCHER: register_result: lock acquired\n",
      "14:47:43 DISPATCHER: job (4, 0, 7) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:47:43 job_id: (4, 0, 7)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 10000, 'denspt': 10, 'initial_lr': 0.0004004626544919465, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 96, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 5.142864091160016, 'info': {'L1': 5.142864091160016, 'L2': 3.620748773314124, 'MAX': 1.2944442932985476, 'TrainTime': 327.484375}}\n",
      "exception: None\n",
      "\n",
      "14:47:43 job_callback for (4, 0, 7) started\n",
      "14:47:43 DISPATCHER: Trying to submit another job.\n",
      "14:47:43 job_callback for (4, 0, 7) got condition\n",
      "14:47:43 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:47:43 done building a new model for budget 1000.000000 based on 9/17 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:47:43 HBMASTER: Trying to run another job!\n",
      "14:47:43 job_callback for (4, 0, 7) finished\n",
      "14:47:43 start sampling a new configuration.\n",
      "14:47:43 best_vector: [4, 2, 0.6357194202463239, 0.55849667023785, 1, 0.6773351252738221, 0.013836458836145149, 4], 6.601907612207803e-31, 0.015147137141859838, -0.0009638469510020047\n",
      "14:47:43 done sampling a new configuration.\n",
      "14:47:43 HBMASTER: schedule new run for iteration 6\n",
      "14:47:43 HBMASTER: trying submitting job (6, 0, 5) to dispatcher\n",
      "14:47:43 HBMASTER: submitting job (6, 0, 5) to dispatcher\n",
      "14:47:43 DISPATCHER: trying to submit job (6, 0, 5)\n",
      "14:47:43 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:47:43 HBMASTER: job (6, 0, 5) submitted to dispatcher\n",
      "14:47:43 DISPATCHER: Trying to submit another job.\n",
      "14:47:43 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:47:43 DISPATCHER: starting job (6, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:47:43 DISPATCHER: job (6, 0, 5) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:47:43 WORKER: start processing job (6, 0, 5)\n",
      "14:47:43 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:47:43 WORKER: args: ()\n",
      "14:47:43 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.005629117035354716, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 11, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Total samples: 2000 \n",
      "Batch size: 2000 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00404: ReduceLROnPlateau reducing learning rate to 0.0009419220150448382.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:48:35 DISPATCHER: Starting worker discovery\n",
      "14:48:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:48:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00544: ReduceLROnPlateau reducing learning rate to 0.0004709610075224191.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:49:35 DISPATCHER: Starting worker discovery\n",
      "14:49:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:49:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 0.002814558567479253.\n",
      "\n",
      "Epoch 00491: ReduceLROnPlateau reducing learning rate to 0.0014072792837396264.\n",
      "\n",
      "Epoch 00983: ReduceLROnPlateau reducing learning rate to 0.00023548050376120955.\n",
      "\n",
      "Epoch 00591: ReduceLROnPlateau reducing learning rate to 0.0007036396418698132.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:50:23 WORKER: done with job (4, 0, 16), trying to register it.\n",
      "14:50:23 WORKER: registered result for job (4, 0, 16) with dispatcher\n",
      "14:50:23 DISPATCHER: job (4, 0, 16) finished\n",
      "14:50:23 DISPATCHER: register_result: lock acquired\n",
      "14:50:23 DISPATCHER: job (4, 0, 16) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:50:23 job_id: (4, 0, 16)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 25000, 'denspt': 12, 'initial_lr': 0.003767688096153248, 'loss': 'mae', 'numLayers': 28, 'numNeurons': 69, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 7.478126068600353, 'info': {'L1': 7.478126068600353, 'L2': 8.45829617291032, 'MAX': 2.5287893632821477, 'TrainTime': 600.578125}}\n",
      "exception: None\n",
      "\n",
      "14:50:23 job_callback for (4, 0, 16) started\n",
      "14:50:23 DISPATCHER: Trying to submit another job.\n",
      "14:50:23 job_callback for (4, 0, 16) got condition\n",
      "14:50:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:50:23 done building a new model for budget 1000.000000 based on 9/17 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:50:23 HBMASTER: Trying to run another job!\n",
      "14:50:23 job_callback for (4, 0, 16) finished\n",
      "14:50:23 start sampling a new configuration.\n",
      "14:50:23 best_vector: [1, 3, 0.8697574111552275, 0.5253278654907084, 0, 0.25861258426539113, 0.9782128360468206, 3], 5.611086164996533e-30, 0.0017821861411401404, -0.001729194167797846\n",
      "14:50:23 done sampling a new configuration.\n",
      "14:50:23 HBMASTER: schedule new run for iteration 7\n",
      "14:50:23 HBMASTER: trying submitting job (7, 0, 0) to dispatcher\n",
      "14:50:23 HBMASTER: submitting job (7, 0, 0) to dispatcher\n",
      "14:50:23 DISPATCHER: trying to submit job (7, 0, 0)\n",
      "14:50:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:50:23 HBMASTER: job (7, 0, 0) submitted to dispatcher\n",
      "14:50:23 DISPATCHER: Trying to submit another job.\n",
      "14:50:23 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:50:23 DISPATCHER: starting job (7, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:50:23 DISPATCHER: job (7, 0, 0) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:50:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:50:23 WORKER: start processing job (7, 0, 0)\n",
      "14:50:23 WORKER: args: ()\n",
      "14:50:23 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.005300745868358014, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 99, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:50:35 DISPATCHER: Starting worker discovery\n",
      "14:50:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:50:35 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00691: ReduceLROnPlateau reducing learning rate to 0.0003518198209349066.\n",
      "\n",
      "Epoch 00791: ReduceLROnPlateau reducing learning rate to 0.0001759099104674533.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:50:57 WORKER: done with job (4, 0, 21), trying to register it.\n",
      "14:50:57 WORKER: registered result for job (4, 0, 21) with dispatcher\n",
      "14:50:57 DISPATCHER: job (4, 0, 21) finished\n",
      "14:50:57 DISPATCHER: register_result: lock acquired\n",
      "14:50:57 DISPATCHER: job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:50:57 job_id: (4, 0, 21)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 3.1988019926344258, 'info': {'L1': 3.1988019926344258, 'L2': 0.9037974003988275, 'MAX': 0.46706901832734804, 'TrainTime': 654.109375}}\n",
      "exception: None\n",
      "\n",
      "14:50:57 job_callback for (4, 0, 21) started\n",
      "14:50:57 DISPATCHER: Trying to submit another job.\n",
      "14:50:57 job_callback for (4, 0, 21) got condition\n",
      "14:50:57 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:50:57 done building a new model for budget 1000.000000 based on 9/18 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:50:57 HBMASTER: Trying to run another job!\n",
      "14:50:57 job_callback for (4, 0, 21) finished\n",
      "14:50:57 ITERATION: Advancing config (4, 0, 21) to next budget 3000.000000\n",
      "14:50:57 HBMASTER: schedule new run for iteration 4\n",
      "14:50:57 HBMASTER: trying submitting job (4, 0, 21) to dispatcher\n",
      "14:50:57 HBMASTER: submitting job (4, 0, 21) to dispatcher\n",
      "14:50:57 DISPATCHER: trying to submit job (4, 0, 21)\n",
      "14:50:57 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:50:57 HBMASTER: job (4, 0, 21) submitted to dispatcher\n",
      "14:50:57 DISPATCHER: Trying to submit another job.\n",
      "14:50:57 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:50:57 DISPATCHER: starting job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:50:57 DISPATCHER: job (4, 0, 21) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496\n",
      "14:50:57 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:50:57 WORKER: start processing job (4, 0, 21)\n",
      "14:50:57 WORKER: args: ()\n",
      "14:50:57 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 00891: ReduceLROnPlateau reducing learning rate to 8.795495523372665e-05.\n",
      "\n",
      "Epoch 00991: ReduceLROnPlateau reducing learning rate to 4.3977477616863325e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:51:26 WORKER: done with job (6, 0, 5), trying to register it.\n",
      "14:51:26 WORKER: registered result for job (6, 0, 5) with dispatcher\n",
      "14:51:26 DISPATCHER: job (6, 0, 5) finished\n",
      "14:51:26 DISPATCHER: register_result: lock acquired\n",
      "14:51:26 DISPATCHER: job (6, 0, 5) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:51:26 job_id: (6, 0, 5)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 15000, 'denspt': 10, 'initial_lr': 0.005629117035354716, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 11, 'optimizer': 'Ftrl'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 91.559876657413, 'info': {'L1': 91.559876657413, 'L2': 429.75206455671554, 'MAX': 4.997967535397038, 'TrainTime': 634.625}}\n",
      "exception: None\n",
      "\n",
      "14:51:26 job_callback for (6, 0, 5) started\n",
      "14:51:26 DISPATCHER: Trying to submit another job.\n",
      "14:51:26 job_callback for (6, 0, 5) got condition\n",
      "14:51:26 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:51:26 done building a new model for budget 1000.000000 based on 9/19 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:51:26 HBMASTER: Trying to run another job!\n",
      "14:51:26 job_callback for (6, 0, 5) finished\n",
      "14:51:26 start sampling a new configuration.\n",
      "14:51:26 done sampling a new configuration.\n",
      "14:51:26 HBMASTER: schedule new run for iteration 7\n",
      "14:51:26 HBMASTER: trying submitting job (7, 0, 1) to dispatcher\n",
      "14:51:26 HBMASTER: submitting job (7, 0, 1) to dispatcher\n",
      "14:51:26 DISPATCHER: trying to submit job (7, 0, 1)\n",
      "14:51:26 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:51:26 HBMASTER: job (7, 0, 1) submitted to dispatcher\n",
      "14:51:26 DISPATCHER: Trying to submit another job.\n",
      "14:51:26 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:51:26 DISPATCHER: starting job (7, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:51:26 DISPATCHER: job (7, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:51:26 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:51:26 WORKER: start processing job (7, 0, 1)\n",
      "14:51:26 WORKER: args: ()\n",
      "14:51:26 WORKER: kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.0019639747899361033, 'loss': 'mae', 'numLayers': 10, 'numNeurons': 48, 'optimizer': 'SGD'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 980 \n",
      "Batch size: 980 \n",
      "Total batches: 1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:51:35 DISPATCHER: Starting worker discovery\n",
      "14:51:35 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:51:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01901: ReduceLROnPlateau reducing learning rate to 0.004068493843078613.\n",
      "\n",
      "Epoch 00738: ReduceLROnPlateau reducing learning rate to 0.0026503729168325663.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:52:36 DISPATCHER: Starting worker discovery\n",
      "14:52:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:52:36 DISPATCHER: Finished worker discovery\n",
      "14:53:17 WORKER: done with job (6, 0, 4), trying to register it.\n",
      "14:53:17 WORKER: registered result for job (6, 0, 4) with dispatcher\n",
      "14:53:17 DISPATCHER: job (6, 0, 4) finished\n",
      "14:53:17 DISPATCHER: register_result: lock acquired\n",
      "14:53:17 DISPATCHER: job (6, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "14:53:17 job_id: (6, 0, 4)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0013312913939173394, 'loss': 'mse', 'numLayers': 15, 'numNeurons': 11, 'optimizer': 'Nadam'}, 'budget': 1000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.7658181279004004, 'info': {'L1': 0.7658181279004004, 'L2': 0.03594281251539929, 'MAX': 0.3279386566989988, 'TrainTime': 951.34375}}\n",
      "exception: None\n",
      "\n",
      "14:53:17 job_callback for (6, 0, 4) started\n",
      "14:53:17 DISPATCHER: Trying to submit another job.\n",
      "14:53:17 job_callback for (6, 0, 4) got condition\n",
      "14:53:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:53:17 done building a new model for budget 1000.000000 based on 9/20 split\n",
      "Best loss for this budget:0.175687\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14:53:17 HBMASTER: Trying to run another job!\n",
      "14:53:17 job_callback for (6, 0, 4) finished\n",
      "14:53:17 ITERATION: Advancing config (6, 0, 1) to next budget 3000.000000\n",
      "14:53:17 ITERATION: Advancing config (6, 0, 4) to next budget 3000.000000\n",
      "14:53:17 HBMASTER: schedule new run for iteration 6\n",
      "14:53:17 HBMASTER: trying submitting job (6, 0, 1) to dispatcher\n",
      "14:53:17 HBMASTER: submitting job (6, 0, 1) to dispatcher\n",
      "14:53:17 DISPATCHER: trying to submit job (6, 0, 1)\n",
      "14:53:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:53:17 HBMASTER: job (6, 0, 1) submitted to dispatcher\n",
      "14:53:17 DISPATCHER: Trying to submit another job.\n",
      "14:53:17 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:53:17 DISPATCHER: starting job (6, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:53:17 DISPATCHER: job (6, 0, 1) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496\n",
      "14:53:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:53:17 WORKER: start processing job (6, 0, 1)\n",
      "14:53:17 WORKER: args: ()\n",
      "14:53:17 WORKER: kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0029909609567580946, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 52, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02587: ReduceLROnPlateau reducing learning rate to 0.0020342469215393066.\n",
      "\n",
      "Epoch 01387: ReduceLROnPlateau reducing learning rate to 0.0013251864584162831.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:53:36 DISPATCHER: Starting worker discovery\n",
      "14:53:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:53:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01687: ReduceLROnPlateau reducing learning rate to 0.0006625932292081416.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:10 WORKER: done with job (5, 0, 4), trying to register it.\n",
      "14:54:10 WORKER: registered result for job (5, 0, 4) with dispatcher\n",
      "14:54:10 DISPATCHER: job (5, 0, 4) finished\n",
      "14:54:10 DISPATCHER: register_result: lock acquired\n",
      "14:54:10 DISPATCHER: job (5, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "14:54:10 job_id: (5, 0, 4)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.06484265556770885, 'info': {'L1': 0.06484265556770885, 'L2': 0.000537993449077609, 'MAX': 0.07442877737747455, 'TrainTime': 1184.0625}}\n",
      "exception: None\n",
      "\n",
      "14:54:10 job_callback for (5, 0, 4) started\n",
      "14:54:10 DISPATCHER: Trying to submit another job.\n",
      "14:54:10 job_callback for (5, 0, 4) got condition\n",
      "14:54:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:54:10 HBMASTER: Trying to run another job!\n",
      "14:54:10 job_callback for (5, 0, 4) finished\n",
      "14:54:10 HBMASTER: schedule new run for iteration 6\n",
      "14:54:10 HBMASTER: trying submitting job (6, 0, 4) to dispatcher\n",
      "14:54:10 HBMASTER: submitting job (6, 0, 4) to dispatcher\n",
      "14:54:10 DISPATCHER: trying to submit job (6, 0, 4)\n",
      "14:54:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:54:10 HBMASTER: job (6, 0, 4) submitted to dispatcher\n",
      "14:54:10 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:54:10 DISPATCHER: Trying to submit another job.\n",
      "14:54:10 DISPATCHER: starting job (6, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:54:10 DISPATCHER: job (6, 0, 4) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496\n",
      "14:54:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:54:10 WORKER: start processing job (6, 0, 4)\n",
      "14:54:10 WORKER: args: ()\n",
      "14:54:10 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0013312913939173394, 'loss': 'mse', 'numLayers': 15, 'numNeurons': 11, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 2420 \n",
      "Batch size: 2420 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 01492: ReduceLROnPlateau reducing learning rate to 0.0009819873375818133.\n",
      "\n",
      "Epoch 01987: ReduceLROnPlateau reducing learning rate to 0.0003312966146040708.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:54:36 DISPATCHER: Starting worker discovery\n",
      "14:54:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:54:36 DISPATCHER: Finished worker discovery\n",
      "14:55:36 DISPATCHER: Starting worker discovery\n",
      "14:55:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:55:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01756: ReduceLROnPlateau reducing learning rate to 0.004727697465568781.\n",
      "\n",
      "Epoch 02148: ReduceLROnPlateau reducing learning rate to 0.0004909936687909067.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:56:36 DISPATCHER: Starting worker discovery\n",
      "14:56:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:56:36 DISPATCHER: Finished worker discovery\n",
      "14:56:40 WORKER: done with job (7, 0, 0), trying to register it.\n",
      "14:56:40 WORKER: registered result for job (7, 0, 0) with dispatcher\n",
      "14:56:40 DISPATCHER: job (7, 0, 0) finished\n",
      "14:56:40 DISPATCHER: register_result: lock acquired\n",
      "14:56:40 DISPATCHER: job (7, 0, 0) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "14:56:40 job_id: (7, 0, 0)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 25000, 'denspt': 11, 'initial_lr': 0.005300745868358014, 'loss': 'mae', 'numLayers': 14, 'numNeurons': 99, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 40.50887564670343, 'info': {'L1': 40.50887564670343, 'L2': 125.28315024428278, 'MAX': 4.6593337543018665, 'TrainTime': 1092.703125}}\n",
      "exception: None\n",
      "\n",
      "14:56:40 job_callback for (7, 0, 0) started\n",
      "14:56:40 DISPATCHER: Trying to submit another job.\n",
      "14:56:40 job_callback for (7, 0, 0) got condition\n",
      "14:56:40 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:56:40 HBMASTER: Trying to run another job!\n",
      "14:56:40 job_callback for (7, 0, 0) finished\n",
      "14:56:40 start sampling a new configuration.\n",
      "14:56:40 best_vector: [5, 2, 0.44924450261565346, 0.6230723315210865, 0, 0.4513262276104339, 0.601825507802185, 0], 2.3535447174659488e-31, 0.042489101336332186, -0.0022236406191553293\n",
      "14:56:40 done sampling a new configuration.\n",
      "14:56:40 HBMASTER: schedule new run for iteration 7\n",
      "14:56:40 HBMASTER: trying submitting job (7, 0, 2) to dispatcher\n",
      "14:56:40 HBMASTER: submitting job (7, 0, 2) to dispatcher\n",
      "14:56:40 DISPATCHER: trying to submit job (7, 0, 2)\n",
      "14:56:40 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:56:40 HBMASTER: job (7, 0, 2) submitted to dispatcher\n",
      "14:56:40 DISPATCHER: Trying to submit another job.\n",
      "14:56:40 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:56:40 DISPATCHER: starting job (7, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:56:40 DISPATCHER: job (7, 0, 2) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496\n",
      "14:56:40 WORKER: start processing job (7, 0, 2)\n",
      "14:56:40 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:56:40 WORKER: args: ()\n",
      "14:56:40 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.006268416082058757, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 64, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 1280 \n",
      "Batch size: 1280 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 01702: ReduceLROnPlateau reducing learning rate to 0.001495480420999229.\n",
      "\n",
      "Epoch 02264: ReduceLROnPlateau reducing learning rate to 0.0023638487327843904.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:36 DISPATCHER: Starting worker discovery\n",
      "14:57:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:57:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02002: ReduceLROnPlateau reducing learning rate to 0.0007477402104996145.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:43 WORKER: done with job (7, 0, 1), trying to register it.\n",
      "14:57:43 WORKER: registered result for job (7, 0, 1) with dispatcher\n",
      "14:57:43 DISPATCHER: job (7, 0, 1) finished\n",
      "14:57:43 DISPATCHER: register_result: lock acquired\n",
      "14:57:43 DISPATCHER: job (7, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "14:57:43 job_id: (7, 0, 1)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 5000, 'denspt': 7, 'initial_lr': 0.0019639747899361033, 'loss': 'mae', 'numLayers': 10, 'numNeurons': 48, 'optimizer': 'SGD'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.997369491873037, 'info': {'L1': 0.997369491873037, 'L2': 0.06084863025285713, 'MAX': 0.18006148506337105, 'TrainTime': 1108.140625}}\n",
      "exception: None\n",
      "\n",
      "14:57:43 job_callback for (7, 0, 1) started\n",
      "14:57:43 job_callback for (7, 0, 1) got condition\n",
      "14:57:43 DISPATCHER: Trying to submit another job.\n",
      "14:57:43 HBMASTER: Trying to run another job!\n",
      "14:57:43 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:57:43 job_callback for (7, 0, 1) finished\n",
      "14:57:43 start sampling a new configuration.\n",
      "14:57:43 best_vector: [5, 1, 0.22935785586303592, 0.7821065562668292, 0, 0.40871217496473866, 0.3359330047585582, 0], 6.549306026463479e-31, 0.015268793303585847, -0.004195595114720092\n",
      "14:57:43 done sampling a new configuration.\n",
      "14:57:43 HBMASTER: schedule new run for iteration 7\n",
      "14:57:43 HBMASTER: trying submitting job (7, 0, 3) to dispatcher\n",
      "14:57:43 HBMASTER: submitting job (7, 0, 3) to dispatcher\n",
      "14:57:43 DISPATCHER: trying to submit job (7, 0, 3)\n",
      "14:57:43 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:57:43 HBMASTER: job (7, 0, 3) submitted to dispatcher\n",
      "14:57:43 DISPATCHER: Trying to submit another job.\n",
      "14:57:43 HBMASTER: running jobs: 5, queue sizes: (4, 5) -> wait\n",
      "14:57:43 DISPATCHER: starting job (7, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:57:43 DISPATCHER: job (7, 0, 3) dispatched on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496\n",
      "14:57:43 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:57:43 WORKER: start processing job (7, 0, 3)\n",
      "14:57:43 WORKER: args: ()\n",
      "14:57:43 WORKER: kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.00784285490704161, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 40, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02564: ReduceLROnPlateau reducing learning rate to 0.0011819243663921952.\n",
      "\n",
      "Total samples: 720 \n",
      "Batch size: 720 \n",
      "Total batches: 1 \n",
      "\n",
      "\n",
      "Epoch 02302: ReduceLROnPlateau reducing learning rate to 0.00037387010524980724.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:58:36 DISPATCHER: Starting worker discovery\n",
      "14:58:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:58:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02864: ReduceLROnPlateau reducing learning rate to 0.0005909621831960976.\n",
      "\n",
      "Epoch 02602: ReduceLROnPlateau reducing learning rate to 0.00018693505262490362.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:59:23 WORKER: done with job (4, 0, 21), trying to register it.\n",
      "14:59:23 WORKER: registered result for job (4, 0, 21) with dispatcher\n",
      "14:59:23 DISPATCHER: job (4, 0, 21) finished\n",
      "14:59:23 DISPATCHER: register_result: lock acquired\n",
      "14:59:23 DISPATCHER: job (4, 0, 21) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.12496 finished\n",
      "14:59:23 job_id: (4, 0, 21)\n",
      "kwargs: {'config': {'activator': 'elu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.00945539491540849, 'loss': 'mse', 'numLayers': 40, 'numNeurons': 31, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 3.893597322893543, 'info': {'L1': 3.893597322893543, 'L2': 1.6937253274343331, 'MAX': 0.8695851579472214, 'TrainTime': 1508.1875}}\n",
      "exception: None\n",
      "\n",
      "14:59:23 job_callback for (4, 0, 21) started\n",
      "14:59:23 DISPATCHER: Trying to submit another job.\n",
      "14:59:23 job_callback for (4, 0, 21) got condition\n",
      "14:59:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:59:23 HBMASTER: Trying to run another job!\n",
      "14:59:23 job_callback for (4, 0, 21) finished\n",
      "14:59:36 DISPATCHER: Starting worker discovery\n",
      "14:59:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "14:59:36 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02902: ReduceLROnPlateau reducing learning rate to 9.346752631245181e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:00:24 WORKER: done with job (6, 0, 1), trying to register it.\n",
      "15:00:24 WORKER: registered result for job (6, 0, 1) with dispatcher\n",
      "15:00:24 DISPATCHER: job (6, 0, 1) finished\n",
      "15:00:24 DISPATCHER: register_result: lock acquired\n",
      "15:00:24 DISPATCHER: job (6, 0, 1) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.22496 finished\n",
      "15:00:24 job_id: (6, 0, 1)\n",
      "kwargs: {'config': {'activator': 'selu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0029909609567580946, 'loss': 'mse', 'numLayers': 14, 'numNeurons': 52, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 4.021843409039223, 'info': {'L1': 4.021843409039223, 'L2': 2.047678194474681, 'MAX': 1.3077829698495305, 'TrainTime': 1281.59375}}\n",
      "exception: None\n",
      "\n",
      "15:00:24 job_callback for (6, 0, 1) started\n",
      "15:00:24 DISPATCHER: Trying to submit another job.\n",
      "15:00:24 job_callback for (6, 0, 1) got condition\n",
      "15:00:24 DISPATCHER: jobs to submit = 0, number of idle workers = 2 -> waiting!\n",
      "15:00:24 HBMASTER: Trying to run another job!\n",
      "15:00:24 job_callback for (6, 0, 1) finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01230: ReduceLROnPlateau reducing learning rate to 0.0006656456971541047.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:00:36 DISPATCHER: Starting worker discovery\n",
      "15:00:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:00:36 DISPATCHER: Finished worker discovery\n",
      "15:01:36 DISPATCHER: Starting worker discovery\n",
      "15:01:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:01:36 DISPATCHER: Finished worker discovery\n",
      "15:02:36 DISPATCHER: Starting worker discovery\n",
      "15:02:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:02:36 DISPATCHER: Finished worker discovery\n",
      "15:03:36 DISPATCHER: Starting worker discovery\n",
      "15:03:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:03:36 DISPATCHER: Finished worker discovery\n",
      "15:04:36 DISPATCHER: Starting worker discovery\n",
      "15:04:36 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:04:36 DISPATCHER: Finished worker discovery\n",
      "15:05:36 DISPATCHER: Starting worker discovery\n",
      "15:05:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:05:37 DISPATCHER: Finished worker discovery\n",
      "15:06:37 DISPATCHER: Starting worker discovery\n",
      "15:06:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:06:37 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02442: ReduceLROnPlateau reducing learning rate to 0.00033282284857705235.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:07:37 DISPATCHER: Starting worker discovery\n",
      "15:07:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:07:37 DISPATCHER: Finished worker discovery\n",
      "15:08:37 DISPATCHER: Starting worker discovery\n",
      "15:08:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:08:37 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01572: ReduceLROnPlateau reducing learning rate to 0.003921427298337221.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:09:37 DISPATCHER: Starting worker discovery\n",
      "15:09:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:09:37 DISPATCHER: Finished worker discovery\n",
      "15:10:16 WORKER: done with job (6, 0, 4), trying to register it.\n",
      "15:10:16 WORKER: registered result for job (6, 0, 4) with dispatcher\n",
      "15:10:16 DISPATCHER: job (6, 0, 4) finished\n",
      "15:10:16 DISPATCHER: register_result: lock acquired\n",
      "15:10:16 DISPATCHER: job (6, 0, 4) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.42496 finished\n",
      "15:10:16 job_id: (6, 0, 4)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 11, 'initial_lr': 0.0013312913939173394, 'loss': 'mse', 'numLayers': 15, 'numNeurons': 11, 'optimizer': 'Nadam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 0.2459338859758297, 'info': {'L1': 0.2459338859758297, 'L2': 0.0072262707099517084, 'MAX': 0.24815643895798623, 'TrainTime': 2683.8125}}\n",
      "exception: None\n",
      "\n",
      "15:10:16 job_callback for (6, 0, 4) started\n",
      "15:10:16 DISPATCHER: Trying to submit another job.\n",
      "15:10:16 job_callback for (6, 0, 4) got condition\n",
      "15:10:16 DISPATCHER: jobs to submit = 0, number of idle workers = 3 -> waiting!\n",
      "15:10:16 HBMASTER: Trying to run another job!\n",
      "15:10:16 job_callback for (6, 0, 4) finished\n",
      "15:10:37 DISPATCHER: Starting worker discovery\n",
      "15:10:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:10:37 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01928: ReduceLROnPlateau reducing learning rate to 0.0019607136491686106.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:11:37 DISPATCHER: Starting worker discovery\n",
      "15:11:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:11:37 DISPATCHER: Finished worker discovery\n",
      "15:12:37 DISPATCHER: Starting worker discovery\n",
      "15:12:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:12:37 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 02650: ReduceLROnPlateau reducing learning rate to 0.003134208032861352.\n",
      "\n",
      "Epoch 02527: ReduceLROnPlateau reducing learning rate to 0.0009803568245843053.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:13:37 DISPATCHER: Starting worker discovery\n",
      "15:13:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:13:37 DISPATCHER: Finished worker discovery\n",
      "15:14:37 DISPATCHER: Starting worker discovery\n",
      "15:14:37 DISPATCHER: Found 5 potential workers, 5 currently in the pool.\n",
      "15:14:37 DISPATCHER: Finished worker discovery\n",
      "15:14:59 WORKER: done with job (7, 0, 2), trying to register it.\n",
      "15:14:59 WORKER: registered result for job (7, 0, 2) with dispatcher\n",
      "15:14:59 DISPATCHER: job (7, 0, 2) finished\n",
      "15:14:59 DISPATCHER: register_result: lock acquired\n",
      "15:14:59 DISPATCHER: job (7, 0, 2) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.52496 finished\n",
      "15:14:59 job_id: (7, 0, 2)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 15000, 'denspt': 8, 'initial_lr': 0.006268416082058757, 'loss': 'mae', 'numLayers': 24, 'numNeurons': 64, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 2.3015270735211084, 'info': {'L1': 2.3015270735211084, 'L2': 0.9640678167549245, 'MAX': 1.655869997852494, 'TrainTime': 2967.53125}}\n",
      "exception: None\n",
      "\n",
      "15:15:00 job_callback for (7, 0, 2) started\n",
      "15:15:00 job_callback for (7, 0, 2) got condition\n",
      "15:15:00 DISPATCHER: Trying to submit another job.\n",
      "15:15:00 DISPATCHER: jobs to submit = 0, number of idle workers = 4 -> waiting!\n",
      "15:15:00 HBMASTER: Trying to run another job!\n",
      "15:15:00 job_callback for (7, 0, 2) finished\n",
      "15:15:20 WORKER: done with job (7, 0, 3), trying to register it.\n",
      "15:15:20 WORKER: registered result for job (7, 0, 3) with dispatcher\n",
      "15:15:20 DISPATCHER: job (7, 0, 3) finished\n",
      "15:15:20 DISPATCHER: register_result: lock acquired\n",
      "15:15:20 DISPATCHER: job (7, 0, 3) on hpbandster.run_rod_GridSearch_1.worker.DESKTOP-NKTB8UL.10312.32496 finished\n",
      "15:15:20 job_id: (7, 0, 3)\n",
      "kwargs: {'config': {'activator': 'Addons>gelu', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.00784285490704161, 'loss': 'mae', 'numLayers': 22, 'numNeurons': 40, 'optimizer': 'Adam'}, 'budget': 3000.0, 'working_directory': '.'}\n",
      "result: {'loss': 1.7238821598339418, 'info': {'L1': 1.7238821598339418, 'L2': 0.6776933990892802, 'MAX': 1.252136267536332, 'TrainTime': 2822.0625}}\n",
      "exception: None\n",
      "\n",
      "15:15:20 job_callback for (7, 0, 3) started\n",
      "15:15:20 DISPATCHER: Trying to submit another job.\n",
      "15:15:20 job_callback for (7, 0, 3) got condition\n",
      "15:15:20 DISPATCHER: jobs to submit = 0, number of idle workers = 5 -> waiting!\n",
      "15:15:20 HBMASTER: Trying to run another job!\n",
      "15:15:20 job_callback for (7, 0, 3) finished\n"
     ]
    }
   ],
   "source": [
    "res = bohb.run(n_iterations=8, min_n_workers=numWorkers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eefa090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:07 HBMASTER: shutdown initiated, shutdown_workers = True\n",
      "15:16:08 WORKER: shutting down now!\n",
      "15:16:08 WORKER: shutting down now!\n",
      "15:16:08 WORKER: shutting down now!\n",
      "15:16:08 WORKER: shutting down now!\n",
      "15:16:08 WORKER: shutting down now!\n"
     ]
    }
   ],
   "source": [
    "np.save('HpBandster_Results',res,allow_pickle=True)\n",
    "\n",
    "bohb.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c357ff0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "config:{'activator': 'elu', 'batch_size': 15000, 'denspt': 7, 'initial_lr': 0.008136987243859039, 'loss': 'mse', 'numLayers': 8, 'numNeurons': 42, 'optimizer': 'Adam'}\n",
       "config_info:\n",
       "{}\n",
       "333.3333333333333: 0.33812513189675153\tlosses:\n",
       "\t1000.0: 0.6484396094324323\tlosses:\n",
       "\t3000.0: 0.06484265556770885\ttime stamps: {333.3333333333333: {'submitted': 2517.8033833503723, 'started': 2517.819383621216, 'finished': 2569.4526178836823}, 1000.0: {'submitted': 2918.9522185325623, 'started': 2919.0231952667236, 'finished': 3041.411642074585}, 3000.0: {'submitted': 3055.3004376888275, 'started': 3055.337425470352, 'finished': 3460.675125360489}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.__dict__['data'][res.get_incumbent_id()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7c006-2fe6-426c-b1d2-bfbefb27447e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
