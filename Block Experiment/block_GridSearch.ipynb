{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e88888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpbandster.core.nameserver as hpns\n",
    "import hpbandster.core.result as hpres\n",
    "\n",
    "from hpbandster.optimizers import BOHB as BOHB\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b3474e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- SCIANN 0.6.4.5 ---------------------- \n",
      "For details, check out our review paper and the documentation at: \n",
      " +  \"https://www.sciencedirect.com/science/article/pii/S0045782520307374\", \n",
      " +  \"https://arxiv.org/abs/2005.08803\", \n",
      " +  \"https://www.sciann.com\". \n",
      "\n",
      " Need support or would like to contribute, please join sciann`s slack group: \n",
      " +  \"https://join.slack.com/t/sciann/shared_invite/zt-ne1f5jlx-k_dY8RGo3ZreDXwz0f~CeA\" \n",
      " \n",
      "TensorFlow Version: 2.3.0 \n",
      "Python Version: 3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "import System as SEQ\n",
    "%run block_EQS.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d3dd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pinn_Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a98964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gridObj = SEQ.Grid(4,{'x':0,'y':1,'z':2,'t':3},[[[-2,2],[-2,2],[-2,2],[0,5]]],5)\n",
    "\n",
    "configspecs = {\n",
    "    'denspt':[5,12],\n",
    "    'numNeurons':[10,100],\n",
    "    'numLayers':[2,50],\n",
    "    'activator': ['elu','relu','selu','swish',\n",
    "                 'tanh','Addons>gelu','Addons>mish',\n",
    "                          'Addons>softshrink'],\n",
    "    'loss':['mae','mse'],\n",
    "    'optimizer':['Adam','RMSprop','SGD','Nadam','Ftrl'],\n",
    "    'batch_size':[5000,10000,15000,25000],\n",
    "    'initial_lr':[1e-4,1e-2]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21a476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating worker\n",
      "Creating client\n",
      "Existing experiment. Recovering id.\n",
      "Applying validation data to test grid\n",
      "Saving PDE and config\n",
      "Worker ready\n",
      "Worker created, starting it\n",
      "Worker running, adding it to the list\n",
      "Worker added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:46:10 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x1d49b007f88; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:9090>\n",
      "14:46:10 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "14:46:10 WORKER: start listening for jobs\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Start a nameserver\n",
    "# Every run needs a nameserver. It could be a 'static' server with a\n",
    "# permanent address, but here it will be started for the local machine with the default port.\n",
    "# The nameserver manages the concurrent running workers across all possible threads or clusternodes.\n",
    "# Note the run_id argument. This uniquely identifies a run of any HpBandSter optimizer.\n",
    "\n",
    "numWorkers = 1\n",
    "experiment_name = 'block_exp'\n",
    "run_id = 'block_GridSearch_1'\n",
    "\n",
    "NS = hpns.NameServer(run_id=run_id, host='127.0.0.1', port=None)\n",
    "NS.start()\n",
    "\n",
    "# Step 2: Start a worker\n",
    "# Now we can instantiate a worker, providing the mandatory information\n",
    "# Besides the sleep_interval, we need to define the nameserver information and\n",
    "# the same run_id as above. After that, we can start the worker in the background,\n",
    "# where it will wait for incoming configurations to evaluate.\n",
    "\n",
    "\n",
    "workers=[]\n",
    "for i in range(1,numWorkers+1):\n",
    "    print('Creating worker')\n",
    "    w = Pinn_Worker.PINN_Worker(\n",
    "    valData = valData,\n",
    "    test_gridObj=test_gridObj,\n",
    "    PDESystem=mySys,\n",
    "    configspecs = configspecs,\n",
    "    valFromFEM=False,\n",
    "    experiment_name = experiment_name,\n",
    "    nameserver='127.0.0.1',\n",
    "    run_id=run_id,\n",
    "    id=i)\n",
    "    \n",
    "    print('Worker created, starting it')\n",
    "    w.run(background=True)\n",
    "    print('Worker running, adding it to the list')\n",
    "    workers.append(w)\n",
    "    print('Worker added')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b10f4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:46:10 wait_for_workers trying to get the condition\n",
      "14:46:10 DISPATCHER: started the 'discover_worker' thread\n",
      "14:46:10 DISPATCHER: started the 'job_runner' thread\n",
      "14:46:10 DISPATCHER: Pyro daemon running on localhost:52261\n",
      "14:46:10 DISPATCHER: Starting worker discovery\n",
      "14:46:10 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
      "14:46:10 DISPATCHER: discovered new worker, hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:46:10 HBMASTER: number of workers changed to 1\n",
      "14:46:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:46:10 HBMASTER: only 1 worker(s) available, waiting for at least 1.\n",
      "14:46:10 adjust_queue_size: lock accquired\n",
      "14:46:10 HBMASTER: adjusted queue size to (0, 1)\n",
      "14:46:10 DISPATCHER: Finished worker discovery\n",
      "14:46:10 DISPATCHER: A new worker triggered discover_worker\n",
      "14:46:10 DISPATCHER: Trying to submit another job.\n",
      "14:46:10 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:46:10 Enough workers to start this run!\n",
      "14:46:10 DISPATCHER: Starting worker discovery\n",
      "14:46:10 HBMASTER: starting run at 1648385170.1502604\n",
      "14:46:10 start sampling a new configuration.\n",
      "14:46:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:46:10 done sampling a new configuration.\n",
      "14:46:10 HBMASTER: schedule new run for iteration 0\n",
      "14:46:10 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "14:46:10 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "14:46:10 DISPATCHER: Finished worker discovery\n",
      "14:46:10 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "14:46:10 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:46:10 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "14:46:10 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:46:10 DISPATCHER: Trying to submit another job.\n",
      "14:46:10 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:46:10 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:46:10 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:46:10 WORKER: start processing job (0, 0, 0)\n",
      "14:46:10 WORKER: args: ()\n",
      "14:46:10 WORKER: kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 15000, 'denspt': 4, 'initial_lr': 0.003804834845191026, 'loss': 'mse', 'numLayers': 46, 'numNeurons': 61, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 15000 \n",
      "Total batches: 6 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:47:10 DISPATCHER: Starting worker discovery\n",
      "14:47:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:47:10 DISPATCHER: Finished worker discovery\n",
      "14:47:23 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "14:47:23 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "14:47:23 DISPATCHER: job (0, 0, 0) finished\n",
      "14:47:23 DISPATCHER: register_result: lock acquired\n",
      "14:47:23 DISPATCHER: job (0, 0, 0) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:47:23 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 15000, 'denspt': 4, 'initial_lr': 0.003804834845191026, 'loss': 'mse', 'numLayers': 46, 'numNeurons': 61, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: None\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[15000,61] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_z/gradients_1/D61b_36/MatMul_grad/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/Identity/_4057]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[15000,61] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_z/gradients_1/D61b_36/MatMul_grad/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "\n",
      "14:47:23 job_callback for (0, 0, 0) started\n",
      "14:47:23 DISPATCHER: Trying to submit another job.\n",
      "14:47:23 job_callback for (0, 0, 0) got condition\n",
      "14:47:23 job (0, 0, 0) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[15000,61] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_z/gradients_1/D61b_36/MatMul_grad/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/Identity/_4057]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[15000,61] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_z/gradients_1/D61b_36/MatMul_grad/MatMul}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "14:47:23 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:47:23 Only 1 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:47:23 HBMASTER: Trying to run another job!\n",
      "14:47:23 job_callback for (0, 0, 0) finished\n",
      "14:47:23 start sampling a new configuration.\n",
      "14:47:23 done sampling a new configuration.\n",
      "14:47:23 HBMASTER: schedule new run for iteration 0\n",
      "14:47:23 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "14:47:23 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "14:47:23 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "14:47:23 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:47:23 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "14:47:23 DISPATCHER: Trying to submit another job.\n",
      "14:47:23 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:47:23 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:47:23 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:47:23 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:47:23 WORKER: start processing job (0, 0, 1)\n",
      "14:47:23 WORKER: args: ()\n",
      "14:47:23 WORKER: kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.0096399433434633, 'loss': 'mae', 'numLayers': 46, 'numNeurons': 78, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 414720 \n",
      "Batch size: 10000 \n",
      "Total batches: 42 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:48:10 DISPATCHER: Starting worker discovery\n",
      "14:48:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:48:10 DISPATCHER: Finished worker discovery\n",
      "14:49:10 DISPATCHER: Starting worker discovery\n",
      "14:49:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:49:10 DISPATCHER: Finished worker discovery\n",
      "14:50:10 DISPATCHER: Starting worker discovery\n",
      "14:50:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:50:10 DISPATCHER: Finished worker discovery\n",
      "14:50:17 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "14:50:17 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "14:50:17 DISPATCHER: job (0, 0, 1) finished\n",
      "14:50:17 DISPATCHER: register_result: lock acquired\n",
      "14:50:17 DISPATCHER: job (0, 0, 1) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:50:17 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.0096399433434633, 'loss': 'mae', 'numLayers': 46, 'numNeurons': 78, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: None\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000,78] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Ftrl/gradients/gradients/Grad2_u_y/_1/gradients_1/sci_activation_15/Sigmoid_grad/SigmoidGrad_grad/mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "\n",
      "14:50:17 job_callback for (0, 0, 1) started\n",
      "14:50:17 DISPATCHER: Trying to submit another job.\n",
      "14:50:17 job_callback for (0, 0, 1) got condition\n",
      "14:50:17 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:50:17 job (0, 0, 1) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[10000,78] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node training/Ftrl/gradients/gradients/Grad2_u_y/_1/gradients_1/sci_activation_15/Sigmoid_grad/SigmoidGrad_grad/mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "14:50:17 Only 2 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:50:17 HBMASTER: Trying to run another job!\n",
      "14:50:17 job_callback for (0, 0, 1) finished\n",
      "14:50:17 start sampling a new configuration.\n",
      "14:50:17 done sampling a new configuration.\n",
      "14:50:17 HBMASTER: schedule new run for iteration 0\n",
      "14:50:17 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "14:50:17 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "14:50:17 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "14:50:17 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:50:17 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "14:50:17 DISPATCHER: Trying to submit another job.\n",
      "14:50:17 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:50:17 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:50:17 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:50:17 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:50:17 WORKER: start processing job (0, 0, 2)\n",
      "14:50:17 WORKER: args: ()\n",
      "14:50:17 WORKER: kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 20000, 'denspt': 6, 'initial_lr': 0.0009452043448242207, 'loss': 'mae', 'numLayers': 33, 'numNeurons': 44, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 414720 \n",
      "Batch size: 20000 \n",
      "Total batches: 21 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:51:10 DISPATCHER: Starting worker discovery\n",
      "14:51:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:51:10 DISPATCHER: Finished worker discovery\n",
      "14:52:10 DISPATCHER: Starting worker discovery\n",
      "14:52:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:52:10 DISPATCHER: Finished worker discovery\n",
      "14:53:10 DISPATCHER: Starting worker discovery\n",
      "14:53:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:53:10 DISPATCHER: Finished worker discovery\n",
      "14:54:10 DISPATCHER: Starting worker discovery\n",
      "14:54:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:54:10 DISPATCHER: Finished worker discovery\n",
      "14:55:10 DISPATCHER: Starting worker discovery\n",
      "14:55:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:55:10 DISPATCHER: Finished worker discovery\n",
      "14:56:10 DISPATCHER: Starting worker discovery\n",
      "14:56:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:56:10 DISPATCHER: Finished worker discovery\n",
      "14:56:28 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "14:56:28 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "14:56:28 DISPATCHER: job (0, 0, 2) finished\n",
      "14:56:28 DISPATCHER: register_result: lock acquired\n",
      "14:56:28 DISPATCHER: job (0, 0, 2) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:56:28 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 20000, 'denspt': 6, 'initial_lr': 0.0009452043448242207, 'loss': 'mae', 'numLayers': 33, 'numNeurons': 44, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 5332.036286085031, 'info': {'L1': 5332.036286085031, 'L2': 89354.03185868876, 'MAX': 18.839441299438477, 'TrainTime': 403.875}}\n",
      "exception: None\n",
      "\n",
      "14:56:28 job_callback for (0, 0, 2) started\n",
      "14:56:28 DISPATCHER: Trying to submit another job.\n",
      "14:56:28 job_callback for (0, 0, 2) got condition\n",
      "14:56:28 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:56:28 Only 3 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:56:28 HBMASTER: Trying to run another job!\n",
      "14:56:28 job_callback for (0, 0, 2) finished\n",
      "14:56:28 start sampling a new configuration.\n",
      "14:56:28 done sampling a new configuration.\n",
      "14:56:28 HBMASTER: schedule new run for iteration 0\n",
      "14:56:28 HBMASTER: trying submitting job (0, 0, 3) to dispatcher\n",
      "14:56:28 HBMASTER: submitting job (0, 0, 3) to dispatcher\n",
      "14:56:28 DISPATCHER: trying to submit job (0, 0, 3)\n",
      "14:56:28 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:56:28 HBMASTER: job (0, 0, 3) submitted to dispatcher\n",
      "14:56:28 DISPATCHER: Trying to submit another job.\n",
      "14:56:28 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:56:28 DISPATCHER: starting job (0, 0, 3) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:56:28 DISPATCHER: job (0, 0, 3) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:56:28 WORKER: start processing job (0, 0, 3)\n",
      "14:56:28 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:56:28 WORKER: args: ()\n",
      "14:56:28 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 3, 'initial_lr': 0.005255782370146361, 'loss': 'mae', 'numLayers': 20, 'numNeurons': 95, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 25920 \n",
      "Batch size: 15000 \n",
      "Total batches: 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:56:45 WORKER: done with job (0, 0, 3), trying to register it.\n",
      "14:56:45 WORKER: registered result for job (0, 0, 3) with dispatcher\n",
      "14:56:45 DISPATCHER: job (0, 0, 3) finished\n",
      "14:56:45 DISPATCHER: register_result: lock acquired\n",
      "14:56:45 DISPATCHER: job (0, 0, 3) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:56:45 job_id: (0, 0, 3)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 3, 'initial_lr': 0.005255782370146361, 'loss': 'mae', 'numLayers': 20, 'numNeurons': 95, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 8886.444712369823, 'info': {'L1': 8886.444712369823, 'L2': 247286.18395205788, 'MAX': 29.946967631578445, 'TrainTime': 15.078125}}\n",
      "exception: None\n",
      "\n",
      "14:56:45 job_callback for (0, 0, 3) started\n",
      "14:56:45 DISPATCHER: Trying to submit another job.\n",
      "14:56:45 job_callback for (0, 0, 3) got condition\n",
      "14:56:45 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:56:45 Only 4 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:56:45 HBMASTER: Trying to run another job!\n",
      "14:56:45 job_callback for (0, 0, 3) finished\n",
      "14:56:45 start sampling a new configuration.\n",
      "14:56:45 done sampling a new configuration.\n",
      "14:56:45 HBMASTER: schedule new run for iteration 0\n",
      "14:56:45 HBMASTER: trying submitting job (0, 0, 4) to dispatcher\n",
      "14:56:45 HBMASTER: submitting job (0, 0, 4) to dispatcher\n",
      "14:56:45 DISPATCHER: trying to submit job (0, 0, 4)\n",
      "14:56:45 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:56:45 HBMASTER: job (0, 0, 4) submitted to dispatcher\n",
      "14:56:45 DISPATCHER: Trying to submit another job.\n",
      "14:56:46 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:56:46 DISPATCHER: starting job (0, 0, 4) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:56:46 DISPATCHER: job (0, 0, 4) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:56:46 WORKER: start processing job (0, 0, 4)\n",
      "14:56:46 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:56:46 WORKER: args: ()\n",
      "14:56:46 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.005595949154344638, 'loss': 'mae', 'numLayers': 4, 'numNeurons': 28, 'optimizer': 'Nadam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 5000 \n",
      "Total batches: 17 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:00 WORKER: done with job (0, 0, 4), trying to register it.\n",
      "14:57:00 WORKER: registered result for job (0, 0, 4) with dispatcher\n",
      "14:57:00 DISPATCHER: job (0, 0, 4) finished\n",
      "14:57:00 DISPATCHER: register_result: lock acquired\n",
      "14:57:00 DISPATCHER: job (0, 0, 4) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:57:00 job_id: (0, 0, 4)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 4, 'initial_lr': 0.005595949154344638, 'loss': 'mae', 'numLayers': 4, 'numNeurons': 28, 'optimizer': 'Nadam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 684.2198645833025, 'info': {'L1': 684.2198645833025, 'L2': 1959.9824883048, 'MAX': 5.281412726430663, 'TrainTime': 17.28125}}\n",
      "exception: None\n",
      "\n",
      "14:57:00 job_callback for (0, 0, 4) started\n",
      "14:57:00 DISPATCHER: Trying to submit another job.\n",
      "14:57:00 job_callback for (0, 0, 4) got condition\n",
      "14:57:00 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:57:00 Only 5 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:57:00 HBMASTER: Trying to run another job!\n",
      "14:57:00 job_callback for (0, 0, 4) finished\n",
      "14:57:00 start sampling a new configuration.\n",
      "14:57:00 done sampling a new configuration.\n",
      "14:57:00 HBMASTER: schedule new run for iteration 0\n",
      "14:57:00 HBMASTER: trying submitting job (0, 0, 5) to dispatcher\n",
      "14:57:00 HBMASTER: submitting job (0, 0, 5) to dispatcher\n",
      "14:57:00 DISPATCHER: trying to submit job (0, 0, 5)\n",
      "14:57:00 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:57:00 HBMASTER: job (0, 0, 5) submitted to dispatcher\n",
      "14:57:00 DISPATCHER: Trying to submit another job.\n",
      "14:57:00 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:57:00 DISPATCHER: starting job (0, 0, 5) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:57:00 DISPATCHER: job (0, 0, 5) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:57:00 WORKER: start processing job (0, 0, 5)\n",
      "14:57:00 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:57:00 WORKER: args: ()\n",
      "14:57:00 WORKER: kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 6, 'initial_lr': 0.0022297487007320186, 'loss': 'mse', 'numLayers': 31, 'numNeurons': 96, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 414720 \n",
      "Batch size: 5000 \n",
      "Total batches: 83 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:57:10 DISPATCHER: Starting worker discovery\n",
      "14:57:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:57:10 DISPATCHER: Finished worker discovery\n",
      "14:58:10 DISPATCHER: Starting worker discovery\n",
      "14:58:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:58:10 DISPATCHER: Finished worker discovery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: Invalid loss, terminating training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:2347: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
      "C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py:1664: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "14:58:54 WORKER: done with job (0, 0, 5), trying to register it.\n",
      "14:58:54 WORKER: registered result for job (0, 0, 5) with dispatcher\n",
      "14:58:54 DISPATCHER: job (0, 0, 5) finished\n",
      "14:58:54 DISPATCHER: register_result: lock acquired\n",
      "14:58:54 DISPATCHER: job (0, 0, 5) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:58:54 job_id: (0, 0, 5)\n",
      "kwargs: {'config': {'activator': 'tanh', 'batch_size': 5000, 'denspt': 6, 'initial_lr': 0.0022297487007320186, 'loss': 'mse', 'numLayers': 31, 'numNeurons': 96, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 1.7976931348623157e+308, 'info': {'L1': nan, 'L2': nan, 'MAX': nan, 'TrainTime': 127.8125}}\n",
      "exception: None\n",
      "\n",
      "14:58:54 job_callback for (0, 0, 5) started\n",
      "14:58:54 DISPATCHER: Trying to submit another job.\n",
      "14:58:54 job_callback for (0, 0, 5) got condition\n",
      "14:58:54 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:58:54 Only 6 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:58:54 HBMASTER: Trying to run another job!\n",
      "14:58:54 job_callback for (0, 0, 5) finished\n",
      "14:58:54 start sampling a new configuration.\n",
      "14:58:54 done sampling a new configuration.\n",
      "14:58:54 HBMASTER: schedule new run for iteration 0\n",
      "14:58:54 HBMASTER: trying submitting job (0, 0, 6) to dispatcher\n",
      "14:58:54 HBMASTER: submitting job (0, 0, 6) to dispatcher\n",
      "14:58:54 DISPATCHER: trying to submit job (0, 0, 6)\n",
      "14:58:54 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:58:54 HBMASTER: job (0, 0, 6) submitted to dispatcher\n",
      "14:58:54 DISPATCHER: Trying to submit another job.\n",
      "14:58:54 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:58:54 DISPATCHER: starting job (0, 0, 6) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:58:54 DISPATCHER: job (0, 0, 6) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:58:54 WORKER: start processing job (0, 0, 6)\n",
      "14:58:54 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:58:54 WORKER: args: ()\n",
      "14:58:54 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 4, 'initial_lr': 0.004477465097237978, 'loss': 'mae', 'numLayers': 11, 'numNeurons': 99, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 10000 \n",
      "Total batches: 9 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:59:10 DISPATCHER: Starting worker discovery\n",
      "14:59:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "14:59:10 DISPATCHER: Finished worker discovery\n",
      "14:59:16 WORKER: done with job (0, 0, 6), trying to register it.\n",
      "14:59:16 WORKER: registered result for job (0, 0, 6) with dispatcher\n",
      "14:59:16 DISPATCHER: job (0, 0, 6) finished\n",
      "14:59:16 DISPATCHER: register_result: lock acquired\n",
      "14:59:16 DISPATCHER: job (0, 0, 6) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "14:59:16 job_id: (0, 0, 6)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 10000, 'denspt': 4, 'initial_lr': 0.004477465097237978, 'loss': 'mae', 'numLayers': 11, 'numNeurons': 99, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 8808.75234672346, 'info': {'L1': 8808.75234672346, 'L2': 242989.9910767469, 'MAX': 29.704178988933563, 'TrainTime': 23.84375}}\n",
      "exception: None\n",
      "\n",
      "14:59:16 job_callback for (0, 0, 6) started\n",
      "14:59:16 DISPATCHER: Trying to submit another job.\n",
      "14:59:16 job_callback for (0, 0, 6) got condition\n",
      "14:59:16 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "14:59:16 Only 7 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "14:59:16 HBMASTER: Trying to run another job!\n",
      "14:59:16 job_callback for (0, 0, 6) finished\n",
      "14:59:16 start sampling a new configuration.\n",
      "14:59:16 done sampling a new configuration.\n",
      "14:59:16 HBMASTER: schedule new run for iteration 0\n",
      "14:59:16 HBMASTER: trying submitting job (0, 0, 7) to dispatcher\n",
      "14:59:16 HBMASTER: submitting job (0, 0, 7) to dispatcher\n",
      "14:59:16 DISPATCHER: trying to submit job (0, 0, 7)\n",
      "14:59:16 DISPATCHER: trying to notify the job_runner thread.\n",
      "14:59:16 HBMASTER: job (0, 0, 7) submitted to dispatcher\n",
      "14:59:16 DISPATCHER: Trying to submit another job.\n",
      "14:59:16 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "14:59:16 DISPATCHER: starting job (0, 0, 7) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:59:16 DISPATCHER: job (0, 0, 7) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "14:59:16 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "14:59:16 WORKER: start processing job (0, 0, 7)\n",
      "14:59:16 WORKER: args: ()\n",
      "14:59:16 WORKER: kwargs: {'config': {'activator': 'softmax', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0038434079234294636, 'loss': 'mse', 'numLayers': 31, 'numNeurons': 86, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 414720 \n",
      "Batch size: 15000 \n",
      "Total batches: 28 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:00:10 DISPATCHER: Starting worker discovery\n",
      "15:00:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:00:10 DISPATCHER: Finished worker discovery\n",
      "15:01:10 DISPATCHER: Starting worker discovery\n",
      "15:01:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:01:10 DISPATCHER: Finished worker discovery\n",
      "15:01:12 WORKER: done with job (0, 0, 7), trying to register it.\n",
      "15:01:12 WORKER: registered result for job (0, 0, 7) with dispatcher\n",
      "15:01:12 DISPATCHER: job (0, 0, 7) finished\n",
      "15:01:12 DISPATCHER: register_result: lock acquired\n",
      "15:01:12 DISPATCHER: job (0, 0, 7) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:01:12 job_id: (0, 0, 7)\n",
      "kwargs: {'config': {'activator': 'softmax', 'batch_size': 15000, 'denspt': 6, 'initial_lr': 0.0038434079234294636, 'loss': 'mse', 'numLayers': 31, 'numNeurons': 86, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: None\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[15000,86] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_7/gradients_1/Grad2_u_x/_7/gradients/sci_activation_19/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/AddN/_4991]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[15000,86] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_7/gradients_1/Grad2_u_x/_7/gradients/sci_activation_19/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "\n",
      "15:01:12 job_callback for (0, 0, 7) started\n",
      "15:01:12 DISPATCHER: Trying to submit another job.\n",
      "15:01:12 job_callback for (0, 0, 7) got condition\n",
      "15:01:12 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:01:12 job (0, 0, 7) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[15000,86] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_7/gradients_1/Grad2_u_x/_7/gradients/sci_activation_19/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/AddN/_4991]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[15000,86] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_7/gradients_1/Grad2_u_x/_7/gradients/sci_activation_19/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "15:01:12 Only 8 run(s) for budget 18.518519 available, need more than 10 -> can't build model!\n",
      "15:01:12 HBMASTER: Trying to run another job!\n",
      "15:01:12 job_callback for (0, 0, 7) finished\n",
      "15:01:12 start sampling a new configuration.\n",
      "15:01:12 done sampling a new configuration.\n",
      "15:01:12 HBMASTER: schedule new run for iteration 0\n",
      "15:01:12 HBMASTER: trying submitting job (0, 0, 8) to dispatcher\n",
      "15:01:12 HBMASTER: submitting job (0, 0, 8) to dispatcher\n",
      "15:01:12 DISPATCHER: trying to submit job (0, 0, 8)\n",
      "15:01:12 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:01:12 HBMASTER: job (0, 0, 8) submitted to dispatcher\n",
      "15:01:12 DISPATCHER: Trying to submit another job.\n",
      "15:01:12 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:01:12 DISPATCHER: starting job (0, 0, 8) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:01:12 DISPATCHER: job (0, 0, 8) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:01:12 WORKER: start processing job (0, 0, 8)\n",
      "15:01:12 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:01:12 WORKER: args: ()\n",
      "15:01:12 WORKER: kwargs: {'config': {'activator': 'softmax', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.005796711617826102, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 60, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 25920 \n",
      "Batch size: 5000 \n",
      "Total batches: 6 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:02:10 DISPATCHER: Starting worker discovery\n",
      "15:02:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:02:10 DISPATCHER: Finished worker discovery\n",
      "15:02:49 WORKER: done with job (0, 0, 8), trying to register it.\n",
      "15:02:49 WORKER: registered result for job (0, 0, 8) with dispatcher\n",
      "15:02:49 DISPATCHER: job (0, 0, 8) finished\n",
      "15:02:49 DISPATCHER: register_result: lock acquired\n",
      "15:02:49 DISPATCHER: job (0, 0, 8) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:02:49 job_id: (0, 0, 8)\n",
      "kwargs: {'config': {'activator': 'softmax', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.005796711617826102, 'loss': 'mse', 'numLayers': 35, 'numNeurons': 60, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 349.331937694974, 'info': {'L1': 349.331937694974, 'L2': 509.3878727501232, 'MAX': 3.0952459631249987, 'TrainTime': 110.15625}}\n",
      "exception: None\n",
      "\n",
      "15:02:49 job_callback for (0, 0, 8) started\n",
      "15:02:49 DISPATCHER: Trying to submit another job.\n",
      "15:02:49 job_callback for (0, 0, 8) got condition\n",
      "15:02:49 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:02:49 HBMASTER: Trying to run another job!\n",
      "15:02:49 job_callback for (0, 0, 8) finished\n",
      "15:02:49 start sampling a new configuration.\n",
      "15:02:49 done sampling a new configuration.\n",
      "15:02:49 HBMASTER: schedule new run for iteration 0\n",
      "15:02:49 HBMASTER: trying submitting job (0, 0, 9) to dispatcher\n",
      "15:02:49 HBMASTER: submitting job (0, 0, 9) to dispatcher\n",
      "15:02:49 DISPATCHER: trying to submit job (0, 0, 9)\n",
      "15:02:49 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:02:49 HBMASTER: job (0, 0, 9) submitted to dispatcher\n",
      "15:02:49 DISPATCHER: Trying to submit another job.\n",
      "15:02:49 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:02:49 DISPATCHER: starting job (0, 0, 9) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:02:49 DISPATCHER: job (0, 0, 9) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:02:49 WORKER: start processing job (0, 0, 9)\n",
      "15:02:49 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:02:49 WORKER: args: ()\n",
      "15:02:49 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0020852751875799005, 'loss': 'mse', 'numLayers': 22, 'numNeurons': 29, 'optimizer': 'RMSprop'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 25920 \n",
      "Batch size: 5000 \n",
      "Total batches: 6 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:03:07 WORKER: done with job (0, 0, 9), trying to register it.\n",
      "15:03:07 WORKER: registered result for job (0, 0, 9) with dispatcher\n",
      "15:03:07 DISPATCHER: job (0, 0, 9) finished\n",
      "15:03:07 DISPATCHER: register_result: lock acquired\n",
      "15:03:07 DISPATCHER: job (0, 0, 9) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:03:07 job_id: (0, 0, 9)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0020852751875799005, 'loss': 'mse', 'numLayers': 22, 'numNeurons': 29, 'optimizer': 'RMSprop'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 995.2385358937274, 'info': {'L1': 995.2385358937274, 'L2': 3452.586036129887, 'MAX': 7.742242457451173, 'TrainTime': 17.0}}\n",
      "exception: None\n",
      "\n",
      "15:03:07 job_callback for (0, 0, 9) started\n",
      "15:03:07 DISPATCHER: Trying to submit another job.\n",
      "15:03:07 job_callback for (0, 0, 9) got condition\n",
      "15:03:07 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:03:07 HBMASTER: Trying to run another job!\n",
      "15:03:07 job_callback for (0, 0, 9) finished\n",
      "15:03:07 start sampling a new configuration.\n",
      "15:03:07 done sampling a new configuration.\n",
      "15:03:07 HBMASTER: schedule new run for iteration 0\n",
      "15:03:07 HBMASTER: trying submitting job (0, 0, 10) to dispatcher\n",
      "15:03:07 HBMASTER: submitting job (0, 0, 10) to dispatcher\n",
      "15:03:07 DISPATCHER: trying to submit job (0, 0, 10)\n",
      "15:03:07 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:03:07 HBMASTER: job (0, 0, 10) submitted to dispatcher\n",
      "15:03:07 DISPATCHER: Trying to submit another job.\n",
      "15:03:07 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:03:07 DISPATCHER: starting job (0, 0, 10) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:03:07 DISPATCHER: job (0, 0, 10) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:03:07 WORKER: start processing job (0, 0, 10)\n",
      "15:03:07 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:03:07 WORKER: args: ()\n",
      "15:03:07 WORKER: kwargs: {'config': {'activator': 'softmax', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0008663276201311967, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 83, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 25920 \n",
      "Batch size: 5000 \n",
      "Total batches: 6 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:03:10 DISPATCHER: Starting worker discovery\n",
      "15:03:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:03:10 DISPATCHER: Finished worker discovery\n",
      "15:03:44 WORKER: done with job (0, 0, 10), trying to register it.\n",
      "15:03:44 WORKER: registered result for job (0, 0, 10) with dispatcher\n",
      "15:03:44 DISPATCHER: job (0, 0, 10) finished\n",
      "15:03:44 DISPATCHER: register_result: lock acquired\n",
      "15:03:44 DISPATCHER: job (0, 0, 10) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:03:44 job_id: (0, 0, 10)\n",
      "kwargs: {'config': {'activator': 'softmax', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.0008663276201311967, 'loss': 'mse', 'numLayers': 12, 'numNeurons': 83, 'optimizer': 'Adam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 8827.575455396556, 'info': {'L1': 8827.575455396556, 'L2': 244027.39893553953, 'MAX': 29.763001203536987, 'TrainTime': 42.203125}}\n",
      "exception: None\n",
      "\n",
      "15:03:44 job_callback for (0, 0, 10) started\n",
      "15:03:44 DISPATCHER: Trying to submit another job.\n",
      "15:03:44 job_callback for (0, 0, 10) got condition\n",
      "15:03:44 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:03:44 HBMASTER: Trying to run another job!\n",
      "15:03:44 job_callback for (0, 0, 10) finished\n",
      "15:03:44 start sampling a new configuration.\n",
      "15:03:44 done sampling a new configuration.\n",
      "15:03:44 HBMASTER: schedule new run for iteration 0\n",
      "15:03:44 HBMASTER: trying submitting job (0, 0, 11) to dispatcher\n",
      "15:03:44 HBMASTER: submitting job (0, 0, 11) to dispatcher\n",
      "15:03:44 DISPATCHER: trying to submit job (0, 0, 11)\n",
      "15:03:44 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:03:44 HBMASTER: job (0, 0, 11) submitted to dispatcher\n",
      "15:03:44 DISPATCHER: Trying to submit another job.\n",
      "15:03:44 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:03:44 DISPATCHER: starting job (0, 0, 11) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:03:44 DISPATCHER: job (0, 0, 11) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:03:44 WORKER: start processing job (0, 0, 11)\n",
      "15:03:44 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:03:44 WORKER: args: ()\n",
      "15:03:44 WORKER: kwargs: {'config': {'activator': 'softmax', 'batch_size': 20000, 'denspt': 4, 'initial_lr': 0.004976895301938728, 'loss': 'mae', 'numLayers': 20, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 81920 \n",
      "Batch size: 20000 \n",
      "Total batches: 5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:04:10 DISPATCHER: Starting worker discovery\n",
      "15:04:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:04:10 DISPATCHER: Finished worker discovery\n",
      "15:05:10 DISPATCHER: Starting worker discovery\n",
      "15:05:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:05:10 DISPATCHER: Finished worker discovery\n",
      "15:05:25 WORKER: done with job (0, 0, 11), trying to register it.\n",
      "15:05:25 WORKER: registered result for job (0, 0, 11) with dispatcher\n",
      "15:05:25 DISPATCHER: job (0, 0, 11) finished\n",
      "15:05:25 DISPATCHER: register_result: lock acquired\n",
      "15:05:25 DISPATCHER: job (0, 0, 11) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:05:25 job_id: (0, 0, 11)\n",
      "kwargs: {'config': {'activator': 'softmax', 'batch_size': 20000, 'denspt': 4, 'initial_lr': 0.004976895301938728, 'loss': 'mae', 'numLayers': 20, 'numNeurons': 89, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: None\n",
      "exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[20000,89] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_11/gradients_1/Grad2_u_x/_11/gradients/sci_activation_15/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/Identity/_3449]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[20000,89] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_11/gradients_1/Grad2_u_x/_11/gradients/sci_activation_15/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "\n",
      "15:05:25 job_callback for (0, 0, 11) started\n",
      "15:05:25 job_callback for (0, 0, 11) got condition\n",
      "15:05:25 DISPATCHER: Trying to submit another job.\n",
      "15:05:25 job (0, 0, 11) failed with exception\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\worker.py\", line 206, in start_computation\n",
      "    result = {'result': self.compute(*args, config_id=id, **kwargs),\n",
      "  File \"..\\Pinn_Worker.py\", line 115, in compute\n",
      "    batch_size=config['batch_size'],learning_rate=config['initial_lr'])\n",
      "  File \"C:\\Users\\Felix\\AppData\\Roaming\\Python\\Python37\\site-packages\\sciann\\models\\model.py\", line 569, in train\n",
      "    **kwargs\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 809, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 590, in fit\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\", line 256, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\", line 1089, in train_on_batch\n",
      "    outputs = self.train_function(ins)  # pylint: disable=not-callable\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3825, in __call__\n",
      "    run_metadata=self.run_metadata)\n",
      "  File \"C:\\Users\\Felix\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1472, in __call__\n",
      "    run_metadata_ptr)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted: OOM when allocating tensor with shape[20000,89] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_11/gradients_1/Grad2_u_x/_11/gradients/sci_activation_15/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[loss_1/Identity/_3449]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted: OOM when allocating tensor with shape[20000,89] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Grad2_u_x/_11/gradients_1/Grad2_u_x/_11/gradients/sci_activation_15/Softmax_grad/mul_grad/Mul_1}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored.\n",
      "\n",
      "15:05:25 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:05:25 HBMASTER: Trying to run another job!\n",
      "15:05:25 job_callback for (0, 0, 11) finished\n",
      "15:05:25 start sampling a new configuration.\n",
      "15:05:25 done sampling a new configuration.\n",
      "15:05:25 HBMASTER: schedule new run for iteration 0\n",
      "15:05:25 HBMASTER: trying submitting job (0, 0, 12) to dispatcher\n",
      "15:05:25 HBMASTER: submitting job (0, 0, 12) to dispatcher\n",
      "15:05:25 DISPATCHER: trying to submit job (0, 0, 12)\n",
      "15:05:25 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:05:25 HBMASTER: job (0, 0, 12) submitted to dispatcher\n",
      "15:05:25 DISPATCHER: Trying to submit another job.\n",
      "15:05:25 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:05:25 DISPATCHER: starting job (0, 0, 12) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:05:25 DISPATCHER: job (0, 0, 12) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:05:25 WORKER: start processing job (0, 0, 12)\n",
      "15:05:25 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:05:25 WORKER: args: ()\n",
      "15:05:25 WORKER: kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.008081473682539838, 'loss': 'mae', 'numLayers': 12, 'numNeurons': 24, 'optimizer': 'Nadam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 25920 \n",
      "Batch size: 5000 \n",
      "Total batches: 6 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:05:44 WORKER: done with job (0, 0, 12), trying to register it.\n",
      "15:05:44 WORKER: registered result for job (0, 0, 12) with dispatcher\n",
      "15:05:44 DISPATCHER: job (0, 0, 12) finished\n",
      "15:05:44 DISPATCHER: register_result: lock acquired\n",
      "15:05:44 DISPATCHER: job (0, 0, 12) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:05:44 job_id: (0, 0, 12)\n",
      "kwargs: {'config': {'activator': 'sigmoid', 'batch_size': 5000, 'denspt': 3, 'initial_lr': 0.008081473682539838, 'loss': 'mae', 'numLayers': 12, 'numNeurons': 24, 'optimizer': 'Nadam'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 3832.9560858897194, 'info': {'L1': 3832.9560858897194, 'L2': 46419.473734412444, 'MAX': 14.154815673828125, 'TrainTime': 20.609375}}\n",
      "exception: None\n",
      "\n",
      "15:05:44 job_callback for (0, 0, 12) started\n",
      "15:05:44 DISPATCHER: Trying to submit another job.\n",
      "15:05:44 job_callback for (0, 0, 12) got condition\n",
      "15:05:44 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:05:44 HBMASTER: Trying to run another job!\n",
      "15:05:44 job_callback for (0, 0, 12) finished\n",
      "15:05:44 start sampling a new configuration.\n",
      "15:05:44 done sampling a new configuration.\n",
      "15:05:44 HBMASTER: schedule new run for iteration 0\n",
      "15:05:44 HBMASTER: trying submitting job (0, 0, 13) to dispatcher\n",
      "15:05:44 HBMASTER: submitting job (0, 0, 13) to dispatcher\n",
      "15:05:44 DISPATCHER: trying to submit job (0, 0, 13)\n",
      "15:05:44 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:05:44 HBMASTER: job (0, 0, 13) submitted to dispatcher\n",
      "15:05:44 DISPATCHER: Trying to submit another job.\n",
      "15:05:44 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:05:44 DISPATCHER: starting job (0, 0, 13) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:05:44 DISPATCHER: job (0, 0, 13) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:05:44 WORKER: start processing job (0, 0, 13)\n",
      "15:05:44 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:05:44 WORKER: args: ()\n",
      "15:05:44 WORKER: kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 5, 'initial_lr': 0.006436430583913265, 'loss': 'mae', 'numLayers': 43, 'numNeurons': 20, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 200000 \n",
      "Batch size: 15000 \n",
      "Total batches: 14 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:06:10 DISPATCHER: Starting worker discovery\n",
      "15:06:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:06:10 DISPATCHER: Finished worker discovery\n",
      "15:06:35 WORKER: done with job (0, 0, 13), trying to register it.\n",
      "15:06:35 WORKER: registered result for job (0, 0, 13) with dispatcher\n",
      "15:06:35 DISPATCHER: job (0, 0, 13) finished\n",
      "15:06:35 DISPATCHER: register_result: lock acquired\n",
      "15:06:35 DISPATCHER: job (0, 0, 13) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:06:35 job_id: (0, 0, 13)\n",
      "kwargs: {'config': {'activator': 'relu', 'batch_size': 15000, 'denspt': 5, 'initial_lr': 0.006436430583913265, 'loss': 'mae', 'numLayers': 43, 'numNeurons': 20, 'optimizer': 'SGD'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 353.7963075470491, 'info': {'L1': 353.7963075470491, 'L2': 616.5765643304507, 'MAX': 2.7583236694335938, 'TrainTime': 58.546875}}\n",
      "exception: None\n",
      "\n",
      "15:06:35 job_callback for (0, 0, 13) started\n",
      "15:06:35 DISPATCHER: Trying to submit another job.\n",
      "15:06:35 job_callback for (0, 0, 13) got condition\n",
      "15:06:35 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:06:35 HBMASTER: Trying to run another job!\n",
      "15:06:35 job_callback for (0, 0, 13) finished\n",
      "15:06:35 start sampling a new configuration.\n",
      "15:06:35 done sampling a new configuration.\n",
      "15:06:35 HBMASTER: schedule new run for iteration 0\n",
      "15:06:35 HBMASTER: trying submitting job (0, 0, 14) to dispatcher\n",
      "15:06:35 HBMASTER: submitting job (0, 0, 14) to dispatcher\n",
      "15:06:35 DISPATCHER: trying to submit job (0, 0, 14)\n",
      "15:06:35 DISPATCHER: trying to notify the job_runner thread.\n",
      "15:06:35 HBMASTER: job (0, 0, 14) submitted to dispatcher\n",
      "15:06:35 DISPATCHER: Trying to submit another job.\n",
      "15:06:35 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "15:06:35 DISPATCHER: starting job (0, 0, 14) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:06:35 DISPATCHER: job (0, 0, 14) dispatched on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296\n",
      "15:06:35 WORKER: start processing job (0, 0, 14)\n",
      "15:06:35 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "15:06:35 WORKER: args: ()\n",
      "15:06:35 WORKER: kwargs: {'config': {'activator': 'softmax', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.008131551571715323, 'loss': 'mse', 'numLayers': 19, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples: 414720 \n",
      "Batch size: 10000 \n",
      "Total batches: 42 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:07:10 DISPATCHER: Starting worker discovery\n",
      "15:07:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:07:10 DISPATCHER: Finished worker discovery\n",
      "15:08:10 DISPATCHER: Starting worker discovery\n",
      "15:08:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:08:10 DISPATCHER: Finished worker discovery\n",
      "15:09:10 DISPATCHER: Starting worker discovery\n",
      "15:09:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:09:10 DISPATCHER: Finished worker discovery\n",
      "15:10:10 DISPATCHER: Starting worker discovery\n",
      "15:10:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:10:10 DISPATCHER: Finished worker discovery\n",
      "15:11:10 DISPATCHER: Starting worker discovery\n",
      "15:11:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:11:10 DISPATCHER: Finished worker discovery\n",
      "15:12:10 DISPATCHER: Starting worker discovery\n",
      "15:12:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:12:10 DISPATCHER: Finished worker discovery\n",
      "15:13:10 DISPATCHER: Starting worker discovery\n",
      "15:13:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:13:10 DISPATCHER: Finished worker discovery\n",
      "15:14:10 DISPATCHER: Starting worker discovery\n",
      "15:14:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:14:10 DISPATCHER: Finished worker discovery\n",
      "15:15:10 DISPATCHER: Starting worker discovery\n",
      "15:15:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:15:10 DISPATCHER: Finished worker discovery\n",
      "15:15:39 WORKER: done with job (0, 0, 14), trying to register it.\n",
      "15:15:39 WORKER: registered result for job (0, 0, 14) with dispatcher\n",
      "15:15:39 DISPATCHER: job (0, 0, 14) finished\n",
      "15:15:39 DISPATCHER: register_result: lock acquired\n",
      "15:15:39 DISPATCHER: job (0, 0, 14) on hpbandster.run_block_GridSearch_1.worker.DESKTOP-NKTB8UL.7940.13296 finished\n",
      "15:15:39 job_id: (0, 0, 14)\n",
      "kwargs: {'config': {'activator': 'softmax', 'batch_size': 10000, 'denspt': 6, 'initial_lr': 0.008131551571715323, 'loss': 'mse', 'numLayers': 19, 'numNeurons': 97, 'optimizer': 'Ftrl'}, 'budget': 18.51851851851852, 'working_directory': '.'}\n",
      "result: {'loss': 8626.202240674877, 'info': {'L1': 8626.202240674877, 'L2': 233043.8885489912, 'MAX': 29.13370990753174, 'TrainTime': 633.21875}}\n",
      "exception: None\n",
      "\n",
      "15:15:39 job_callback for (0, 0, 14) started\n",
      "15:15:39 DISPATCHER: Trying to submit another job.\n",
      "15:15:39 job_callback for (0, 0, 14) got condition\n",
      "15:15:39 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "15:15:39 HBMASTER: Trying to run another job!\n",
      "15:15:39 job_callback for (0, 0, 14) finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7940/1726514967.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m               \u001b[0mmin_budget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_budget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m            )\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbohb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_n_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumWorkers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\master.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_iterations, min_n_workers, iteration_kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                         \u001b[0mnext_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\hpbandster\\core\\master.py\u001b[0m in \u001b[0;36m_queue_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m                         \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_running_jobs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_queue_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HBMASTER: running jobs: %i, queue sizes: %s -> wait'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_running_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_queue_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m                                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthread_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_submit_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbudget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# bohb = BOHB(  configspace = w.get_configspace(),\n",
    "#               run_id = 'example1', nameserver='127.0.0.1',\n",
    "#               min_budget=10, max_budget=1000\n",
    "#            )\n",
    "# res = bohb.run(n_iterations=25\n",
    "#               )\n",
    "\n",
    "bohb = BOHB(  configspace = w.get_configspace(),\n",
    "              run_id = run_id,\n",
    "              min_budget=10, max_budget=500\n",
    "           )\n",
    "res = bohb.run(n_iterations=5, min_n_workers=numWorkers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:16:10 DISPATCHER: Starting worker discovery\n",
      "15:16:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:16:10 DISPATCHER: Finished worker discovery\n",
      "15:17:10 DISPATCHER: Starting worker discovery\n",
      "15:17:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:17:10 DISPATCHER: Finished worker discovery\n",
      "15:18:10 DISPATCHER: Starting worker discovery\n",
      "15:18:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:18:10 DISPATCHER: Finished worker discovery\n",
      "15:19:10 DISPATCHER: Starting worker discovery\n",
      "15:19:10 DISPATCHER: Found 1 potential workers, 1 currently in the pool.\n",
      "15:19:10 DISPATCHER: Finished worker discovery\n"
     ]
    }
   ],
   "source": [
    "np.save('HpBandster_Results',res,allow_pickle=True)\n",
    "bohb.shutdown(shutdown_workers=True)\n",
    "NS.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d7f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
